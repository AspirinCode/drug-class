{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dogs breeds\n",
    "\n",
    "https://youtu.be/JNxcznsrRb8?t=1h31m8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Dog Breed Identification. Get data from https://www.kaggle.com/c/dog-breed-identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1805\n",
      "[7348 7358 8293 6898 1468]\n"
     ]
    }
   ],
   "source": [
    "PATH = \"data/drugs/pics/\"\n",
    "sz = 224\n",
    "arch = resnext101_64\n",
    "bs = 150\n",
    "label_csv = f'{PATH}5labels.csv'\n",
    "n = len(list(open(label_csv))) - 1 # header is not counted (-1)\n",
    "val_idxs = get_cv_idxs(n, seed=random.sample(range(9), 1)) # random 20% data for validation set\n",
    "print(len(val_idxs))\n",
    "print(val_idxs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sz, bs): # sz: image size, bs: batch size\n",
    "    tfms = tfms_from_model(arch, sz, aug_tfms=transforms_side_on, max_zoom=1.1)\n",
    "    data = ImageClassifierData.from_csv(PATH, 'train', f'{PATH}5labels.csv',\n",
    "                                       val_idxs=val_idxs, suffix='.png', tfms=tfms, bs=bs)\n",
    "    \n",
    "    # http://forums.fast.ai/t/how-to-train-on-the-full-dataset-using-imageclassifierdata-from-csv/7761/13\n",
    "    # http://forums.fast.ai/t/how-to-train-on-the-full-dataset-using-imageclassifierdata-from-csv/7761/37\n",
    "    return data if sz > 300 else data.resize(340, 'tmp') # Reading the jpgs and resizing is slow for big images, so resizing them all to 340 first saves time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_loop1(k, epochs):\n",
    "    validation_accuracy = []\n",
    "    for reps in range(k):\n",
    "        val_idxs = get_cv_idxs(n, seed=random.sample(range(1000), 1)) # random 20% data for validation set\n",
    "        data = get_data(sz, bs)\n",
    "        learn = ConvLearner.pretrained(arch, data, precompute=True)\n",
    "        learn.fit(1e-2, epochs)\n",
    "        validation_accuracy.append(val_acc)\n",
    "        learn.save(str(reps)+'_5cls_100epochs_10fold.model')\n",
    "    return validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5472a2a1e2184733be60ddc83df87e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 49/49 [01:03<00:00,  1.07it/s]\n",
      "100%|██████████| 13/13 [00:16<00:00,  1.23s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f45b5f6c5e47fc8af26171041d1bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.453737   1.192425   0.513573  \n",
      "    1      1.261324   1.130666   0.542936                 \n",
      "    2      1.173712   1.102371   0.552355                 \n",
      "    3      1.115144   1.060745   0.563989                 \n",
      "    4      1.058528   1.05111    0.576731                 \n",
      "    5      1.017212   1.016641   0.596122                 \n",
      "    6      0.983419   1.000995   0.597784                  \n",
      "    7      0.949759   1.019241   0.586704                  \n",
      "    8      0.933433   0.978841   0.606648                  \n",
      "    9      0.916181   0.968044   0.6                       \n",
      "    10     0.885175   0.956123   0.616066                  \n",
      "    11     0.855921   0.963197   0.607202                  \n",
      "    12     0.843555   0.947487   0.625485                  \n",
      "    13     0.823425   0.940084   0.617729                  \n",
      "    14     0.82074    0.945169   0.623823                  \n",
      "    15     0.813485   0.951876   0.626593                  \n",
      "    16     0.796758   0.933399   0.629917                  \n",
      "    17     0.776133   0.91957    0.633241                  \n",
      "    18     0.769596   0.930108   0.634349                  \n",
      "    19     0.74344    0.943821   0.636565                  \n",
      "    20     0.739366   0.916301   0.631579                  \n",
      "    21     0.724678   0.92401    0.637119                  \n",
      "    22     0.722571   0.913861   0.637673                  \n",
      "    23     0.709937   0.920271   0.641551                  \n",
      "    24     0.696429   0.921315   0.641551                  \n",
      "    25     0.681975   0.933945   0.635457                  \n",
      "    26     0.666939   0.911424   0.632687                  \n",
      "    27     0.661244   0.9037     0.642659                  \n",
      "    28     0.651732   0.918351   0.634903                  \n",
      "    29     0.653759   0.91737    0.638227                  \n",
      "    30     0.645612   0.914931   0.636565                  \n",
      "    31     0.630983   0.903538   0.636565                  \n",
      "    32     0.619625   0.92386    0.644875                  \n",
      "    33     0.621771   0.912262   0.649307                  \n",
      "    34     0.614953   0.916704   0.642105                  \n",
      "    35     0.610043   0.903924   0.653186                  \n",
      "    36     0.597226   0.91414    0.652632                  \n",
      "    37     0.596299   0.918164   0.639335                  \n",
      "    38     0.58467    0.917467   0.641551                  \n",
      "    39     0.579159   0.921194   0.654848                  \n",
      "    40     0.566503   0.918202   0.634349                  \n",
      "    41     0.555816   0.917611   0.65097                   \n",
      "    42     0.56075    0.920904   0.65097                   \n",
      "    43     0.544859   0.930453   0.649307                  \n",
      "    44     0.545481   0.929022   0.645983                  \n",
      "    45     0.530202   0.923309   0.646537                  \n",
      "    46     0.541849   0.95656    0.640443                  \n",
      "    47     0.537979   0.929809   0.644875                  \n",
      "    48     0.529308   0.915718   0.644321                  \n",
      "    49     0.516799   0.917494   0.653186                  \n",
      "    50     0.511223   0.927154   0.648753                  \n",
      "    51     0.519332   0.950969   0.647645                  \n",
      "    52     0.509786   0.959209   0.640443                  \n",
      "    53     0.522393   0.940555   0.641551                  \n",
      "    54     0.495452   0.941904   0.645429                  \n",
      "    55     0.48941    0.939323   0.649861                  \n",
      "    56     0.490919   0.941558   0.646537                  \n",
      "    57     0.492301   0.94568    0.65097                   \n",
      "    58     0.497062   0.927532   0.65651                   \n",
      "    59     0.477855   0.949242   0.654294                  \n",
      "    60     0.464673   0.942447   0.655402                  \n",
      "    61     0.449035   0.944588   0.655956                  \n",
      "    62     0.467723   0.957034   0.648199                  \n",
      "    63     0.469687   0.946748   0.65651                   \n",
      "    64     0.451547   0.953888   0.651524                  \n",
      "    65     0.442831   0.966388   0.650416                  \n",
      "    66     0.457355   0.981447   0.652632                  \n",
      "    67     0.448817   0.950435   0.65651                   \n",
      "    68     0.4449     0.96289    0.647645                  \n",
      "    69     0.450598   0.98661    0.649307                  \n",
      "    70     0.441168   0.977852   0.650416                  \n",
      "    71     0.438573   0.978417   0.65651                   \n",
      "    72     0.423502   0.964912   0.641551                  \n",
      "    73     0.41523    0.970923   0.658172                  \n",
      "    74     0.409598   0.971659   0.658172                  \n",
      "    75     0.418081   0.980797   0.647091                  \n",
      "    76     0.408445   0.98913    0.654294                  \n",
      "    77     0.40726    0.99301    0.655402                  \n",
      "    78     0.415001   0.986096   0.645983                  \n",
      "    79     0.41835    0.985788   0.648199                  \n",
      "    80     0.4057     0.990017   0.645429                  \n",
      "    81     0.395109   0.999987   0.646537                  \n",
      "    82     0.392758   0.98307    0.655956                  \n",
      "    83     0.395603   0.991959   0.651524                  \n",
      "    84     0.393936   0.978381   0.651524                  \n",
      "    85     0.393636   0.97287    0.655956                  \n",
      "    86     0.381874   0.99562    0.651524                  \n",
      "    87     0.397144   0.990206   0.655402                  \n",
      "    88     0.386798   0.9903     0.645429                  \n",
      "    89     0.379938   0.990488   0.648199                  \n",
      "    90     0.383139   1.00116    0.654848                  \n",
      "    91     0.392466   0.99181    0.646537                  \n",
      "    92     0.394649   1.014701   0.645983                  \n",
      "    93     0.400133   1.015365   0.651524                  \n",
      "    94     0.388371   1.004391   0.655402                  \n",
      "    95     0.377812   1.010929   0.653186                  \n",
      "    96     0.376965   1.014578   0.652632                  \n",
      "    97     0.37154    1.024721   0.659834                  \n",
      "    98     0.365576   0.999049   0.652078                  \n",
      "    99     0.361746   1.022045   0.65651                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f648a50e3d4b51bba4e69462a1eb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ceb998d4564579af54788e35cbdaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.435978   1.191013   0.528532  \n",
      "    1      1.259555   1.126087   0.551801                 \n",
      "    2      1.168455   1.088987   0.564543                 \n",
      "    3      1.110963   1.064261   0.575069                 \n",
      "    4      1.082567   1.043541   0.578393                 \n",
      "    5      1.031799   1.027778   0.583934                 \n",
      "    6      0.990865   1.024378   0.58892                   \n",
      "    7      0.960475   0.986113   0.60277                   \n",
      "    8      0.936463   0.976815   0.607202                  \n",
      "    9      0.918873   0.985702   0.598338                  \n",
      "    10     0.902272   0.972813   0.606648                  \n",
      "    11     0.875664   0.958221   0.617729                  \n",
      "    12     0.854179   0.94199    0.621607                  \n",
      "    13     0.835289   0.949644   0.611634                  \n",
      "    14     0.818458   0.931321   0.618837                  \n",
      "    15     0.795712   0.957112   0.609972                  \n",
      "    16     0.78738    0.926048   0.629363                  \n",
      "    17     0.769915   0.923754   0.622715                  \n",
      "    18     0.749246   0.926788   0.627701                  \n",
      "    19     0.74143    0.927521   0.631025                  \n",
      "    20     0.740785   0.923712   0.634349                  \n",
      "    21     0.727862   0.916402   0.629917                  \n",
      "    22     0.707242   0.917615   0.636565                  \n",
      "    23     0.699137   0.931541   0.633795                  \n",
      "    24     0.711441   0.920545   0.636565                  \n",
      "    25     0.687518   0.912607   0.642105                  \n",
      "    26     0.678408   0.934277   0.629917                  \n",
      "    27     0.673999   0.916842   0.631579                  \n",
      "    28     0.6533     0.915482   0.637673                  \n",
      "    29     0.640298   0.915633   0.637119                  \n",
      "    30     0.631776   0.919362   0.637119                  \n",
      "    31     0.633547   0.922456   0.634349                  \n",
      "    32     0.621398   0.908834   0.630471                  \n",
      "    33     0.613338   0.914654   0.638781                  \n",
      "    34     0.617642   0.910053   0.642105                  \n",
      "    35     0.616722   0.906114   0.640997                  \n",
      "    36     0.598464   0.901492   0.651524                  \n",
      "    37     0.580304   0.903693   0.644875                  \n",
      "    38     0.583004   0.914055   0.643213                  \n",
      "    39     0.591325   0.911136   0.640997                  \n",
      "    40     0.582147   0.904862   0.645429                  \n",
      "    41     0.567942   0.913681   0.642105                  \n",
      "    42     0.555946   0.938775   0.649861                  \n",
      "    43     0.551298   0.921445   0.645983                  \n",
      "    44     0.54429    0.927416   0.647091                  \n",
      "    45     0.538552   0.937077   0.645429                  \n",
      "    46     0.5416     0.939092   0.649307                  \n",
      "    47     0.529791   0.926515   0.647091                  \n",
      "    48     0.527024   0.927234   0.655956                  \n",
      "    49     0.515934   0.932873   0.645429                  \n",
      "    50     0.51654    0.945534   0.645983                  \n",
      "    51     0.504288   0.934268   0.639335                  \n",
      "    52     0.49211    0.939695   0.642659                  \n",
      "    53     0.482654   0.951809   0.643213                  \n",
      "    54     0.486489   0.945325   0.642659                  \n",
      "    55     0.4794     0.938762   0.643213                  \n",
      "    56     0.487957   0.934755   0.646537                  \n",
      "    57     0.486152   0.939488   0.646537                  \n",
      "    58     0.478876   0.948073   0.65374                   \n",
      "    59     0.474805   0.942894   0.647645                  \n",
      "    60     0.468156   0.956828   0.658172                  \n",
      "    61     0.465492   0.957775   0.647645                  \n",
      "    62     0.455038   0.945042   0.659834                  \n",
      "    63     0.441933   0.949192   0.658726                  \n",
      "    64     0.446649   0.943719   0.660388                  \n",
      "    65     0.445066   0.957077   0.65374                   \n",
      "    66     0.444503   0.946152   0.652632                  \n",
      "    67     0.449055   0.961884   0.641551                  \n",
      "    68     0.446769   0.93883    0.654848                  \n",
      "    69     0.452343   0.952355   0.654294                  \n",
      "    70     0.447795   0.94796    0.652632                  \n",
      "    71     0.455872   0.970595   0.641551                  \n",
      "    72     0.428655   0.956489   0.65097                   \n",
      "    73     0.430272   0.961919   0.645429                  \n",
      "    74     0.424904   0.97022    0.65097                   \n",
      "    75     0.414673   0.974077   0.649861                  \n",
      "    76     0.41173    0.984592   0.650416                  \n",
      "    77     0.412605   0.97657    0.644875                  \n",
      "    78     0.409178   0.992757   0.650416                  \n",
      "    79     0.40725    0.967286   0.650416                  \n",
      "    80     0.412478   0.969846   0.651524                  \n",
      "    81     0.397021   0.972079   0.647645                  \n",
      "    82     0.392117   0.97383    0.65374                   \n",
      "    83     0.388008   0.983004   0.65928                   \n",
      "    84     0.387336   0.994372   0.648199                  \n",
      "    85     0.386367   0.994382   0.661496                  \n",
      "    86     0.385013   0.988534   0.65097                   \n",
      "    87     0.389253   0.995504   0.653186                  \n",
      "    88     0.379613   0.989716   0.651524                  \n",
      "    89     0.376241   0.98655    0.660942                  \n",
      "    90     0.3818     0.9917     0.658726                  \n",
      "    91     0.382623   0.990922   0.655402                  \n",
      "    92     0.384633   1.002335   0.654294                  \n",
      "    93     0.372887   1.017967   0.658726                  \n",
      "    94     0.370511   1.014488   0.65097                   \n",
      "    95     0.375832   1.008373   0.649307                  \n",
      "    96     0.382802   0.985953   0.647645                  \n",
      "    97     0.369573   1.008541   0.658172                  \n",
      "    98     0.362457   1.004841   0.648753                  \n",
      "    99     0.356595   0.995239   0.660388                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb991bd97a2d4626827788b1ed7150e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1bc3c8d5a44211a9ca7edd7117cd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.449755   1.204397   0.515789  \n",
      "    1      1.266147   1.130048   0.541274                 \n",
      "    2      1.182086   1.09911    0.563989                 \n",
      "    3      1.112089   1.070741   0.570083                 \n",
      "    4      1.066852   1.044317   0.582825                 \n",
      "    5      1.017027   1.038918   0.578947                 \n",
      "    6      0.987067   1.018178   0.576731                  \n",
      "    7      0.965997   1.00328    0.590028                  \n",
      "    8      0.932846   0.985203   0.595014                  \n",
      "    9      0.896889   0.994886   0.593352                  \n",
      "    10     0.886875   0.96919    0.598338                  \n",
      "    11     0.873911   0.964345   0.607202                  \n",
      "    12     0.852308   0.984549   0.590582                  \n",
      "    13     0.838381   0.97029    0.613296                  \n",
      "    14     0.822073   0.969307   0.603878                  \n",
      "    15     0.804821   0.945439   0.609418                  \n",
      "    16     0.788838   0.945968   0.617175                  \n",
      "    17     0.782533   0.938927   0.61385                   \n",
      "    18     0.764155   0.926819   0.621607                  \n",
      "    19     0.746368   0.943838   0.60554                   \n",
      "    20     0.744879   0.93374    0.632133                  \n",
      "    21     0.723375   0.959091   0.612188                  \n",
      "    22     0.70138    0.937831   0.627147                  \n",
      "    23     0.699822   0.934468   0.633795                  \n",
      "    24     0.694204   0.938308   0.624931                  \n",
      "    25     0.676527   0.925365   0.628809                  \n",
      "    26     0.67303    0.919738   0.627701                  \n",
      "    27     0.662947   0.918452   0.631025                  \n",
      "    28     0.646581   0.922354   0.628809                  \n",
      "    29     0.649968   0.951837   0.623269                  \n",
      "    30     0.652561   0.93236    0.631579                  \n",
      "    31     0.625934   0.933033   0.638227                  \n",
      "    32     0.619911   0.930667   0.628255                  \n",
      "    33     0.616994   0.916557   0.635457                  \n",
      "    34     0.611645   0.91369    0.637119                  \n",
      "    35     0.598988   0.906819   0.632133                  \n",
      "    36     0.597865   0.916097   0.631025                  \n",
      "    37     0.589609   0.942828   0.628809                  \n",
      "    38     0.584632   0.947825   0.627701                  \n",
      "    39     0.569144   0.928206   0.638227                  \n",
      "    40     0.573958   0.914242   0.633795                  \n",
      "    41     0.560389   0.926162   0.638781                  \n",
      "    42     0.558916   0.948929   0.633241                  \n",
      "    43     0.557485   0.93779    0.638227                  \n",
      "    44     0.557343   0.939958   0.631579                  \n",
      "    45     0.545852   0.946813   0.631579                  \n",
      "    46     0.535816   0.949775   0.631579                  \n",
      "    47     0.526468   0.954567   0.634903                  \n",
      "    48     0.524308   0.955366   0.640443                  \n",
      "    49     0.516003   0.951564   0.638227                  \n",
      "    50     0.516284   0.949923   0.637673                  \n",
      "    51     0.5051     0.959232   0.642659                  \n",
      "    52     0.516959   0.943652   0.649861                  \n",
      "    53     0.495963   0.958709   0.644321                  \n",
      "    54     0.49738    0.973534   0.640997                  \n",
      "    55     0.48758    0.976719   0.638227                  \n",
      "    56     0.493997   0.960703   0.637119                  \n",
      "    57     0.476177   0.961905   0.640443                  \n",
      "    58     0.463464   0.959547   0.642659                  \n",
      "    59     0.463831   0.960225   0.634349                  \n",
      "    60     0.464588   0.966379   0.633241                  \n",
      "    61     0.477195   0.964616   0.640443                  \n",
      "    62     0.472754   0.955319   0.639335                  \n",
      "    63     0.466821   0.965926   0.636565                  \n",
      "    64     0.4488     0.977734   0.639889                  \n",
      "    65     0.445791   0.965833   0.643213                  \n",
      "    66     0.443329   0.98342    0.640443                  \n",
      "    67     0.443385   0.960163   0.646537                  \n",
      "    68     0.449218   0.981624   0.641551                  \n",
      "    69     0.433297   0.970981   0.639889                  \n",
      "    70     0.436365   0.984373   0.640997                  \n",
      "    71     0.438935   0.960193   0.637673                  \n",
      "    72     0.423954   0.968221   0.641551                  \n",
      "    73     0.413615   0.989065   0.638781                  \n",
      "    74     0.408017   0.983956   0.643213                  \n",
      "    75     0.422156   0.984528   0.649861                  \n",
      "    76     0.409671   0.989647   0.647645                  \n",
      "    77     0.420076   0.999373   0.65097                   \n",
      "    78     0.418936   0.981616   0.648753                  \n",
      "    79     0.406757   1.000221   0.640443                  \n",
      "    80     0.394528   1.011969   0.644875                  \n",
      "    81     0.404853   1.003538   0.642105                  \n",
      "    82     0.402621   0.981743   0.647091                  \n",
      "    83     0.393886   1.00806    0.65097                   \n",
      "    84     0.375867   0.990789   0.642105                  \n",
      "    85     0.375715   1.005      0.645983                  \n",
      "    86     0.38154    1.000887   0.648753                  \n",
      "    87     0.384622   0.997535   0.645429                  \n",
      "    88     0.380357   1.010464   0.639889                  \n",
      "    89     0.375087   0.993464   0.648753                  \n",
      "    90     0.369983   1.013541   0.637673                  \n",
      "    91     0.364176   1.000667   0.647645                  \n",
      "    92     0.372396   1.018435   0.638227                  \n",
      "    93     0.376127   1.031407   0.638227                  \n",
      "    94     0.382003   1.010703   0.642105                  \n",
      "    95     0.384857   1.017407   0.642105                  \n",
      "    96     0.376221   1.006326   0.647091                  \n",
      "    97     0.377636   1.019412   0.649861                  \n",
      "    98     0.379023   1.012741   0.649861                  \n",
      "    99     0.376235   1.02183    0.644321                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7bbd824e4b4da69ad3c676e43d5f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30343a05a55f4598a5c44df8c6621453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.464083   1.208488   0.511357  \n",
      "    1      1.288544   1.14095    0.547922                 \n",
      "    2      1.179604   1.094628   0.559003                 \n",
      "    3      1.108967   1.106958   0.562881                 \n",
      "    4      1.072471   1.050951   0.589474                 \n",
      "    5      1.029483   1.024861   0.593906                 \n",
      "    6      0.993524   1.025369   0.585042                  \n",
      "    7      0.969257   1.002562   0.599446                  \n",
      "    8      0.937731   1.032065   0.59723                   \n",
      "    9      0.913873   0.989749   0.596122                  \n",
      "    10     0.899232   0.970221   0.604986                  \n",
      "    11     0.868562   0.967643   0.618283                  \n",
      "    12     0.85711    0.9885     0.604986                  \n",
      "    13     0.856812   0.957274   0.627147                  \n",
      "    14     0.838489   0.94693    0.615512                  \n",
      "    15     0.815874   0.929725   0.629363                  \n",
      "    16     0.800626   0.939121   0.620499                  \n",
      "    17     0.777498   0.933297   0.628255                  \n",
      "    18     0.77194    0.936384   0.627701                  \n",
      "    19     0.747608   0.915158   0.632133                  \n",
      "    20     0.731299   0.925903   0.637119                  \n",
      "    21     0.72047    0.925015   0.634349                  \n",
      "    22     0.710582   0.917558   0.629917                  \n",
      "    23     0.703559   0.927545   0.631579                  \n",
      "    24     0.681753   0.92381    0.632687                  \n",
      "    25     0.676238   0.906052   0.644875                  \n",
      "    26     0.664855   0.923831   0.641551                  \n",
      "    27     0.654568   0.908671   0.646537                  \n",
      "    28     0.645465   0.926959   0.642659                  \n",
      "    29     0.635778   0.92212    0.640997                  \n",
      "    30     0.628687   0.925422   0.638227                  \n",
      "    31     0.61872    0.925209   0.637119                  \n",
      "    32     0.627642   0.922833   0.648753                  \n",
      "    33     0.616555   0.904603   0.643767                  \n",
      "    34     0.610967   0.930865   0.640997                  \n",
      "    35     0.605164   0.921796   0.649307                  \n",
      "    36     0.596737   0.951782   0.636565                  \n",
      "    37     0.581634   0.932243   0.645429                  \n",
      "    38     0.58153    0.924175   0.641551                  \n",
      "    39     0.575478   0.926946   0.644875                  \n",
      "    40     0.575417   0.941711   0.640997                  \n",
      "    41     0.562344   0.942602   0.652078                  \n",
      "    42     0.554778   0.923328   0.649861                  \n",
      "    43     0.552051   0.945891   0.647645                  \n",
      "    44     0.558407   0.94054    0.639335                  \n",
      "    45     0.552429   0.925682   0.640443                  \n",
      "    46     0.544098   0.920702   0.654294                  \n",
      "    47     0.536677   0.952179   0.640997                  \n",
      "    48     0.529117   0.948436   0.646537                  \n",
      "    49     0.512558   0.943612   0.646537                  \n",
      "    50     0.521138   0.936194   0.649307                  \n",
      "    51     0.507269   0.928905   0.650416                  \n",
      "    52     0.503619   0.958845   0.647091                  \n",
      "    53     0.492347   0.959164   0.647645                  \n",
      "    54     0.503949   0.941922   0.645983                  \n",
      "    55     0.504081   0.944909   0.653186                  \n",
      "    56     0.506399   0.970116   0.648199                  \n",
      "    57     0.493969   0.967874   0.648753                  \n",
      "    58     0.490488   0.971932   0.642105                  \n",
      "    59     0.475533   0.956568   0.644875                  \n",
      "    60     0.457961   0.967205   0.646537                  \n",
      "    61     0.454463   0.962499   0.654294                  \n",
      "    62     0.461204   0.965953   0.645983                  \n",
      "    63     0.473336   0.976922   0.657064                  \n",
      "    64     0.459848   0.962026   0.648199                  \n",
      "    65     0.457876   0.964418   0.644875                  \n",
      "    66     0.447888   0.958155   0.652078                  \n",
      "    67     0.448942   0.94204    0.65374                   \n",
      "    68     0.438241   0.959849   0.65651                   \n",
      "    69     0.431073   0.962523   0.653186                  \n",
      "    70     0.440229   0.957709   0.647645                  \n",
      "    71     0.431736   0.970239   0.65097                   \n",
      "    72     0.419672   0.987025   0.654848                  \n",
      "    73     0.419123   1.001521   0.651524                  \n",
      "    74     0.408709   0.977179   0.655402                  \n",
      "    75     0.397369   0.965262   0.658726                  \n",
      "    76     0.402836   0.990157   0.65374                   \n",
      "    77     0.412007   0.991006   0.652632                  \n",
      "    78     0.402285   0.976953   0.65651                   \n",
      "    79     0.404754   0.973733   0.654294                  \n",
      "    80     0.407981   0.97361    0.654848                  \n",
      "    81     0.405211   0.995097   0.652078                  \n",
      "    82     0.416253   0.983224   0.651524                  \n",
      "    83     0.406677   0.982684   0.647645                  \n",
      "    84     0.402206   0.995007   0.652632                  \n",
      "    85     0.385053   1.008277   0.652078                  \n",
      "    86     0.381702   1.019023   0.648753                  \n",
      "    87     0.384065   0.99527    0.650416                  \n",
      "    88     0.3786     0.995542   0.657064                  \n",
      "    89     0.390036   0.987855   0.647091                  \n",
      "    90     0.390231   0.993076   0.652078                  \n",
      "    91     0.385485   0.989017   0.657618                  \n",
      "    92     0.382725   0.997037   0.654294                  \n",
      "    93     0.376823   1.01719    0.653186                  \n",
      "    94     0.389245   1.014357   0.644875                  \n",
      "    95     0.382603   1.01532    0.646537                  \n",
      "    96     0.383977   1.004806   0.651524                  \n",
      "    97     0.376301   1.024946   0.651524                  \n",
      "    98     0.373321   1.016475   0.648199                  \n",
      "    99     0.371814   1.017172   0.645983                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f829add9dc1f440e844201efd42083f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5da65343e2446e837f55a6536065fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.483433   1.192393   0.519114  \n",
      "    1      1.273407   1.127804   0.550139                 \n",
      "    2      1.178998   1.086767   0.565097                 \n",
      "    3      1.117812   1.0625     0.563989                 \n",
      "    4      1.063685   1.035295   0.588366                 \n",
      "    5      1.017804   1.018083   0.592244                 \n",
      "    6      0.990713   1.013706   0.593352                  \n",
      "    7      0.958264   0.991607   0.604986                  \n",
      "    8      0.936636   0.977431   0.603878                  \n",
      "    9      0.922905   0.964425   0.619391                  \n",
      "    10     0.89677    0.957924   0.614958                  \n",
      "    11     0.861026   0.960189   0.615512                  \n",
      "    12     0.847653   0.960656   0.618283                  \n",
      "    13     0.842672   0.960857   0.603878                  \n",
      "    14     0.82206    0.93711    0.630471                  \n",
      "    15     0.799239   0.944109   0.619391                  \n",
      "    16     0.790743   0.930068   0.631579                  \n",
      "    17     0.772354   0.937457   0.632687                  \n",
      "    18     0.7586     0.930438   0.623823                  \n",
      "    19     0.739859   0.927512   0.633241                  \n",
      "    20     0.732707   0.930878   0.626039                  \n",
      "    21     0.729982   0.933598   0.632133                  \n",
      "    22     0.716182   0.943232   0.629917                  \n",
      "    23     0.707374   0.929611   0.633795                  \n",
      "    24     0.694353   0.936243   0.638227                  \n",
      "    25     0.678989   0.912762   0.637673                  \n",
      "    26     0.677993   0.914719   0.639889                  \n",
      "    27     0.648873   0.912469   0.636565                  \n",
      "    28     0.650084   0.927402   0.635457                  \n",
      "    29     0.640907   0.930577   0.639889                  \n",
      "    30     0.637535   0.898747   0.648199                  \n",
      "    31     0.633759   0.91708    0.633795                  \n",
      "    32     0.617852   0.915976   0.643767                  \n",
      "    33     0.612861   0.923152   0.643767                  \n",
      "    34     0.612695   0.907851   0.645983                  \n",
      "    35     0.609529   0.924905   0.634903                  \n",
      "    36     0.615369   0.899021   0.655956                  \n",
      "    37     0.588203   0.923652   0.649861                  \n",
      "    38     0.577842   0.931008   0.649307                  \n",
      "    39     0.566993   0.935361   0.638781                  \n",
      "    40     0.555595   0.920968   0.642105                  \n",
      "    41     0.570163   0.926623   0.634903                  \n",
      "    42     0.551154   0.933177   0.647645                  \n",
      "    43     0.55186    0.926706   0.642105                  \n",
      "    44     0.559443   0.939065   0.647091                  \n",
      "    45     0.546573   0.930746   0.645429                  \n",
      "    46     0.544685   0.928042   0.642105                  \n",
      "    47     0.521682   0.935045   0.644321                  \n",
      "    48     0.511086   0.938734   0.647091                  \n",
      "    49     0.515645   0.932947   0.654848                  \n",
      "    50     0.525924   0.923508   0.653186                  \n",
      "    51     0.519416   0.936986   0.644875                  \n",
      "    52     0.50418    0.941113   0.643213                  \n",
      "    53     0.491832   0.934415   0.648753                  \n",
      "    54     0.496471   0.956316   0.648753                  \n",
      "    55     0.495794   0.944382   0.637673                  \n",
      "    56     0.48451    0.964861   0.645429                  \n",
      "    57     0.493415   0.975669   0.650416                  \n",
      "    58     0.483713   0.944631   0.648753                  \n",
      "    59     0.478293   0.972913   0.643767                  \n",
      "    60     0.480851   0.964845   0.647645                  \n",
      "    61     0.474786   0.963924   0.649862                  \n",
      "    62     0.472086   0.9729     0.647645                  \n",
      "    63     0.465718   0.964583   0.648753                  \n",
      "    64     0.463355   0.973582   0.648753                  \n",
      "    65     0.44852    0.982415   0.644875                  \n",
      "    66     0.448383   0.974495   0.648753                  \n",
      "    67     0.432849   0.978304   0.655402                  \n",
      "    68     0.440687   0.986079   0.655402                  \n",
      "    69     0.428688   0.973621   0.653186                  \n",
      "    70     0.429363   0.995354   0.647091                  \n",
      "    71     0.41941    0.973686   0.645429                  \n",
      "    72     0.422065   0.987521   0.654294                  \n",
      "    73     0.417252   0.99539    0.652078                  \n",
      "    74     0.419594   1.006408   0.648753                  \n",
      "    75     0.409081   0.987537   0.658726                  \n",
      "    76     0.408692   0.989163   0.643767                  \n",
      "    77     0.405571   1.011476   0.644875                  \n",
      "    78     0.412796   1.01048    0.642659                  \n",
      "    79     0.414618   1.002254   0.654848                  \n",
      "    80     0.40659    0.99792    0.645983                  \n",
      "    81     0.403247   1.009468   0.647645                  \n",
      "    82     0.39691    1.021506   0.649307                  \n",
      "    83     0.39152    1.025272   0.641551                  \n",
      "    84     0.383162   1.00602    0.652078                  \n",
      "    85     0.39919    0.992137   0.65374                   \n",
      "    86     0.398865   1.010136   0.65097                   \n",
      "    87     0.39399    0.992422   0.65097                   \n",
      "    88     0.389573   1.015802   0.652078                  \n",
      "    89     0.388713   1.001824   0.647645                  \n",
      "    90     0.384729   1.004486   0.648753                  \n",
      "    91     0.368818   1.012455   0.649307                  \n",
      "    92     0.378688   1.025247   0.645983                  \n",
      "    93     0.381424   1.030721   0.644321                  \n",
      "    94     0.385846   1.014798   0.650416                  \n",
      "    95     0.377375   1.018857   0.655402                  \n",
      "    96     0.367442   1.028768   0.645429                  \n",
      "    97     0.366156   1.026484   0.65374                   \n",
      "    98     0.357847   1.035544   0.651524                  \n",
      "    99     0.372136   1.026145   0.643767                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4855bc478607441685e9dce103f753a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a21435467364d6fbb1de6cfb3b116f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.485251   1.205142   0.516898  \n",
      "    1      1.287252   1.127601   0.544044                 \n",
      "    2      1.182445   1.080826   0.568421                 \n",
      "    3      1.112742   1.061405   0.580609                 \n",
      "    4      1.065648   1.035139   0.586704                 \n",
      "    5      1.027398   1.020375   0.584488                 \n",
      "    6      0.992094   0.998234   0.59169                   \n",
      "    7      0.95735    0.995158   0.595568                  \n",
      "    8      0.929153   0.97771    0.601662                  \n",
      "    9      0.905245   0.987492   0.603324                  \n",
      "    10     0.885759   0.967437   0.609972                  \n",
      "    11     0.879775   0.945648   0.61662                   \n",
      "    12     0.86067    0.950527   0.619391                  \n",
      "    13     0.840969   0.958225   0.604986                  \n",
      "    14     0.827933   0.938758   0.616621                  \n",
      "    15     0.806501   0.935663   0.615512                  \n",
      "    16     0.783202   0.929016   0.623269                  \n",
      "    17     0.770002   0.907611   0.638227                  \n",
      "    18     0.755452   0.921523   0.634349                  \n",
      "    19     0.75979    0.911948   0.635457                  \n",
      "    20     0.734521   0.926001   0.635457                  \n",
      "    21     0.725189   0.93229    0.625485                  \n",
      "    22     0.712599   0.919019   0.631579                  \n",
      "    23     0.706471   0.916651   0.634903                  \n",
      "    24     0.691652   0.921677   0.638227                  \n",
      "    25     0.673254   0.923651   0.630471                  \n",
      "    26     0.661779   0.915427   0.633241                  \n",
      "    27     0.658737   0.929312   0.635457                  \n",
      "    28     0.663443   0.921899   0.640443                  \n",
      "    29     0.64282    0.920162   0.639335                  \n",
      "    30     0.63878    0.920963   0.639335                  \n",
      "    31     0.631112   0.922369   0.630471                  \n",
      "    32     0.618398   0.920138   0.632133                  \n",
      "    33     0.615068   0.929174   0.634903                  \n",
      "    34     0.614758   0.914672   0.643767                  \n",
      "    35     0.59875    0.923272   0.636011                  \n",
      "    36     0.589186   0.917688   0.642659                  \n",
      "    37     0.588767   0.907612   0.648753                  \n",
      "    38     0.583481   0.916775   0.640997                  \n",
      "    39     0.591872   0.92458    0.639335                  \n",
      "    40     0.57698    0.937352   0.640443                  \n",
      "    41     0.564263   0.939965   0.644321                  \n",
      "    42     0.552392   0.931825   0.628809                  \n",
      "    43     0.558145   0.939543   0.637119                  \n",
      "    44     0.547904   0.930687   0.638227                  \n",
      "    45     0.538184   0.9386     0.640443                  \n",
      "    46     0.532106   0.947728   0.633795                  \n",
      "    47     0.527735   0.938106   0.643213                  \n",
      "    48     0.512777   0.942644   0.644321                  \n",
      "    49     0.521179   0.9325     0.643767                  \n",
      "    50     0.513302   0.94562    0.643767                  \n",
      "    51     0.516319   0.949939   0.642105                  \n",
      "    52     0.513416   0.938314   0.642105                  \n",
      "    53     0.501311   0.945274   0.638227                  \n",
      "    54     0.479588   0.971722   0.641551                  \n",
      "    55     0.480153   0.948011   0.643213                  \n",
      "    56     0.481164   0.949787   0.650416                  \n",
      "    57     0.483399   0.95072    0.637119                  \n",
      "    58     0.475981   0.968982   0.645429                  \n",
      "    59     0.46787    0.945548   0.639335                  \n",
      "    60     0.460482   0.974825   0.647645                  \n",
      "    61     0.457378   0.966327   0.643767                  \n",
      "    62     0.459282   0.952825   0.646537                  \n",
      "    63     0.455782   0.961291   0.639335                  \n",
      "    64     0.452879   0.96257    0.652078                  \n",
      "    65     0.452461   0.953304   0.653186                  \n",
      "    66     0.448564   0.966151   0.65097                   \n",
      "    67     0.455546   0.948737   0.653186                  \n",
      "    68     0.435951   0.955871   0.651524                  \n",
      "    69     0.42496    0.987238   0.647091                  \n",
      "    70     0.441521   0.963144   0.657064                  \n",
      "    71     0.440938   0.975816   0.643767                  \n",
      "    72     0.434883   0.966527   0.650416                  \n",
      "    73     0.426764   0.958859   0.643213                  \n",
      "    74     0.416508   0.973066   0.649307                  \n",
      "    75     0.427558   0.964159   0.655956                  \n",
      "    76     0.4261     0.952073   0.651524                  \n",
      "    77     0.422305   0.966116   0.650416                  \n",
      "    78     0.412159   0.964417   0.649861                  \n",
      "    79     0.415803   0.979308   0.65651                   \n",
      "    80     0.413174   0.959822   0.655956                  \n",
      "    81     0.400575   0.968524   0.649861                  \n",
      "    82     0.396108   0.986103   0.650416                  \n",
      "    83     0.403736   0.968983   0.650416                  \n",
      "    84     0.415189   0.975197   0.652078                  \n",
      "    85     0.398185   0.972495   0.652632                  \n",
      "    86     0.401428   0.986796   0.657064                  \n",
      "    87     0.386799   0.987867   0.649307                  \n",
      "    88     0.38993    0.98589    0.655956                  \n",
      "    89     0.376877   1.00394    0.658172                  \n",
      "    90     0.379211   1.005332   0.648199                  \n",
      "    91     0.37981    0.991358   0.645429                  \n",
      "    92     0.374028   0.981475   0.652078                  \n",
      "    93     0.363239   0.991831   0.654294                  \n",
      "    94     0.360626   0.999408   0.650416                  \n",
      "    95     0.364191   0.995911   0.652632                  \n",
      "    96     0.364539   1.011307   0.652078                  \n",
      "    97     0.356508   0.995255   0.645429                  \n",
      "    98     0.355585   1.007385   0.652632                  \n",
      "    99     0.356381   1.012471   0.653186                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ef3ef7708f4fccba5d30a338f5a35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9338e69c5af8428cb41a72ec0cd42267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.448337   1.17817    0.522438  \n",
      "    1      1.262692   1.122295   0.53795                  \n",
      "    2      1.179152   1.086029   0.560111                 \n",
      "    3      1.115934   1.064727   0.565651                 \n",
      "    4      1.054823   1.041695   0.583379                 \n",
      "    5      1.021591   1.017115   0.583934                 \n",
      "    6      1.001829   1.003367   0.585042                  \n",
      "    7      0.968975   1.00414    0.595568                  \n",
      "    8      0.934482   0.972935   0.610526                  \n",
      "    9      0.901933   0.977588   0.609418                  \n",
      "    10     0.885743   0.959408   0.616066                  \n",
      "    11     0.864866   0.956279   0.596676                  \n",
      "    12     0.836384   0.956098   0.607756                  \n",
      "    13     0.825167   0.947683   0.618283                  \n",
      "    14     0.802258   0.958186   0.614958                  \n",
      "    15     0.784135   0.94501    0.612188                  \n",
      "    16     0.774044   0.929308   0.61385                   \n",
      "    17     0.763068   0.932618   0.622715                  \n",
      "    18     0.750054   0.931887   0.628809                  \n",
      "    19     0.737701   0.937459   0.633241                  \n",
      "    20     0.726806   0.926448   0.622161                  \n",
      "    21     0.722479   0.92682    0.624931                  \n",
      "    22     0.709723   0.927156   0.639889                  \n",
      "    23     0.704368   0.922653   0.628255                  \n",
      "    24     0.697088   0.918582   0.630471                  \n",
      "    25     0.690349   0.924558   0.631579                  \n",
      "    26     0.679156   0.91995    0.642105                  \n",
      "    27     0.662688   0.919937   0.640997                  \n",
      "    28     0.661757   0.904985   0.636565                  \n",
      "    29     0.643035   0.930481   0.636011                  \n",
      "    30     0.626406   0.926578   0.634903                  \n",
      "    31     0.610565   0.93322    0.636011                  \n",
      "    32     0.614408   0.927208   0.639335                  \n",
      "    33     0.613728   0.935456   0.642105                  \n",
      "    34     0.604668   0.925386   0.637673                  \n",
      "    35     0.602907   0.924697   0.644875                  \n",
      "    36     0.590101   0.92975    0.643213                  \n",
      "    37     0.574776   0.924307   0.638227                  \n",
      "    38     0.580963   0.934261   0.638227                  \n",
      "    39     0.579817   0.948255   0.633795                  \n",
      "    40     0.572289   0.939929   0.641551                  \n",
      "    41     0.563385   0.93965    0.640443                  \n",
      "    42     0.560158   0.948364   0.640443                  \n",
      "    43     0.56916    0.947477   0.636011                  \n",
      "    44     0.557851   0.937269   0.645429                  \n",
      "    45     0.552502   0.933946   0.638781                  \n",
      "    46     0.545914   0.940109   0.643213                  \n",
      "    47     0.528823   0.946077   0.649307                  \n",
      "    48     0.525555   0.941326   0.654294                  \n",
      "    49     0.512888   0.952642   0.645983                  \n",
      "    50     0.506926   0.944575   0.645429                  \n",
      "    51     0.497985   0.933389   0.640443                  \n",
      "    52     0.502265   0.93879    0.646537                  \n",
      "    53     0.507718   0.942663   0.651524                  \n",
      "    54     0.504804   0.945061   0.644321                  \n",
      "    55     0.503832   0.949564   0.645429                  \n",
      "    56     0.487787   0.937227   0.644321                  \n",
      "    57     0.487252   0.945496   0.652078                  \n",
      "    58     0.490245   0.94926    0.65374                   \n",
      "    59     0.474378   0.965251   0.648199                  \n",
      "    60     0.466015   0.953824   0.648753                  \n",
      "    61     0.464661   0.950437   0.648753                  \n",
      "    62     0.456049   0.965991   0.643213                  \n",
      "    63     0.454203   0.97956    0.645983                  \n",
      "    64     0.459568   0.956811   0.648753                  \n",
      "    65     0.453895   0.968531   0.647091                  \n",
      "    66     0.449185   0.955694   0.648753                  \n",
      "    67     0.436058   0.96238    0.640997                  \n",
      "    68     0.427071   0.979013   0.646537                  \n",
      "    69     0.431196   0.961825   0.648753                  \n",
      "    70     0.423607   0.978582   0.642659                  \n",
      "    71     0.423526   0.983212   0.645983                  \n",
      "    72     0.424025   0.958845   0.65097                   \n",
      "    73     0.427805   0.974028   0.647091                  \n",
      "    74     0.41949    0.98165    0.644321                  \n",
      "    75     0.412803   0.979614   0.65097                   \n",
      "    76     0.406432   0.967656   0.65374                   \n",
      "    77     0.397296   0.993205   0.654294                  \n",
      "    78     0.406796   0.982021   0.640997                  \n",
      "    79     0.415952   0.988376   0.65097                   \n",
      "    80     0.413772   0.99176    0.652632                  \n",
      "    81     0.417385   1.010971   0.651524                  \n",
      "    82     0.414953   0.992063   0.644875                  \n",
      "    83     0.393238   0.997131   0.651524                  \n",
      "    84     0.397432   0.994956   0.654848                  \n",
      "    85     0.392028   0.983522   0.660388                  \n",
      "    86     0.388011   0.990329   0.651524                  \n",
      "    87     0.395617   1.007809   0.648199                  \n",
      "    88     0.390914   1.001068   0.657618                  \n",
      "    89     0.382092   1.011057   0.653186                  \n",
      "    90     0.391286   1.013999   0.652078                  \n",
      "    91     0.387596   0.997014   0.652078                  \n",
      "    92     0.3861     0.997968   0.651524                  \n",
      "    93     0.383589   1.01791    0.649307                  \n",
      "    94     0.377862   1.011646   0.65097                   \n",
      "    95     0.373992   1.013521   0.649307                  \n",
      "    96     0.378213   1.015791   0.65651                   \n",
      "    97     0.391942   1.042202   0.653186                  \n",
      "    98     0.399129   1.016964   0.652632                  \n",
      "    99     0.386373   1.020048   0.647645                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e180f6d2e8f44e3a8c8e7fd8bf263d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b28eee6df7e4d059e3cff3288c471d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.46901    1.193592   0.515789  \n",
      "    1      1.27166    1.13195    0.545706                 \n",
      "    2      1.161394   1.096765   0.554571                 \n",
      "    3      1.110665   1.068394   0.559003                 \n",
      "    4      1.0682     1.045939   0.568421                 \n",
      "    5      1.012707   1.020403   0.58338                  \n",
      "    6      0.9899     1.00296    0.596122                  \n",
      "    7      0.957521   0.999634   0.59723                   \n",
      "    8      0.932254   1.009313   0.588366                  \n",
      "    9      0.906389   0.990916   0.598892                  \n",
      "    10     0.880872   0.974144   0.609418                  \n",
      "    11     0.860778   0.97046    0.598338                  \n",
      "    12     0.849024   0.966147   0.604986                  \n",
      "    13     0.830127   0.960109   0.606648                  \n",
      "    14     0.80753    0.967436   0.61662                   \n",
      "    15     0.799862   0.961381   0.61662                   \n",
      "    16     0.803755   0.942405   0.611634                  \n",
      "    17     0.780663   0.940891   0.612742                  \n",
      "    18     0.776587   0.917572   0.626039                  \n",
      "    19     0.757582   0.932625   0.624931                  \n",
      "    20     0.751445   0.940235   0.631579                  \n",
      "    21     0.728683   0.930769   0.630471                  \n",
      "    22     0.719889   0.922621   0.631579                  \n",
      "    23     0.717638   0.9193     0.631025                  \n",
      "    24     0.696757   0.927546   0.634903                  \n",
      "    25     0.694779   0.918516   0.639889                  \n",
      "    26     0.663924   0.915348   0.638781                  \n",
      "    27     0.650456   0.909169   0.639335                  \n",
      "    28     0.64491    0.914794   0.637119                  \n",
      "    29     0.649985   0.912217   0.642105                  \n",
      "    30     0.649117   0.916241   0.637673                  \n",
      "    31     0.643997   0.919851   0.643767                  \n",
      "    32     0.643462   0.921089   0.642659                  \n",
      "    33     0.637481   0.925226   0.636565                  \n",
      "    34     0.619532   0.913396   0.642659                  \n",
      "    35     0.619716   0.909848   0.637119                  \n",
      "    36     0.604575   0.907807   0.640997                  \n",
      "    37     0.606525   0.916366   0.643213                  \n",
      "    38     0.589706   0.928706   0.637119                  \n",
      "    39     0.579804   0.91924    0.638781                  \n",
      "    40     0.568569   0.920364   0.643213                  \n",
      "    41     0.557911   0.91772    0.647091                  \n",
      "    42     0.553727   0.931478   0.644321                  \n",
      "    43     0.557674   0.935688   0.639889                  \n",
      "    44     0.556844   0.924151   0.646537                  \n",
      "    45     0.544869   0.928169   0.640443                  \n",
      "    46     0.543482   0.911052   0.644875                  \n",
      "    47     0.532276   0.92504    0.639335                  \n",
      "    48     0.524629   0.934619   0.643213                  \n",
      "    49     0.517055   0.940061   0.644875                  \n",
      "    50     0.507395   0.964309   0.640443                  \n",
      "    51     0.497343   0.944138   0.648753                  \n",
      "    52     0.495159   0.944754   0.654848                  \n",
      "    53     0.491951   0.945969   0.646537                  \n",
      "    54     0.482578   0.945212   0.643767                  \n",
      "    55     0.49157    0.949069   0.652078                  \n",
      "    56     0.486623   0.937361   0.647645                  \n",
      "    57     0.499369   0.952253   0.646537                  \n",
      "    58     0.495419   0.923579   0.652632                  \n",
      "    59     0.491674   0.932556   0.648199                  \n",
      "    60     0.482942   0.958697   0.650416                  \n",
      "    61     0.478404   0.947666   0.645429                  \n",
      "    62     0.475818   0.954943   0.648199                  \n",
      "    63     0.471231   0.947416   0.640997                  \n",
      "    64     0.465321   0.942188   0.653186                  \n",
      "    65     0.455607   0.956303   0.647645                  \n",
      "    66     0.448501   0.965513   0.648753                  \n",
      "    67     0.449736   0.944405   0.649861                  \n",
      "    68     0.443944   0.952207   0.652632                  \n",
      "    69     0.443474   0.957316   0.645429                  \n",
      "    70     0.429311   0.960966   0.647645                  \n",
      "    71     0.418784   0.980599   0.651524                  \n",
      "    72     0.417263   0.960674   0.654848                  \n",
      "    73     0.417753   0.96704    0.642659                  \n",
      "    74     0.413053   0.976037   0.648753                  \n",
      "    75     0.403293   0.976112   0.65374                   \n",
      "    76     0.407136   0.966677   0.646537                  \n",
      "    77     0.419197   0.976394   0.65374                   \n",
      "    78     0.414503   0.998344   0.643213                  \n",
      "    79     0.399964   0.966525   0.646537                  \n",
      "    80     0.398868   0.978148   0.648199                  \n",
      "    81     0.395942   0.991555   0.647645                  \n",
      "    82     0.411393   0.987864   0.643767                  \n",
      "    83     0.399879   0.991342   0.643213                  \n",
      "    84     0.391275   0.983666   0.654848                  \n",
      "    85     0.424703   0.995287   0.643213                  \n",
      "    86     0.408983   0.983703   0.648753                  \n",
      "    87     0.397827   0.986832   0.652632                  \n",
      "    88     0.386402   0.998393   0.655956                  \n",
      "    89     0.382945   0.994468   0.651524                  \n",
      "    90     0.38413    1.002198   0.649861                  \n",
      "    91     0.387775   0.993528   0.653186                  \n",
      "    92     0.38122    0.985351   0.645983                  \n",
      "    93     0.391398   1.011341   0.648199                  \n",
      "    94     0.392422   0.999956   0.645429                  \n",
      "    95     0.390877   0.990125   0.651524                  \n",
      "    96     0.381051   1.009839   0.65097                   \n",
      "    97     0.374861   0.991886   0.655402                  \n",
      "    98     0.380525   0.991458   0.65928                   \n",
      "    99     0.383478   0.978465   0.657064                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a936cc394f46c89abd4d815f14bbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c413be46e54641b1fe5ef08a3fccc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.450675   1.198882   0.511357  \n",
      "    1      1.252108   1.123809   0.553463                 \n",
      "    2      1.159139   1.105156   0.566205                 \n",
      "    3      1.095903   1.085495   0.555125                 \n",
      "    4      1.043053   1.046639   0.580055                 \n",
      "    5      1.010764   1.035548   0.576177                 \n",
      "    6      0.989334   1.020852   0.586704                  \n",
      "    7      0.963382   0.989686   0.597784                  \n",
      "    8      0.931072   0.994938   0.586704                  \n",
      "    9      0.909348   0.982661   0.60554                   \n",
      "    10     0.896112   0.986347   0.606094                  \n",
      "    11     0.875287   0.956724   0.609418                  \n",
      "    12     0.848471   0.960649   0.609418                  \n",
      "    13     0.825577   0.953573   0.614958                  \n",
      "    14     0.812466   0.956719   0.60554                   \n",
      "    15     0.792842   0.955655   0.617175                  \n",
      "    16     0.780507   0.954884   0.61385                   \n",
      "    17     0.767728   0.930761   0.616066                  \n",
      "    18     0.742904   0.95078    0.619945                  \n",
      "    19     0.747207   0.941052   0.619945                  \n",
      "    20     0.729848   0.930341   0.621053                  \n",
      "    21     0.719039   0.932898   0.634903                  \n",
      "    22     0.711243   0.940631   0.632687                  \n",
      "    23     0.699488   0.929594   0.622715                  \n",
      "    24     0.691703   0.936722   0.621053                  \n",
      "    25     0.684104   0.92271    0.632687                  \n",
      "    26     0.677836   0.918654   0.629363                  \n",
      "    27     0.67146    0.92436    0.628809                  \n",
      "    28     0.651652   0.932087   0.632687                  \n",
      "    29     0.634145   0.938116   0.634903                  \n",
      "    30     0.639903   0.942573   0.633795                  \n",
      "    31     0.625602   0.920999   0.627147                  \n",
      "    32     0.617933   0.924287   0.626039                  \n",
      "    33     0.622291   0.930657   0.635457                  \n",
      "    34     0.609631   0.923189   0.640443                  \n",
      "    35     0.611839   0.921351   0.634903                  \n",
      "    36     0.594947   0.923751   0.637119                  \n",
      "    37     0.582666   0.923137   0.643213                  \n",
      "    38     0.578689   0.93188    0.641551                  \n",
      "    39     0.576504   0.947816   0.642105                  \n",
      "    40     0.571776   0.933992   0.636565                  \n",
      "    41     0.560477   0.932108   0.638227                  \n",
      "    42     0.556418   0.932497   0.642659                  \n",
      "    43     0.546413   0.928358   0.643213                  \n",
      "    44     0.526002   0.943575   0.649861                  \n",
      "    45     0.534027   0.936844   0.638781                  \n",
      "    46     0.531866   0.938901   0.637119                  \n",
      "    47     0.519259   0.951376   0.638781                  \n",
      "    48     0.515001   0.93773    0.647091                  \n",
      "    49     0.511012   0.940841   0.634349                  \n",
      "    50     0.517037   0.935836   0.647645                  \n",
      "    51     0.512032   0.968134   0.639335                  \n",
      "    52     0.507953   0.94344    0.643767                  \n",
      "    53     0.491221   0.948662   0.647091                  \n",
      "    54     0.488086   0.943791   0.647091                  \n",
      "    55     0.475041   0.951618   0.643213                  \n",
      "    56     0.473506   0.974572   0.639335                  \n",
      "    57     0.466106   0.959663   0.644875                  \n",
      "    58     0.475619   0.961248   0.643767                  \n",
      "    59     0.489726   0.96925    0.640443                  \n",
      "    60     0.470717   0.949578   0.648199                  \n",
      "    61     0.460501   0.964273   0.65097                   \n",
      "    62     0.456911   0.970814   0.644875                  \n",
      "    63     0.448995   0.983662   0.645983                  \n",
      "    64     0.46273    0.971812   0.644875                  \n",
      "    65     0.45075    0.980693   0.647091                  \n",
      "    66     0.435227   0.962117   0.645429                  \n",
      "    67     0.436271   0.965118   0.648753                  \n",
      "    68     0.444937   0.981361   0.654848                  \n",
      "    69     0.433163   1.014952   0.639889                  \n",
      "    70     0.421663   0.980033   0.652078                  \n",
      "    71     0.416958   0.973239   0.653186                  \n",
      "    72     0.407144   0.995883   0.643213                  \n",
      "    73     0.415036   0.992136   0.642105                  \n",
      "    74     0.423273   1.01934    0.645983                  \n",
      "    75     0.417388   0.989498   0.641551                  \n",
      "    76     0.404697   0.980387   0.644321                  \n",
      "    77     0.408615   1.010735   0.648199                  \n",
      "    78     0.409477   0.995548   0.639889                  \n",
      "    79     0.409128   1.002013   0.652078                  \n",
      "    80     0.397873   1.012521   0.646537                  \n",
      "    81     0.402794   1.007263   0.646537                  \n",
      "    82     0.404804   1.017276   0.649307                  \n",
      "    83     0.431063   1.011662   0.648199                  \n",
      "    84     0.417152   1.001229   0.644875                  \n",
      "    85     0.407206   1.008198   0.642105                  \n",
      "    86     0.389937   1.014096   0.648199                  \n",
      "    87     0.386995   0.989908   0.651524                  \n",
      "    88     0.394736   1.009566   0.65097                   \n",
      "    89     0.393386   1.005828   0.652632                  \n",
      "    90     0.386483   1.029283   0.644321                  \n",
      "    91     0.377711   1.01008    0.651524                  \n",
      "    92     0.382989   1.009271   0.652632                  \n",
      "    93     0.378631   1.003995   0.649307                  \n",
      "    94     0.370031   1.042867   0.644875                  \n",
      "    95     0.368594   1.026602   0.640997                  \n",
      "    96     0.365342   1.037389   0.645429                  \n",
      "    97     0.362685   1.018662   0.648753                  \n",
      "    98     0.359857   1.035504   0.638781                  \n",
      "    99     0.36033    1.028564   0.643767                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94bb28da97b466c8f56de0983d2c28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d59e37f75e40f1b3edbe82b79f8af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.448377   1.19827    0.512465  \n",
      "    1      1.260635   1.139456   0.53518                  \n",
      "    2      1.160187   1.087548   0.555125                 \n",
      "    3      1.106586   1.08163    0.568975                 \n",
      "    4      1.059978   1.053443   0.572853                 \n",
      "    5      1.024749   1.023159   0.59169                  \n",
      "    6      0.987546   1.00249    0.59446                   \n",
      "    7      0.953851   0.989516   0.598892                  \n",
      "    8      0.9244     1.002914   0.592798                  \n",
      "    9      0.910369   0.984229   0.590582                  \n",
      "    10     0.886778   0.967978   0.604432                  \n",
      "    11     0.865767   0.971817   0.615512                  \n",
      "    12     0.84273    0.951153   0.608864                  \n",
      "    13     0.826171   0.956957   0.623269                  \n",
      "    14     0.802958   0.953071   0.618837                  \n",
      "    15     0.7815     0.946917   0.622715                  \n",
      "    16     0.789674   0.959311   0.622715                  \n",
      "    17     0.77862    0.952137   0.623823                  \n",
      "    18     0.775583   0.932608   0.624931                  \n",
      "    19     0.760918   0.913802   0.633795                  \n",
      "    20     0.74474    0.910561   0.631579                  \n",
      "    21     0.725784   0.914535   0.636565                  \n",
      "    22     0.718145   0.926735   0.632133                  \n",
      "    23     0.704721   0.920954   0.636011                  \n",
      "    24     0.680305   0.933463   0.630471                  \n",
      "    25     0.672712   0.92388    0.639889                  \n",
      "    26     0.673289   0.922138   0.633795                  \n",
      "    27     0.663253   0.924933   0.642105                  \n",
      "    28     0.660203   0.932741   0.631579                  \n",
      "    29     0.648187   0.915344   0.634349                  \n",
      "    30     0.643511   0.913106   0.631579                  \n",
      "    31     0.626421   0.926108   0.638227                  \n",
      "    32     0.625832   0.937268   0.637119                  \n",
      "    33     0.613149   0.916119   0.637119                  \n",
      "    34     0.607326   0.921686   0.633795                  \n",
      "    35     0.600274   0.919846   0.628809                  \n",
      "    36     0.594291   0.932197   0.643767                  \n",
      "    37     0.60449    0.918276   0.640443                  \n",
      "    38     0.584056   0.925745   0.639335                  \n",
      "    39     0.585946   0.919293   0.641551                  \n",
      "    40     0.566442   0.942418   0.632687                  \n",
      "    41     0.549918   0.918934   0.642105                  \n",
      "    42     0.54694    0.928539   0.640997                  \n",
      "    43     0.552777   0.919231   0.644321                  \n",
      "    44     0.545287   0.914416   0.644875                  \n",
      "    45     0.54389    0.925762   0.645983                  \n",
      "    46     0.54776    0.923505   0.642659                  \n",
      "    47     0.541341   0.927611   0.646537                  \n",
      "    48     0.533307   0.932359   0.640997                  \n",
      "    49     0.522859   0.924215   0.650416                  \n",
      "    50     0.527502   0.951499   0.646537                  \n",
      "    51     0.529144   0.922669   0.655402                  \n",
      "    52     0.52038    0.949874   0.647645                  \n",
      "    53     0.509662   0.929277   0.649307                  \n",
      "    54     0.513277   0.928404   0.649861                  \n",
      "    55     0.485856   0.930715   0.649861                  \n",
      "    56     0.482139   0.939155   0.650416                  \n",
      "    57     0.482619   0.94361    0.642659                  \n",
      "    58     0.48353    0.958476   0.644321                  \n",
      "    59     0.475595   0.963407   0.641551                  \n",
      "    60     0.468943   0.955145   0.651524                  \n",
      "    61     0.46027    0.952897   0.643767                  \n",
      "    62     0.453894   0.950098   0.648753                  \n",
      "    63     0.453297   0.969346   0.639889                  \n",
      "    64     0.44825    0.970803   0.644321                  \n",
      "    65     0.451287   0.960209   0.648199                  \n",
      "    66     0.441288   0.966818   0.650415                  \n",
      "    67     0.449096   0.951865   0.65374                   \n",
      "    68     0.442074   0.967503   0.644875                  \n",
      "    69     0.434128   0.970111   0.644321                  \n",
      "    70     0.445164   0.969786   0.648753                  \n",
      "    71     0.437907   0.971497   0.65097                   \n",
      "    72     0.417563   0.993024   0.650416                  \n",
      "    73     0.42935    0.969151   0.652632                  \n",
      "    74     0.401908   0.99046    0.653186                  \n",
      "    75     0.408983   0.996001   0.65374                   \n",
      "    76     0.425175   0.987932   0.646537                  \n",
      "    77     0.417753   0.987649   0.653186                  \n",
      "    78     0.434251   0.977233   0.644321                  \n",
      "    79     0.431089   0.985146   0.649307                  \n",
      "    80     0.413646   0.976029   0.647645                  \n",
      "    81     0.413535   1.003425   0.644875                  \n",
      "    82     0.404379   0.980832   0.652078                  \n",
      "    83     0.401362   0.978775   0.65374                   \n",
      "    84     0.40565    0.992009   0.648753                  \n",
      "    85     0.409937   0.986012   0.645983                  \n",
      "    86     0.392651   0.982961   0.643213                  \n",
      "    87     0.388276   0.992477   0.644321                  \n",
      "    88     0.386213   0.994918   0.644321                  \n",
      "    89     0.386668   0.982595   0.647091                  \n",
      "    90     0.372517   0.977311   0.652632                  \n",
      "    91     0.372165   0.97761    0.657064                  \n",
      "    92     0.374996   0.988406   0.65097                   \n",
      "    93     0.379286   0.99553    0.644875                  \n",
      "    94     0.372486   1.003063   0.654294                  \n",
      "    95     0.382665   1.00717    0.647645                  \n",
      "    96     0.381507   0.988216   0.649861                  \n",
      "    97     0.359558   0.993184   0.655956                  \n",
      "    98     0.35945    0.992035   0.648753                  \n",
      "    99     0.362396   1.002829   0.648199                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "v_a = k_fold_cross_loop1(10,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean=0.6500767\n",
      "stdev.=0.005884115754299874\n"
     ]
    }
   ],
   "source": [
    "valacc = [0.65651,0.6604,0.64432,0.64598,0.6437,0.653186,0.647645,0.65706,0.643767,0.648199]\n",
    "print('mean='+str(np.mean(valacc)))\n",
    "print('stdev.='+str(np.std(valacc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-93a52ce5cdd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_preds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTTA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "log_preds,y = learn.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "\n",
    "preds = np.argmax(probs, axis=1)\n",
    "probs = probs[:,1]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, preds)\n",
    "adj = cm.transpose()/cm.sum(axis=1)\n",
    "\n",
    "adj = adj.round(2)\n",
    "plot_confusion_matrix(adj.transpose(), data.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26dd54e8cd5045c7b3df24a2272b7159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = get_data(sz, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.pretrained(arch, data, precompute=True, ps=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801f474c3543454e8ac480ca05f9e2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.381579   0.977774   0.652632  \n",
      "    1      0.362424   0.979615   0.650416                  \n",
      "    2      0.363948   0.988909   0.649861                  \n",
      "    3      0.371648   0.974583   0.65097                   \n",
      "    4      0.375591   0.997509   0.655956                  \n",
      "    5      0.38825    0.979243   0.652078                  \n",
      "    6      0.370741   0.976403   0.652632                  \n",
      "    7      0.364548   0.977965   0.65097                   \n",
      "    8      0.361835   0.991816   0.65097                   \n",
      "    9      0.374712   0.993832   0.648199                  \n",
      "    10     0.374453   1.003261   0.657064                  \n",
      "    11     0.373121   0.992282   0.655402                  \n",
      "    12     0.379622   0.970048   0.652078                  \n",
      "    13     0.372552   0.979644   0.65374                   \n",
      "    14     0.366194   0.98446    0.650416                  \n",
      "    15     0.357217   0.97559    0.654848                  \n",
      "    16     0.367217   0.995288   0.657618                  \n",
      "    17     0.354719   0.987417   0.652632                  \n",
      "    18     0.354297   0.998557   0.65651                   \n",
      "    19     0.348514   1.002145   0.65651                   \n",
      "    20     0.353746   1.004252   0.65374                   \n",
      "    21     0.347103   0.999897   0.657064                  \n",
      "    22     0.358462   1.004756   0.650416                  \n",
      "    23     0.36355    1.006885   0.652078                  \n",
      "    24     0.360651   0.997289   0.659834                  \n",
      "    25     0.349684   0.991195   0.65651                   \n",
      "    26     0.368047   0.995972   0.657618                  \n",
      "    27     0.367903   0.981868   0.657064                  \n",
      "    28     0.356925   0.979577   0.654848                  \n",
      "    29     0.351817   1.004359   0.657618                  \n",
      "    30     0.350166   0.999509   0.65374                   \n",
      "    31     0.350283   1.008886   0.652078                  \n",
      "    32     0.356237   1.022287   0.653186                  \n",
      "    33     0.353529   1.012957   0.648199                  \n",
      "    34     0.355483   1.010619   0.655956                  \n",
      "    35     0.367336   1.012873   0.657064                  \n",
      "    36     0.358024   1.004782   0.65097                   \n",
      "    37     0.364167   1.026238   0.652078                  \n",
      "    38     0.348876   1.010022   0.653186                  \n",
      "    39     0.358784   1.007721   0.645983                  \n",
      "    40     0.353255   1.012021   0.657618                  \n",
      "    41     0.358672   1.019067   0.66205                   \n",
      "    42     0.342431   1.019918   0.654294                  \n",
      "    43     0.335284   1.018226   0.652632                  \n",
      "    44     0.331366   1.007987   0.660388                  \n",
      "    45     0.329019   1.010862   0.65651                   \n",
      "    46     0.333239   1.019547   0.66205                   \n",
      "    47     0.337776   1.003999   0.652632                  \n",
      "    48     0.340542   1.006596   0.657064                  \n",
      "    49     0.329715   1.015622   0.655402                  \n",
      "    50     0.342144   1.027904   0.65374                   \n",
      "    51     0.344797   1.025695   0.658726                  \n",
      "    52     0.342938   1.030947   0.658172                  \n",
      "    53     0.327748   1.026016   0.658172                  \n",
      "    54     0.341724   1.037434   0.65374                   \n",
      "    55     0.352514   1.026964   0.653186                  \n",
      "    56     0.347975   1.00516    0.657064                  \n",
      "    57     0.338956   1.01984    0.65097                   \n",
      "    58     0.356872   1.02193    0.658172                  \n",
      "    59     0.34812    1.006024   0.65651                   \n",
      "    60     0.338867   1.016752   0.652078                  \n",
      "    61     0.333995   1.011331   0.655956                  \n",
      "    62     0.336842   1.020443   0.649307                  \n",
      "    63     0.329685   1.024212   0.65928                   \n",
      "    64     0.327427   1.030103   0.654848                  \n",
      "    65     0.33484    1.009674   0.650416                  \n",
      "    66     0.338013   1.03058    0.65651                   \n",
      "    67     0.337891   1.024229   0.65651                   \n",
      "    68     0.341156   1.034824   0.655402                  \n",
      "    69     0.33586    1.026077   0.65928                   \n",
      "    70     0.339677   1.021572   0.653186                  \n",
      "    71     0.337647   1.027971   0.654848                  \n",
      "    72     0.332639   1.024983   0.655956                  \n",
      "    73     0.324251   1.029351   0.652632                  \n",
      "    74     0.326665   1.025729   0.658172                  \n",
      "    75     0.332919   1.031987   0.65097                   \n",
      "    76     0.329074   1.024374   0.65097                   \n",
      "    77     0.326968   1.022382   0.648753                  \n",
      "    78     0.322839   1.021882   0.648753                  \n",
      "    79     0.323834   1.027083   0.654294                  \n",
      "    80     0.326153   1.0297     0.657618                  \n",
      "    81     0.321985   1.039788   0.657064                  \n",
      "    82     0.325477   1.022094   0.65928                   \n",
      "    83     0.326325   1.039168   0.660942                  \n",
      "    84     0.330688   1.024065   0.65928                   \n",
      "    85     0.321114   1.027292   0.659834                  \n",
      "    86     0.328828   1.03549    0.657618                  \n",
      "    87     0.324553   1.109727   0.65374                   \n",
      "    88     0.329298   1.037692   0.657064                  \n",
      "    89     0.323122   1.048212   0.661496                  \n",
      "    90     0.315133   1.045627   0.652632                  \n",
      "    91     0.313809   1.041689   0.658172                  \n",
      "    92     0.318606   1.020986   0.657618                  \n",
      "    93     0.330578   1.037496   0.658172                  \n",
      "    94     0.326829   1.038179   0.655956                  \n",
      "    95     0.324992   1.030432   0.657618                  \n",
      "    96     0.320991   1.031454   0.65651                   \n",
      "    97     0.311481   1.03749    0.65374                   \n",
      "    98     0.319573   1.036577   0.657618                  \n",
      "    99     0.316209   1.037554   0.653186                  \n",
      "   100     0.315271   1.038981   0.658172                  \n",
      "   101     0.305333   1.054026   0.652632                  \n",
      "   102     0.308544   1.049374   0.652078                  \n",
      "   103     0.310953   1.036196   0.652632                  \n",
      "   104     0.311395   1.025592   0.65651                   \n",
      "   105     0.31566    1.040047   0.652632                  \n",
      "   106     0.314159   1.04667    0.649861                  \n",
      "   107     0.313798   1.050347   0.653186                  \n",
      "   108     0.313638   1.035193   0.655956                  \n",
      "   109     0.309492   1.025632   0.65651                   \n",
      "   110     0.307838   1.037992   0.655402                  \n",
      "   111     0.306574   1.039788   0.65928                   \n",
      "   112     0.312022   1.040849   0.658726                  \n",
      "   113     0.303005   1.045968   0.654294                  \n",
      "   114     0.308187   1.043033   0.659834                  \n",
      "   115     0.306375   1.040795   0.65928                   \n",
      "   116     0.308008   1.039658   0.658172                  \n",
      "   117     0.308299   1.039146   0.655402                  \n",
      "   118     0.306448   1.04099    0.660388                  \n",
      "   119     0.31765    1.035074   0.657618                  \n",
      "   120     0.317948   1.06369    0.660942                  \n",
      "   121     0.331013   1.03167    0.662604                  \n",
      "   122     0.331199   1.038304   0.65651                   \n",
      "   123     0.321669   1.040471   0.661496                  \n",
      "   124     0.312446   1.040925   0.659834                  \n",
      "   125     0.311102   1.039092   0.659834                  \n",
      "   126     0.311412   1.035517   0.659834                  \n",
      "   127     0.304642   1.033537   0.657618                  \n",
      "   128     0.306091   1.044957   0.658726                  \n",
      "   129     0.307785   1.043349   0.654848                  \n",
      "   130     0.305728   1.036895   0.65651                   \n",
      "   131     0.304174   1.034805   0.658726                  \n",
      "   132     0.309941   1.057204   0.657064                  \n",
      "   133     0.312089   1.049551   0.65651                   \n",
      "   134     0.32011    1.044133   0.658172                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135     0.302131   1.048499   0.655956                  \n",
      "   136     0.304496   1.037207   0.655402                  \n",
      "   137     0.309104   1.039357   0.655956                  \n",
      "   138     0.312144   1.0509     0.659834                  \n",
      "   139     0.307933   1.032783   0.660942                  \n",
      "   140     0.301661   1.046336   0.657618                  \n",
      "   141     0.31559    1.039839   0.65928                   \n",
      "   142     0.308919   1.066562   0.655956                  \n",
      "   143     0.303886   1.058882   0.657064                  \n",
      "   144     0.301976   1.0549     0.653186                  \n",
      "   145     0.309965   1.047844   0.649861                  \n",
      "   146     0.313892   1.047496   0.658726                  \n",
      "   147     0.301082   1.03653    0.65928                   \n",
      "   148     0.314161   1.068915   0.654848                  \n",
      "   149     0.318754   1.068736   0.658726                  \n",
      "   150     0.317649   1.048159   0.657618                  \n",
      "   151     0.308812   1.05318    0.65651                   \n",
      "   152     0.310427   1.052682   0.654848                  \n",
      "   153     0.30597    1.051117   0.65651                   \n",
      "   154     0.319761   1.039835   0.658172                  \n",
      "   155     0.311991   1.054751   0.65651                   \n",
      "   156     0.316174   1.062865   0.658172                  \n",
      "   157     0.304336   1.046398   0.654294                  \n",
      "   158     0.304832   1.064644   0.65651                   \n",
      "   159     0.304083   1.05456    0.660942                  \n",
      "   160     0.303683   1.072354   0.657618                  \n",
      "   161     0.300904   1.045907   0.658172                  \n",
      "   162     0.29738    1.063584   0.657618                  \n",
      "   163     0.299792   1.073468   0.657618                  \n",
      "   164     0.305608   1.064957   0.657618                  \n",
      "   165     0.312879   1.074316   0.658172                  \n",
      "   166     0.301272   1.061091   0.658726                  \n",
      "   167     0.307111   1.071597   0.66205                   \n",
      "   168     0.304383   1.082017   0.657064                  \n",
      "   169     0.292386   1.07584    0.655956                  \n",
      "   170     0.300308   1.086042   0.65374                   \n",
      "   171     0.292464   1.068735   0.658172                  \n",
      "   172     0.302638   1.070737   0.649307                  \n",
      "   173     0.308562   1.071252   0.658726                  \n",
      "   174     0.310189   1.051694   0.659834                  \n",
      "   175     0.297662   1.067959   0.657064                  \n",
      "   176     0.294479   1.064651   0.658172                  \n",
      "   177     0.3001     1.061225   0.658726                  \n",
      "   178     0.299375   1.06205    0.657064                  \n",
      "   179     0.293782   1.065219   0.655956                  \n",
      "   180     0.289312   1.063823   0.657618                  \n",
      "   181     0.293097   1.065419   0.659834                  \n",
      "   182     0.28564    1.062721   0.66482                   \n",
      "   183     0.290059   1.07179    0.657064                  \n",
      "   184     0.283753   1.076834   0.657618                  \n",
      "   185     0.289785   1.090073   0.661496                  \n",
      "   186     0.286027   1.079506   0.659834                  \n",
      "   187     0.28839    1.081259   0.65374                   \n",
      "   188     0.282993   1.081655   0.65651                   \n",
      "   189     0.295731   1.067249   0.657064                  \n",
      "   190     0.315521   1.085924   0.65928                   \n",
      "   191     0.304107   1.061552   0.655956                  \n",
      "   192     0.306289   1.073863   0.652078                  \n",
      "   193     0.303623   1.074346   0.652632                  \n",
      "   194     0.300916   1.080381   0.657064                  \n",
      "   195     0.297206   1.069621   0.657064                  \n",
      "   196     0.295005   1.07075    0.654294                  \n",
      "   197     0.297177   1.08909    0.655956                  \n",
      "   198     0.29348    1.084514   0.66205                   \n",
      "   199     0.287017   1.064643   0.655402                  \n",
      "   200     0.290521   1.071403   0.65374                   \n",
      "   201     0.296003   1.083123   0.658726                  \n",
      "   202     0.297621   1.089479   0.657618                  \n",
      "   203     0.290006   1.063997   0.661496                  \n",
      "   204     0.291533   1.070191   0.660388                  \n",
      "   205     0.284247   1.086938   0.66205                   \n",
      "   206     0.275228   1.066453   0.65651                   \n",
      "   207     0.276859   1.077693   0.657064                  \n",
      "   208     0.294497   1.076877   0.65651                   \n",
      "   209     0.292351   1.086124   0.660942                  \n",
      "   210     0.287869   1.097387   0.65651                   \n",
      "   211     0.285297   1.083073   0.660942                  \n",
      "   212     0.283586   1.085473   0.665374                  \n",
      "   213     0.290389   1.077412   0.663712                  \n",
      "   214     0.288314   1.077937   0.66205                   \n",
      "   215     0.281669   1.074184   0.657618                  \n",
      "   216     0.290261   1.086388   0.657064                  \n",
      "   217     0.294874   1.087196   0.652632                  \n",
      "   218     0.29253    1.092027   0.655956                  \n",
      "   219     0.295205   1.089705   0.65928                   \n",
      "   220     0.286876   1.092735   0.66482                   \n",
      "   221     0.287915   1.093041   0.654848                  \n",
      "   222     0.290431   1.091038   0.657618                  \n",
      "   223     0.28915    1.088877   0.660942                  \n",
      "   224     0.292343   1.085732   0.65651                   \n",
      "   225     0.278847   1.072425   0.658172                  \n",
      "   226     0.27881    1.091444   0.660388                  \n",
      "   227     0.276591   1.08074    0.658172                  \n",
      "   228     0.286848   1.094925   0.65928                   \n",
      "   229     0.291617   1.095039   0.654848                  \n",
      "   230     0.283621   1.086169   0.661496                  \n",
      "   231     0.293759   1.083758   0.655402                  \n",
      "   232     0.299012   1.075908   0.663712                  \n",
      "   233     0.291381   1.090044   0.659834                  \n",
      "   234     0.282387   1.085501   0.66205                   \n",
      "   235     0.294833   1.082205   0.660942                  \n",
      "   236     0.298854   1.088982   0.661496                  \n",
      "   237     0.29311    1.085986   0.65651                   \n",
      "   238     0.284247   1.081174   0.657618                  \n",
      "   239     0.283185   1.086405   0.657064                  \n",
      "   240     0.290705   1.099526   0.657064                  \n",
      "   241     0.28191    1.093703   0.65928                   \n",
      "   242     0.284576   1.101609   0.654848                  \n",
      "   243     0.291409   1.086538   0.655956                  \n",
      "   244     0.281227   1.097097   0.655956                  \n",
      "   245     0.291429   1.085831   0.652632                  \n",
      "   246     0.279935   1.084472   0.658172                  \n",
      "   247     0.285346   1.109452   0.654294                  \n",
      "   248     0.282304   1.094968   0.654294                  \n",
      "   249     0.287299   1.102173   0.657064                  \n",
      "   250     0.296155   1.096724   0.657064                  \n",
      "   251     0.29933    1.086391   0.655402                  \n",
      "   252     0.290695   1.095008   0.655402                  \n",
      "   253     0.282685   1.085664   0.654294                  \n",
      "   254     0.28137    1.095673   0.65374                   \n",
      "   255     0.302546   1.087929   0.655402                  \n",
      "   256     0.288812   1.079275   0.659834                  \n",
      "   257     0.301465   1.079763   0.661496                  \n",
      "   258     0.299148   1.086135   0.661496                  \n",
      "   259     0.297511   1.087867   0.653186                  \n",
      "   260     0.290478   1.088386   0.653186                  \n",
      "   261     0.280961   1.069113   0.657064                  \n",
      "   262     0.297698   1.065653   0.660942                  \n",
      "   263     0.287058   1.074633   0.659834                  \n",
      "   264     0.290225   1.070647   0.661496                  \n",
      "   265     0.275576   1.0725     0.65651                   \n",
      "   266     0.27593    1.103007   0.654848                  \n",
      "   267     0.282452   1.080969   0.655956                  \n",
      "   268     0.283906   1.108653   0.650416                  \n",
      "   269     0.281765   1.098782   0.65374                   \n",
      "   270     0.293498   1.094664   0.65097                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   271     0.274488   1.089899   0.658172                  \n",
      "   272     0.278951   1.08695    0.652078                  \n",
      "   273     0.281021   1.090979   0.652632                  \n",
      "   274     0.273799   1.076919   0.654848                  \n",
      "   275     0.274681   1.072251   0.654848                  \n",
      "   276     0.272769   1.08456    0.655956                  \n",
      "   277     0.284091   1.085562   0.654848                  \n",
      "   278     0.27002    1.086529   0.654294                  \n",
      "   279     0.276945   1.07707    0.657064                  \n",
      "   280     0.284735   1.077788   0.655402                  \n",
      "   281     0.282628   1.093159   0.65651                   \n",
      "   282     0.287715   1.081808   0.658172                  \n",
      "   283     0.290332   1.065614   0.65651                   \n",
      "   284     0.290318   1.062093   0.658726                  \n",
      "   285     0.290602   1.093878   0.655956                  \n",
      "   286     0.286211   1.088038   0.658172                  \n",
      "   287     0.27633    1.084798   0.655956                  \n",
      "   288     0.277194   1.087129   0.660388                  \n",
      "   289     0.277278   1.086162   0.657064                  \n",
      "   290     0.284009   1.098074   0.658172                  \n",
      "   291     0.282566   1.105444   0.661496                  \n",
      "   292     0.285777   1.095848   0.659834                  \n",
      "   293     0.272345   1.088537   0.660388                  \n",
      "   294     0.273252   1.104801   0.662604                  \n",
      "   295     0.264644   1.098292   0.658726                  \n",
      "   296     0.27779    1.107063   0.65928                   \n",
      "   297     0.275838   1.094802   0.66482                   \n",
      "   298     0.271233   1.108849   0.65928                   \n",
      "   299     0.277109   1.110437   0.658172                  \n",
      "   300     0.279578   1.105637   0.658172                  \n",
      "   301     0.268013   1.083623   0.657618                  \n",
      "   302     0.266312   1.09894    0.659834                  \n",
      "   303     0.272674   1.104611   0.65928                   \n",
      "   304     0.269649   1.101036   0.654848                  \n",
      "   305     0.270996   1.09218    0.658172                  \n",
      "   306     0.270583   1.096034   0.657064                  \n",
      "   307     0.278372   1.089954   0.652632                  \n",
      "   308     0.271837   1.089043   0.65097                   \n",
      "   309     0.26657    1.093126   0.655402                  \n",
      "   310     0.275102   1.109667   0.657618                  \n",
      "   311     0.270263   1.092115   0.65374                   \n",
      "   312     0.279164   1.097982   0.652632                  \n",
      "   313     0.278004   1.088314   0.658172                  \n",
      "   314     0.265649   1.098137   0.65651                   \n",
      "   315     0.272408   1.107155   0.661496                  \n",
      "   316     0.277526   1.093092   0.658726                  \n",
      "   317     0.272404   1.103593   0.660942                  \n",
      "   318     0.274185   1.110061   0.657618                  \n",
      "   319     0.277441   1.099199   0.65374                   \n",
      "   320     0.276132   1.106579   0.66205                   \n",
      "   321     0.270964   1.087125   0.659834                  \n",
      "   322     0.271803   1.084252   0.668144                  \n",
      "   323     0.271042   1.076652   0.66205                   \n",
      "   324     0.279052   1.083188   0.664266                  \n",
      "   325     0.283873   1.074277   0.66205                   \n",
      "   326     0.28412    1.090045   0.664266                  \n",
      "   327     0.27738    1.07577    0.660942                  \n",
      "   328     0.276489   1.072559   0.66759                   \n",
      "   329     0.296905   1.074665   0.66205                   \n",
      "   330     0.277609   1.089692   0.65928                   \n",
      "   331     0.278059   1.082477   0.661496                  \n",
      "   332     0.267768   1.077635   0.660942                  \n",
      "   333     0.262413   1.091277   0.661496                  \n",
      "   334     0.267342   1.114524   0.658726                  \n",
      "   335     0.270245   1.090984   0.658172                  \n",
      "   336     0.272728   1.10192    0.66205                   \n",
      "   337     0.265751   1.10807    0.65928                   \n",
      "   338     0.273198   1.091345   0.660388                  \n",
      "   339     0.275638   1.111611   0.659834                  \n",
      "   340     0.272625   1.091318   0.662604                  \n",
      "   341     0.272269   1.102047   0.657064                  \n",
      "   342     0.268537   1.114144   0.65651                   \n",
      "   343     0.269784   1.100072   0.661496                  \n",
      "   344     0.27773    1.094345   0.662604                  \n",
      "   345     0.275957   1.100787   0.662604                  \n",
      "   346     0.266232   1.102053   0.65928                   \n",
      "   347     0.263575   1.107103   0.65928                   \n",
      "   348     0.265303   1.104238   0.658726                  \n",
      "   349     0.266833   1.122655   0.652078                  \n",
      "   350     0.271377   1.109113   0.655956                  \n",
      "   351     0.265484   1.111498   0.660388                  \n",
      "   352     0.270283   1.118094   0.657618                  \n",
      "   353     0.265102   1.106565   0.659834                  \n",
      "   354     0.274124   1.125804   0.664266                  \n",
      "   355     0.260417   1.1218     0.663712                  \n",
      "   356     0.254888   1.122006   0.662604                  \n",
      "   357     0.280578   1.121077   0.659834                  \n",
      "   358     0.277589   1.097767   0.660942                  \n",
      "   359     0.269865   1.106753   0.663158                  \n",
      "   360     0.266602   1.092849   0.663158                  \n",
      "   361     0.262479   1.089815   0.665928                  \n",
      "   362     0.259594   1.102569   0.669252                  \n",
      "   363     0.271697   1.119198   0.66205                   \n",
      "   364     0.275056   1.106373   0.658726                  \n",
      "   365     0.279959   1.092435   0.661496                  \n",
      "   366     0.279603   1.092468   0.65928                   \n",
      "   367     0.268767   1.087827   0.66205                   \n",
      "   368     0.28011    1.085033   0.660942                  \n",
      "   369     0.274563   1.104991   0.659834                  \n",
      "   370     0.278012   1.084779   0.660942                  \n",
      "   371     0.264111   1.118659   0.661496                  \n",
      "   372     0.276536   1.102536   0.662604                  \n",
      "   373     0.274027   1.099815   0.66482                   \n",
      "   374     0.276854   1.100928   0.663712                  \n",
      "   375     0.270868   1.086197   0.66482                   \n",
      "   376     0.272202   1.104123   0.665374                  \n",
      "   377     0.28204    1.11953    0.663712                  \n",
      "   378     0.279101   1.097369   0.661496                  \n",
      "   379     0.27464    1.09654    0.661496                  \n",
      "   380     0.271395   1.111043   0.663712                  \n",
      "   381     0.264219   1.099793   0.66205                   \n",
      "   382     0.259303   1.106988   0.662604                  \n",
      "   383     0.266399   1.116279   0.659834                  \n",
      "   384     0.271988   1.101653   0.658726                  \n",
      "   385     0.262691   1.088007   0.65928                   \n",
      "   386     0.262442   1.097812   0.661496                  \n",
      "   387     0.263789   1.110164   0.663712                  \n",
      "   388     0.267774   1.10668    0.660388                  \n",
      "   389     0.272512   1.090332   0.659834                  \n",
      "   390     0.262762   1.107078   0.659834                  \n",
      "   391     0.264402   1.077134   0.65928                   \n",
      "   392     0.271683   1.086095   0.663712                  \n",
      "   393     0.267974   1.079672   0.660942                  \n",
      "   394     0.266745   1.081757   0.661496                  \n",
      "   395     0.264137   1.09381    0.66205                   \n",
      "   396     0.269631   1.093819   0.66205                   \n",
      "   397     0.273199   1.089776   0.660942                  \n",
      "   398     0.270699   1.089789   0.657064                  \n",
      "   399     0.273302   1.088212   0.663158                  \n",
      "   400     0.277283   1.097547   0.663158                  \n",
      "   401     0.270054   1.088023   0.662604                  \n",
      "   402     0.267303   1.09926    0.660942                  \n",
      "   403     0.267094   1.089215   0.660942                  \n",
      "   404     0.261967   1.094109   0.65928                   \n",
      "   405     0.256865   1.097517   0.663158                  \n",
      "   406     0.258459   1.081011   0.661496                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   407     0.257119   1.094365   0.657618                  \n",
      "   408     0.260886   1.093307   0.658172                  \n",
      "   409     0.256483   1.082794   0.66205                   \n",
      "   410     0.25957    1.095834   0.660388                  \n",
      "   411     0.265248   1.090835   0.660942                  \n",
      "   412     0.27267    1.091967   0.662604                  \n",
      "   413     0.26994    1.091017   0.659834                  \n",
      "   414     0.260466   1.101549   0.660388                  \n",
      "   415     0.261771   1.112733   0.65928                   \n",
      "   416     0.261383   1.114799   0.660942                  \n",
      "   417     0.262625   1.11041    0.659834                  \n",
      "   418     0.256523   1.122247   0.654848                  \n",
      "   419     0.260066   1.119649   0.660942                  \n",
      "   420     0.257761   1.123025   0.663712                  \n",
      "   421     0.260087   1.107542   0.661496                  \n",
      "   422     0.267938   1.114301   0.660942                  \n",
      "   423     0.262047   1.110315   0.660388                  \n",
      "   424     0.25847    1.111375   0.662604                  \n",
      "   425     0.258635   1.107237   0.658726                  \n",
      "   426     0.266525   1.106611   0.662604                  \n",
      "   427     0.267911   1.108309   0.659834                  \n",
      "   428     0.272861   1.11519    0.661496                  \n",
      "   429     0.269073   1.109477   0.663158                  \n",
      "   430     0.259025   1.101656   0.657618                  \n",
      "   431     0.270263   1.109303   0.661496                  \n",
      "   432     0.264059   1.099192   0.660942                  \n",
      "   433     0.25782    1.111401   0.660388                  \n",
      "   434     0.25488    1.117223   0.66205                   \n",
      "   435     0.260008   1.10731    0.660942                  \n",
      "   436     0.268366   1.10299    0.663158                  \n",
      "   437     0.26211    1.10215    0.663158                  \n",
      "   438     0.254404   1.115359   0.664266                  \n",
      "   439     0.255959   1.115371   0.660388                  \n",
      "   440     0.263082   1.116385   0.663712                  \n",
      "   441     0.262123   1.111078   0.665374                  \n",
      "   442     0.264761   1.109514   0.668144                  \n",
      "   443     0.265375   1.120337   0.662604                  \n",
      "   444     0.26525    1.13271    0.657618                  \n",
      "   445     0.261137   1.117551   0.658172                  \n",
      "   446     0.262938   1.120804   0.65928                   \n",
      "   447     0.26718    1.112895   0.660388                  \n",
      "   448     0.258389   1.130092   0.659834                  \n",
      "   449     0.26235    1.129963   0.658172                  \n",
      "   450     0.263409   1.123926   0.659834                  \n",
      "   451     0.265486   1.13658    0.664266                  \n",
      "   452     0.257822   1.129025   0.664266                  \n",
      "   453     0.250169   1.129418   0.65928                   \n",
      "   454     0.270857   1.12504    0.659834                  \n",
      "   455     0.261885   1.119375   0.662604                  \n",
      "   456     0.25821    1.125407   0.662604                  \n",
      "   457     0.257872   1.119678   0.66482                   \n",
      "   458     0.260458   1.103564   0.665374                  \n",
      "   459     0.256464   1.115944   0.666482                  \n",
      "   460     0.25584    1.11731    0.66482                   \n",
      "   461     0.255117   1.121792   0.668144                  \n",
      "   462     0.256415   1.12476    0.664266                  \n",
      "   463     0.256579   1.129681   0.67036                   \n",
      "   464     0.253622   1.10339    0.66759                   \n",
      "   465     0.253264   1.105642   0.663712                  \n",
      "   466     0.249406   1.113157   0.664266                  \n",
      "   467     0.247995   1.117105   0.668144                  \n",
      "   468     0.256196   1.12481    0.665374                  \n",
      "   469     0.263536   1.117997   0.670914                  \n",
      "   470     0.265065   1.130924   0.663712                  \n",
      "   471     0.259505   1.129785   0.663712                  \n",
      "   472     0.265641   1.105472   0.665374                  \n",
      "   473     0.25812    1.133036   0.662604                  \n",
      "   474     0.261763   1.132089   0.666482                  \n",
      "   475     0.265646   1.124252   0.667036                  \n",
      "   476     0.262858   1.127732   0.668144                  \n",
      "   477     0.253653   1.136464   0.65928                   \n",
      "   478     0.255396   1.142016   0.664266                  \n",
      "   479     0.264153   1.111114   0.66482                   \n",
      "   480     0.263407   1.139662   0.66205                   \n",
      "   481     0.266958   1.1369     0.657618                  \n",
      "   482     0.265094   1.142123   0.66205                   \n",
      "   483     0.266169   1.136786   0.662604                  \n",
      "   484     0.267526   1.11831    0.667036                  \n",
      "   485     0.284212   1.121733   0.659834                  \n",
      "   486     0.270785   1.141297   0.660942                  \n",
      "   487     0.27535    1.114266   0.663158                  \n",
      "   488     0.25734    1.115428   0.661496                  \n",
      "   489     0.260392   1.119714   0.658726                  \n",
      "   490     0.258197   1.1248     0.659834                  \n",
      "   491     0.259111   1.13752    0.663712                  \n",
      "   492     0.261622   1.141523   0.657064                  \n",
      "   493     0.24874    1.138753   0.660942                  \n",
      "   494     0.249963   1.131762   0.65928                   \n",
      "   495     0.253936   1.126119   0.658726                  \n",
      "   496     0.250495   1.146321   0.662604                  \n",
      "   497     0.247747   1.138722   0.658172                  \n",
      "   498     0.250631   1.142632   0.658172                  \n",
      "   499     0.253971   1.159133   0.658172                  \n",
      "   500     0.254322   1.143232   0.657064                  \n",
      "   501     0.251361   1.157295   0.654848                  \n",
      "   502     0.248817   1.157427   0.658726                  \n",
      "   503     0.246473   1.146613   0.65651                   \n",
      "   504     0.251812   1.153429   0.660388                  \n",
      "   505     0.258328   1.153248   0.655402                  \n",
      "   506     0.257832   1.159451   0.655956                  \n",
      "   507     0.263495   1.174877   0.657064                  \n",
      "   508     0.254963   1.161225   0.655402                  \n",
      "   509     0.247659   1.158032   0.658172                  \n",
      "   510     0.255361   1.15807    0.65928                   \n",
      "   511     0.245094   1.165138   0.661496                  \n",
      "   512     0.24512    1.161701   0.65651                   \n",
      "   513     0.24478    1.151436   0.661496                  \n",
      "   514     0.250003   1.140169   0.660942                  \n",
      "   515     0.248368   1.147675   0.661496                  \n",
      "   516     0.2454     1.137248   0.660388                  \n",
      "   517     0.249485   1.144176   0.657618                  \n",
      "   518     0.242785   1.152833   0.65928                   \n",
      "   519     0.239809   1.158919   0.657618                  \n",
      "   520     0.251685   1.167626   0.659834                  \n",
      "   521     0.249281   1.158988   0.659834                  \n",
      "   522     0.25259    1.156502   0.655956                  \n",
      "   523     0.248572   1.160327   0.65651                   \n",
      "   524     0.248103   1.152925   0.660388                  \n",
      "   525     0.264664   1.149613   0.657618                  \n",
      "   526     0.257993   1.173411   0.655956                  \n",
      "   527     0.259191   1.167254   0.654848                  \n",
      "   528     0.24798    1.154608   0.654848                  \n",
      "   529     0.246706   1.15609    0.655402                  \n",
      "   530     0.254885   1.136327   0.65928                   \n",
      "   531     0.255123   1.142245   0.654294                  \n",
      "   532     0.254342   1.137408   0.655956                  \n",
      "   533     0.245014   1.151454   0.657618                  \n",
      "   534     0.243228   1.162135   0.655402                  \n",
      "   535     0.253829   1.14325    0.663712                  \n",
      "   536     0.254194   1.148194   0.655956                  \n",
      "   537     0.243111   1.139803   0.658726                  \n",
      "   538     0.242279   1.148883   0.65651                   \n",
      "   539     0.247757   1.151841   0.654294                  \n",
      "   540     0.242812   1.14788    0.654294                  \n",
      "   541     0.242296   1.154928   0.657618                  \n",
      "   542     0.241888   1.165705   0.660942                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   543     0.249842   1.143816   0.660942                  \n",
      "   544     0.243487   1.145889   0.663158                  \n",
      "   545     0.246567   1.14141    0.657618                  \n",
      "   546     0.247439   1.153994   0.659834                  \n",
      "   547     0.242045   1.148695   0.664266                  \n",
      "   548     0.241566   1.133472   0.660942                  \n",
      "   549     0.243321   1.131676   0.66205                   \n",
      "   550     0.246008   1.132066   0.660942                  \n",
      "   551     0.251169   1.139823   0.657618                  \n",
      "   552     0.246319   1.168876   0.658172                  \n",
      "   553     0.251232   1.163611   0.660942                  \n",
      "   554     0.250612   1.157071   0.661496                  \n",
      "   555     0.255529   1.163597   0.660942                  \n",
      "   556     0.253638   1.150667   0.657064                  \n",
      "   557     0.247509   1.145745   0.658172                  \n",
      "   558     0.251132   1.128768   0.655956                  \n",
      "   559     0.252195   1.131849   0.65928                   \n",
      "   560     0.244075   1.132011   0.659834                  \n",
      "   561     0.254509   1.158586   0.662604                  \n",
      "   562     0.250869   1.136418   0.655402                  \n",
      "   563     0.251866   1.14365    0.65651                   \n",
      "   564     0.248531   1.135079   0.65928                   \n",
      "   565     0.253309   1.142795   0.659834                  \n",
      "   566     0.244455   1.107125   0.660942                  \n",
      "   567     0.242597   1.142216   0.65651                   \n",
      "   568     0.249528   1.14749    0.659834                  \n",
      "   569     0.250115   1.138658   0.66205                   \n",
      "   570     0.251673   1.147809   0.657618                  \n",
      "   571     0.249448   1.142829   0.658726                  \n",
      "   572     0.249989   1.149155   0.657618                  \n",
      "   573     0.252487   1.148052   0.660942                  \n",
      "   574     0.257095   1.161303   0.663712                  \n",
      "   575     0.251823   1.156973   0.663712                  \n",
      "   576     0.249357   1.15825    0.657618                  \n",
      "   577     0.249603   1.16028    0.660942                  \n",
      "   578     0.247987   1.152522   0.663712                  \n",
      "   579     0.255902   1.153181   0.660942                  \n",
      "   580     0.251497   1.152562   0.660942                  \n",
      "   581     0.251556   1.151617   0.66205                   \n",
      "   582     0.246476   1.159533   0.660388                  \n",
      "   583     0.252274   1.17005    0.65651                   \n",
      "   584     0.248707   1.156477   0.661496                  \n",
      "   585     0.247355   1.154871   0.662604                  \n",
      "   586     0.250208   1.138441   0.658726                  \n",
      "   587     0.255454   1.161346   0.660388                  \n",
      "   588     0.249229   1.143779   0.657618                  \n",
      "   589     0.259385   1.145302   0.657618                  \n",
      "   590     0.261281   1.145419   0.657618                  \n",
      "   591     0.250114   1.144276   0.657064                  \n",
      "   592     0.253499   1.145763   0.655402                  \n",
      "   593     0.2551     1.154589   0.657064                  \n",
      "   594     0.25769    1.145743   0.65928                   \n",
      "   595     0.254724   1.143225   0.659834                  \n",
      "   596     0.247976   1.147585   0.657064                  \n",
      "   597     0.243226   1.150636   0.655402                  \n",
      "   598     0.247736   1.148667   0.655956                  \n",
      "   599     0.241245   1.163747   0.658726                  \n",
      "   600     0.250866   1.154027   0.65928                   \n",
      "   601     0.245066   1.155114   0.655956                  \n",
      "   602     0.247318   1.156859   0.655956                  \n",
      "   603     0.23732    1.159759   0.65928                   \n",
      "   604     0.243574   1.167726   0.657064                  \n",
      "   605     0.244775   1.183357   0.655956                  \n",
      "   606     0.247566   1.158953   0.658172                  \n",
      "   607     0.247844   1.180379   0.65651                   \n",
      "   608     0.241835   1.182081   0.661496                  \n",
      "   609     0.234847   1.173968   0.660388                  \n",
      "   610     0.243822   1.160473   0.65928                   \n",
      "   611     0.243877   1.16858    0.658172                  \n",
      "   612     0.239985   1.160299   0.658172                  \n",
      "   613     0.239269   1.156751   0.661496                  \n",
      "   614     0.240707   1.171698   0.65928                   \n",
      "   615     0.26137    1.159403   0.660388                  \n",
      "   616     0.253772   1.168157   0.66205                   \n",
      "   617     0.244784   1.151558   0.666482                  \n",
      "   618     0.248342   1.155284   0.663712                  \n",
      "   619     0.245101   1.146326   0.665374                  \n",
      "   620     0.250079   1.16582    0.662604                  \n",
      "   621     0.245996   1.144018   0.66482                   \n",
      "   622     0.242382   1.146825   0.666482                  \n",
      "   623     0.248396   1.140064   0.665928                  \n",
      "   624     0.259779   1.152564   0.66482                   \n",
      "   625     0.255719   1.155036   0.663712                  \n",
      "   626     0.25175    1.161167   0.661496                  \n",
      "   627     0.249316   1.141507   0.661496                  \n",
      "   628     0.246585   1.150861   0.66205                   \n",
      "   629     0.253274   1.143457   0.664266                  \n",
      "   630     0.248519   1.158768   0.660388                  \n",
      "   631     0.253267   1.155283   0.657618                  \n",
      "   632     0.240501   1.145639   0.662604                  \n",
      "   633     0.237784   1.152406   0.663712                  \n",
      "   634     0.24075    1.163682   0.66205                   \n",
      "   635     0.244119   1.165887   0.663158                  \n",
      "   636     0.244487   1.166944   0.658726                  \n",
      "   637     0.24346    1.15809    0.655956                  \n",
      "   638     0.244286   1.177259   0.657618                  \n",
      "   639     0.245217   1.148151   0.657064                  \n",
      "   640     0.242976   1.162139   0.660942                  \n",
      "   641     0.241718   1.17169    0.661496                  \n",
      "   642     0.242139   1.172133   0.66205                   \n",
      "   643     0.236126   1.144179   0.660942                  \n",
      "   644     0.243055   1.163568   0.665928                  \n",
      "   645     0.254261   1.15864    0.663158                  \n",
      "   646     0.248554   1.156082   0.663158                  \n",
      "   647     0.240035   1.165249   0.66205                   \n",
      "   648     0.242429   1.16676    0.660942                  \n",
      "   649     0.245125   1.146611   0.661496                  \n",
      "   650     0.246507   1.159927   0.663712                  \n",
      "   651     0.244443   1.153957   0.662604                  \n",
      "   652     0.242483   1.142348   0.66205                   \n",
      "   653     0.245516   1.166376   0.664266                  \n",
      "   654     0.253494   1.167445   0.664266                  \n",
      "   655     0.255677   1.163021   0.660388                  \n",
      "   656     0.251672   1.163316   0.662604                  \n",
      "   657     0.244389   1.165601   0.657064                  \n",
      "   658     0.241604   1.169543   0.65928                   \n",
      "   659     0.254716   1.161263   0.66205                   \n",
      "   660     0.248377   1.156886   0.663712                  \n",
      "   661     0.239694   1.156161   0.663158                  \n",
      "   662     0.248321   1.163004   0.665374                  \n",
      "   663     0.258675   1.153214   0.668698                  \n",
      "   664     0.249124   1.1642     0.665928                  \n",
      "   665     0.243891   1.164594   0.664266                  \n",
      "   666     0.246848   1.167133   0.671468                  \n",
      "   667     0.249319   1.16726    0.66759                   \n",
      "   668     0.241224   1.182283   0.660388                  \n",
      "   669     0.235863   1.159519   0.662604                  \n",
      "   670     0.235929   1.172265   0.663158                  \n",
      "   671     0.242402   1.169081   0.66205                   \n",
      "   672     0.237775   1.163917   0.663712                  \n",
      "   673     0.236874   1.187437   0.660942                  \n",
      "   674     0.244056   1.17321    0.666482                  \n",
      "   675     0.246675   1.154535   0.666482                  \n",
      "   676     0.248496   1.15688    0.662604                  \n",
      "   677     0.246671   1.176017   0.655956                  \n",
      "   678     0.248475   1.162857   0.660388                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   679     0.25294    1.163014   0.659834                  \n",
      "   680     0.246492   1.172273   0.662604                  \n",
      "   681     0.242886   1.177355   0.660388                  \n",
      "   682     0.247874   1.173682   0.660942                  \n",
      "   683     0.240619   1.17583    0.663712                  \n",
      "   684     0.24754    1.179474   0.660388                  \n",
      "   685     0.248372   1.167617   0.65928                   \n",
      "   686     0.252111   1.171364   0.660942                  \n",
      "   687     0.24269    1.168883   0.661496                  \n",
      "   688     0.252243   1.162776   0.659834                  \n",
      "   689     0.249951   1.172488   0.66205                   \n",
      "   690     0.243272   1.172799   0.65928                   \n",
      "   691     0.234198   1.172194   0.662604                  \n",
      "   692     0.246961   1.183528   0.65928                   \n",
      "   693     0.250498   1.177973   0.660942                  \n",
      "   694     0.251303   1.163655   0.662604                  \n",
      "   695     0.253019   1.175343   0.663158                  \n",
      "   696     0.246273   1.150661   0.659834                  \n",
      "   697     0.238129   1.164195   0.658726                  \n",
      "   698     0.243183   1.168978   0.661496                  \n",
      "   699     0.247729   1.163932   0.658726                  \n",
      "   700     0.248537   1.181901   0.660388                  \n",
      "   701     0.253379   1.156321   0.660942                  \n",
      "   702     0.249146   1.168187   0.660942                  \n",
      "   703     0.252776   1.160218   0.660388                  \n",
      "   704     0.252085   1.166756   0.660942                  \n",
      "   705     0.265359   1.185189   0.65928                   \n",
      "   706     0.263823   1.183978   0.657064                  \n",
      "   707     0.253239   1.173681   0.65651                   \n",
      "   708     0.274863   1.17814    0.654848                  \n",
      "   709     0.254102   1.190202   0.65928                   \n",
      "   710     0.24897    1.172538   0.654294                  \n",
      "   711     0.259985   1.178907   0.65651                   \n",
      "   712     0.255845   1.171661   0.654848                  \n",
      "   713     0.244309   1.183575   0.658726                  \n",
      "   714     0.236936   1.161122   0.655956                  \n",
      "   715     0.238201   1.170679   0.65651                   \n",
      "   716     0.236664   1.175822   0.653186                  \n",
      "   717     0.245621   1.178808   0.658726                  \n",
      "   718     0.237606   1.180559   0.654294                  \n",
      "   719     0.241585   1.170704   0.661496                  \n",
      "   720     0.242084   1.168637   0.655956                  \n",
      "   721     0.251953   1.187289   0.658172                  \n",
      "   722     0.248985   1.159578   0.657064                  \n",
      "   723     0.238892   1.167842   0.659834                  \n",
      "   724     0.244351   1.169645   0.65928                   \n",
      "   725     0.24626    1.163836   0.658172                  \n",
      "   726     0.242067   1.180185   0.658726                  \n",
      "   727     0.242424   1.191412   0.661496                  \n",
      "   728     0.238409   1.178084   0.660942                  \n",
      "   729     0.233452   1.175288   0.65651                   \n",
      "   730     0.23375    1.167874   0.654848                  \n",
      "   731     0.239989   1.190811   0.654294                  \n",
      "   732     0.240847   1.193461   0.658726                  \n",
      "   733     0.240405   1.178097   0.65374                   \n",
      "   734     0.240971   1.17022    0.658172                  \n",
      "   735     0.241983   1.181977   0.658726                  \n",
      "   736     0.241716   1.170555   0.655956                  \n",
      "   737     0.233903   1.176406   0.655402                  \n",
      "   738     0.231421   1.181081   0.655956                  \n",
      "   739     0.24107    1.171387   0.65651                   \n",
      "   740     0.233812   1.184848   0.655402                  \n",
      "   741     0.241153   1.172715   0.652632                  \n",
      "   742     0.240113   1.161286   0.65097                   \n",
      "   743     0.233934   1.178729   0.65651                   \n",
      "   744     0.235682   1.180377   0.65928                   \n",
      "   745     0.237618   1.179861   0.654848                  \n",
      "   746     0.233262   1.169276   0.65374                   \n",
      "   747     0.239629   1.176499   0.659834                  \n",
      "   748     0.246291   1.16846    0.657618                  \n",
      "   749     0.240848   1.173711   0.658726                  \n",
      "   750     0.241973   1.181866   0.658172                  \n",
      "   751     0.240351   1.181953   0.658172                  \n",
      "   752     0.241742   1.188121   0.657064                  \n",
      "   753     0.249989   1.172522   0.662604                  \n",
      "   754     0.252271   1.175609   0.655402                  \n",
      "   755     0.247278   1.165477   0.65651                   \n",
      "   756     0.238506   1.178368   0.657618                  \n",
      "   757     0.248103   1.182836   0.658726                  \n",
      "   758     0.243264   1.187053   0.658726                  \n",
      "   759     0.239847   1.184391   0.653186                  \n",
      "   760     0.234392   1.181712   0.65651                   \n",
      "   761     0.233268   1.2013     0.657064                  \n",
      "   762     0.238578   1.182525   0.66205                   \n",
      "   763     0.248674   1.190604   0.657618                  \n",
      "   764     0.241831   1.193119   0.660388                  \n",
      "   765     0.239446   1.187684   0.658726                  \n",
      "   766     0.259007   1.181905   0.659834                  \n",
      "   767     0.249218   1.205995   0.658726                  \n",
      "   768     0.244314   1.188414   0.65928                   \n",
      "   769     0.239479   1.186963   0.65374                   \n",
      "   770     0.230414   1.17426    0.658172                  \n",
      "   771     0.23337    1.187838   0.663158                  \n",
      "   772     0.235925   1.187089   0.655402                  \n",
      "   773     0.246827   1.188043   0.66205                   \n",
      "   774     0.235794   1.183209   0.658172                  \n",
      "   775     0.236114   1.176485   0.660942                  \n",
      "   776     0.23056    1.194168   0.658726                  \n",
      "   777     0.22788    1.171951   0.65928                   \n",
      "   778     0.229198   1.179042   0.66205                   \n",
      "   779     0.233293   1.180266   0.65928                   \n",
      "   780     0.237799   1.191426   0.659834                  \n",
      "   781     0.2406     1.172941   0.65928                   \n",
      "   782     0.230353   1.168229   0.659834                  \n",
      "   783     0.228524   1.189795   0.659834                  \n",
      "   784     0.233708   1.175034   0.658726                  \n",
      "   785     0.228534   1.18432    0.65928                   \n",
      "   786     0.22776    1.180154   0.654294                  \n",
      "   787     0.227278   1.189116   0.655956                  \n",
      "   788     0.254539   1.205477   0.65651                   \n",
      "   789     0.238732   1.187594   0.658726                  \n",
      "   790     0.241406   1.213567   0.658172                  \n",
      "   791     0.244166   1.193023   0.658172                  \n",
      "   792     0.232262   1.19747    0.658726                  \n",
      "   793     0.237203   1.21355    0.657064                  \n",
      "   794     0.232389   1.211849   0.663712                  \n",
      "   795     0.235263   1.199485   0.661496                  \n",
      "   796     0.236249   1.191074   0.659834                  \n",
      "   797     0.226519   1.199361   0.659834                  \n",
      "   798     0.22932    1.195526   0.664266                  \n",
      "   799     0.229403   1.204472   0.662604                  \n",
      "   800     0.230567   1.196096   0.660388                  \n",
      "   801     0.228334   1.195293   0.66205                   \n",
      "   802     0.231936   1.180667   0.661496                  \n",
      "   803     0.247436   1.191899   0.658726                  \n",
      "   804     0.23773    1.198551   0.660388                  \n",
      "   805     0.235716   1.199333   0.661496                  \n",
      "   806     0.225767   1.173351   0.659834                  \n",
      "   807     0.237445   1.186998   0.660388                  \n",
      "   808     0.239482   1.185965   0.658726                  \n",
      "   809     0.237782   1.186166   0.658172                  \n",
      "   810     0.229534   1.204124   0.660942                  \n",
      "   811     0.234974   1.173901   0.654294                  \n",
      "   812     0.239168   1.18038    0.658172                  \n",
      "   813     0.235912   1.19051    0.659834                  \n",
      "   814     0.254183   1.18265    0.660942                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   815     0.250439   1.181799   0.660388                  \n",
      "   816     0.258019   1.180085   0.658726                  \n",
      "   817     0.255183   1.173952   0.660388                  \n",
      "   818     0.254133   1.176585   0.660942                  \n",
      "   819     0.24211    1.174311   0.65928                   \n",
      "   820     0.243052   1.165652   0.658172                  \n",
      "   821     0.241802   1.178274   0.65928                   \n",
      "   822     0.247421   1.184121   0.660388                  \n",
      "   823     0.243676   1.189842   0.663712                  \n",
      "   824     0.242889   1.186723   0.660388                  \n",
      "   825     0.242348   1.175952   0.660942                  \n",
      "   826     0.236334   1.16754    0.660388                  \n",
      "   827     0.234544   1.172152   0.663712                  \n",
      "   828     0.224027   1.17028    0.660942                  \n",
      "   829     0.229296   1.180581   0.661496                  \n",
      "   830     0.238175   1.180813   0.657618                  \n",
      "   831     0.230227   1.189562   0.660388                  \n",
      "   832     0.231131   1.191561   0.659834                  \n",
      "   833     0.232512   1.192697   0.658172                  \n",
      "   834     0.231867   1.188423   0.664266                  \n",
      "   835     0.229953   1.176926   0.657618                  \n",
      "   836     0.236195   1.180041   0.660388                  \n",
      "   837     0.226138   1.20613    0.657618                  \n",
      "   838     0.227332   1.18686    0.655956                  \n",
      "   839     0.228693   1.215167   0.654848                  \n",
      "   840     0.230172   1.202871   0.654848                  \n",
      "   841     0.227077   1.185659   0.658726                  \n",
      "   842     0.232713   1.181872   0.658172                  \n",
      "   843     0.238748   1.206707   0.658726                  \n",
      "   844     0.2332     1.186999   0.658172                  \n",
      "   845     0.239105   1.186691   0.65928                   \n",
      "   846     0.228787   1.185497   0.660942                  \n",
      "   847     0.241444   1.186232   0.654848                  \n",
      "   848     0.233865   1.186971   0.658726                  \n",
      "   849     0.22749    1.183989   0.661496                  \n",
      "   850     0.228012   1.173309   0.654848                  \n",
      "   851     0.222225   1.17489    0.652632                  \n",
      "   852     0.225975   1.19517    0.658726                  \n",
      "   853     0.233764   1.178322   0.659834                  \n",
      "   854     0.230771   1.169188   0.657618                  \n",
      "   855     0.226394   1.168586   0.657618                  \n",
      "   856     0.228111   1.186205   0.66205                   \n",
      "   857     0.239387   1.186593   0.66205                   \n",
      "   858     0.247133   1.198938   0.661496                  \n",
      "   859     0.23446    1.179416   0.660388                  \n",
      "   860     0.232061   1.184996   0.660388                  \n",
      "   861     0.234224   1.200031   0.663158                  \n",
      "   862     0.229883   1.199058   0.660942                  \n",
      "   863     0.241251   1.205112   0.66205                   \n",
      "   864     0.24433    1.15257    0.659834                  \n",
      "   865     0.238964   1.157834   0.667036                  \n",
      "   866     0.237581   1.172747   0.662604                  \n",
      "   867     0.240162   1.172775   0.661496                  \n",
      "   868     0.248035   1.164349   0.66205                   \n",
      "   869     0.238871   1.1651     0.660388                  \n",
      "   870     0.232533   1.164197   0.663158                  \n",
      "   871     0.232467   1.171603   0.657618                  \n",
      "   872     0.238388   1.167978   0.664266                  \n",
      "   873     0.242032   1.168414   0.663158                  \n",
      "   874     0.231511   1.182567   0.655402                  \n",
      "   875     0.232394   1.173613   0.660388                  \n",
      "   876     0.233423   1.177804   0.662604                  \n",
      "   877     0.230937   1.164359   0.660942                  \n",
      "   878     0.238035   1.171113   0.65928                   \n",
      "   879     0.232434   1.176452   0.657618                  \n",
      "   880     0.230624   1.189499   0.657618                  \n",
      "   881     0.223849   1.191607   0.657064                  \n",
      "   882     0.229791   1.20192    0.655956                  \n",
      "   883     0.231039   1.19909    0.659834                  \n",
      "   884     0.230025   1.199789   0.660942                  \n",
      "   885     0.235972   1.204869   0.660942                  \n",
      "   886     0.243675   1.198543   0.662604                  \n",
      "   887     0.240694   1.197708   0.660388                  \n",
      "   888     0.23532    1.179752   0.661496                  \n",
      "   889     0.231358   1.184378   0.660942                  \n",
      "   890     0.241096   1.180622   0.663158                  \n",
      "   891     0.23689    1.192787   0.660388                  \n",
      "   892     0.236731   1.204887   0.660942                  \n",
      "   893     0.240196   1.203105   0.658726                  \n",
      "   894     0.238714   1.197281   0.658726                  \n",
      "   895     0.232697   1.175139   0.65928                   \n",
      "   896     0.236573   1.200829   0.658726                  \n",
      "   897     0.236813   1.196674   0.65651                   \n",
      "   898     0.23697    1.196102   0.661496                  \n",
      "   899     0.236936   1.201878   0.658172                  \n",
      "   900     0.230419   1.191267   0.66205                   \n",
      "   901     0.222224   1.186137   0.66205                   \n",
      "   902     0.227386   1.188714   0.664266                  \n",
      "   903     0.24636    1.196696   0.663158                  \n",
      "   904     0.24867    1.190024   0.663158                  \n",
      "   905     0.232126   1.190472   0.661496                  \n",
      "   906     0.225072   1.194099   0.664266                  \n",
      "   907     0.231831   1.207567   0.660942                  \n",
      "   908     0.23424    1.204831   0.662604                  \n",
      "   909     0.237475   1.196135   0.661496                  \n",
      "   910     0.234897   1.190472   0.66205                   \n",
      "   911     0.235438   1.185201   0.659834                  \n",
      "   912     0.235553   1.19084    0.660388                  \n",
      "   913     0.23295    1.190351   0.664266                  \n",
      "   914     0.232009   1.198512   0.666482                  \n",
      "   915     0.228058   1.195916   0.663712                  \n",
      "   916     0.233242   1.215232   0.662604                  \n",
      "   917     0.226628   1.212034   0.661496                  \n",
      "   918     0.231114   1.213004   0.660942                  \n",
      "   919     0.22636    1.218703   0.659834                  \n",
      "   920     0.232087   1.212059   0.663158                  \n",
      "   921     0.237193   1.217463   0.657064                  \n",
      "   922     0.231981   1.21594    0.66205                   \n",
      "   923     0.237496   1.209582   0.664266                  \n",
      "   924     0.235854   1.200323   0.65928                   \n",
      "   925     0.242303   1.19907    0.667036                  \n",
      "   926     0.243282   1.217806   0.660388                  \n",
      "   927     0.242955   1.206777   0.664266                  \n",
      "   928     0.237809   1.210861   0.662604                  \n",
      "   929     0.23985    1.187712   0.664266                  \n",
      "   930     0.246514   1.209496   0.661496                  \n",
      "   931     0.235327   1.191986   0.66205                   \n",
      "   932     0.228727   1.214395   0.671468                  \n",
      "   933     0.232676   1.182017   0.663158                  \n",
      "   934     0.233545   1.175505   0.659834                  \n",
      "   935     0.240672   1.185658   0.662604                  \n",
      "   936     0.23654    1.170131   0.660942                  \n",
      "   937     0.235443   1.1865     0.664266                  \n",
      "   938     0.236793   1.192051   0.665374                  \n",
      "   939     0.236626   1.180114   0.65928                   \n",
      "   940     0.235837   1.162741   0.666482                  \n",
      "   941     0.234536   1.187023   0.662604                  \n",
      "   942     0.233411   1.170747   0.66205                   \n",
      "   943     0.234474   1.174395   0.659834                  \n",
      "   944     0.229748   1.174161   0.663712                  \n",
      "   945     0.233283   1.160796   0.663158                  \n",
      "   946     0.229939   1.177642   0.667036                  \n",
      "   947     0.242125   1.173647   0.661496                  \n",
      "   948     0.237517   1.173568   0.662604                  \n",
      "   949     0.229787   1.164923   0.664266                  \n",
      "   950     0.230126   1.183091   0.667036                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   951     0.237042   1.172766   0.661496                  \n",
      "   952     0.234774   1.165052   0.661496                  \n",
      "   953     0.229201   1.176885   0.664266                  \n",
      "   954     0.240216   1.179889   0.66205                   \n",
      "   955     0.244386   1.176504   0.66759                   \n",
      "   956     0.254148   1.162154   0.662604                  \n",
      "   957     0.243497   1.14334    0.665374                  \n",
      "   958     0.234618   1.166816   0.669806                  \n",
      "   959     0.242556   1.177689   0.665928                  \n",
      "   960     0.239226   1.173526   0.65928                   \n",
      "   961     0.244607   1.166703   0.665928                  \n",
      "   962     0.24603    1.162792   0.66205                   \n",
      "   963     0.247605   1.161058   0.67036                   \n",
      "   964     0.245084   1.164309   0.669806                  \n",
      "   965     0.241896   1.159026   0.665374                  \n",
      "   966     0.241834   1.16271    0.668698                  \n",
      "   967     0.234709   1.184321   0.657064                  \n",
      "   968     0.239269   1.17721    0.65928                   \n",
      "   969     0.237956   1.182297   0.660942                  \n",
      "   970     0.234899   1.171342   0.665928                  \n",
      "   971     0.239078   1.166862   0.664266                  \n",
      "   972     0.23822    1.183628   0.660942                  \n",
      "   973     0.235308   1.17561    0.659834                  \n",
      "   974     0.233181   1.18435    0.659834                  \n",
      "   975     0.232259   1.191948   0.665374                  \n",
      "   976     0.241114   1.179308   0.662604                  \n",
      "   977     0.236949   1.174693   0.663712                  \n",
      "   978     0.232986   1.177827   0.660388                  \n",
      "   979     0.242138   1.160121   0.665374                  \n",
      "   980     0.238156   1.173501   0.663158                  \n",
      "   981     0.2395     1.17912    0.660942                  \n",
      "   982     0.231687   1.17696    0.664266                  \n",
      "   983     0.23088    1.181334   0.663712                  \n",
      "   984     0.223767   1.197619   0.666482                  \n",
      "   985     0.224279   1.173703   0.665928                  \n",
      "   986     0.228747   1.188889   0.664266                  \n",
      "   987     0.228111   1.177139   0.662604                  \n",
      "   988     0.227711   1.182865   0.66205                   \n",
      "   989     0.230483   1.17766    0.663712                  \n",
      "   990     0.23027    1.194594   0.663712                  \n",
      "   991     0.240018   1.178701   0.673684                  \n",
      "   992     0.22871    1.17432    0.665928                  \n",
      "   993     0.228908   1.170378   0.662604                  \n",
      "   994     0.235587   1.193651   0.660942                  \n",
      "   995     0.233974   1.172624   0.665374                  \n",
      "   996     0.240757   1.184988   0.664266                  \n",
      "   997     0.24404    1.197204   0.662604                  \n",
      "   998     0.237268   1.17877    0.665374                  \n",
      "   999     0.232099   1.185758   0.662604                  \n",
      "   1000    0.226571   1.19399    0.669252                  \n",
      "   1001    0.231805   1.18601    0.666482                  \n",
      "   1002    0.234612   1.189906   0.663712                  \n",
      "   1003    0.231762   1.186884   0.662604                  \n",
      "   1004    0.229199   1.193705   0.666482                  \n",
      "   1005    0.237653   1.199898   0.668698                  \n",
      "   1006    0.236942   1.18802    0.668144                  \n",
      "   1007    0.22926    1.189036   0.66482                   \n",
      "   1008    0.228703   1.17657    0.668698                  \n",
      "   1009    0.243982   1.182863   0.667036                  \n",
      "   1010    0.233074   1.186626   0.663158                  \n",
      "   1011    0.229838   1.192421   0.66482                   \n",
      "   1012    0.230471   1.178907   0.660942                  \n",
      "   1013    0.239359   1.227017   0.659834                  \n",
      "   1014    0.23661    1.179442   0.663158                  \n",
      "   1015    0.232852   1.194902   0.663712                  \n",
      "   1016    0.222473   1.205612   0.65651                   \n",
      "   1017    0.232505   1.198738   0.662604                  \n",
      "   1018    0.235587   1.196022   0.665374                  \n",
      "   1019    0.224227   1.197458   0.665374                  \n",
      "   1020    0.229874   1.186155   0.661496                  \n",
      "   1021    0.225808   1.185036   0.667036                  \n",
      "   1022    0.232314   1.188425   0.66205                   \n",
      "   1023    0.228123   1.209033   0.658172                  \n",
      "   1024    0.223602   1.193004   0.661496                  \n",
      "   1025    0.227589   1.181794   0.661496                  \n",
      "   1026    0.225136   1.197943   0.665374                  \n",
      "   1027    0.232841   1.192807   0.66482                   \n",
      "   1028    0.231015   1.202612   0.663158                  \n",
      "   1029    0.229317   1.204529   0.669252                  \n",
      "   1030    0.227356   1.206197   0.665928                  \n",
      "   1031    0.232934   1.198414   0.65928                   \n",
      "   1032    0.232031   1.201161   0.663712                  \n",
      "   1033    0.228683   1.213392   0.663158                  \n",
      "   1034    0.236323   1.219539   0.665928                  \n",
      "   1035    0.234332   1.205626   0.669252                  \n",
      "   1036    0.236997   1.214646   0.668698                  \n",
      "   1037    0.249709   1.228502   0.662604                  \n",
      "   1038    0.240965   1.186436   0.668144                  \n",
      "   1039    0.233298   1.204346   0.666482                  \n",
      "   1040    0.230115   1.203941   0.657064                  \n",
      "   1041    0.233903   1.233269   0.65928                   \n",
      "   1042    0.228909   1.209124   0.661496                  \n",
      "   1043    0.234067   1.211708   0.658172                  \n",
      "   1044    0.234918   1.222004   0.65928                   \n",
      "   1045    0.226197   1.203037   0.657618                  \n",
      "   1046    0.227428   1.187442   0.659834                  \n",
      "   1047    0.240444   1.216417   0.658172                  \n",
      "   1048    0.229416   1.197504   0.654294                  \n",
      "   1049    0.236046   1.212253   0.660388                  \n",
      "   1050    0.228905   1.189702   0.660942                  \n",
      "   1051    0.226955   1.212174   0.65651                   \n",
      "   1052    0.234173   1.205814   0.654848                  \n",
      "   1053    0.242672   1.220634   0.65651                   \n",
      "   1054    0.23999    1.201147   0.652632                  \n",
      "   1055    0.231867   1.210173   0.658172                  \n",
      "   1056    0.22557    1.21867    0.657618                  \n",
      "   1057    0.235714   1.211821   0.658172                  \n",
      "   1058    0.22922    1.204606   0.66482                   \n",
      "   1059    0.228991   1.206726   0.65928                   \n",
      "   1060    0.222911   1.207229   0.667036                  \n",
      "   1061    0.231235   1.207686   0.662604                  \n",
      "   1062    0.219906   1.191065   0.663712                  \n",
      "   1063    0.225191   1.208381   0.66482                   \n",
      "   1064    0.229019   1.215521   0.663158                  \n",
      "   1065    0.228829   1.219022   0.658726                  \n",
      "   1066    0.234161   1.220294   0.660388                  \n",
      "   1067    0.239556   1.211924   0.662604                  \n",
      "   1068    0.233935   1.198192   0.65928                   \n",
      "   1069    0.231639   1.20247    0.663158                  \n",
      "   1070    0.224185   1.206322   0.66205                   \n",
      "   1071    0.233521   1.217343   0.66482                   \n",
      "   1072    0.225862   1.198625   0.664266                  \n",
      "   1073    0.232024   1.214974   0.663158                  \n",
      "   1074    0.23236    1.219349   0.66759                   \n",
      "   1075    0.233013   1.230267   0.663712                  \n",
      "   1076    0.241359   1.206977   0.665374                  \n",
      "   1077    0.237361   1.216597   0.665928                  \n",
      "   1078    0.241328   1.223087   0.65928                   \n",
      "   1079    0.226666   1.222131   0.660942                  \n",
      "   1080    0.226273   1.210238   0.661496                  \n",
      "   1081    0.233509   1.20577    0.659834                  \n",
      "   1082    0.236842   1.212563   0.662604                  \n",
      "   1083    0.231434   1.210263   0.660388                  \n",
      "   1084    0.239148   1.223158   0.663158                  \n",
      "   1085    0.231537   1.217835   0.661496                  \n",
      "   1086    0.230607   1.214744   0.663158                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1087    0.234904   1.223876   0.664266                  \n",
      "   1088    0.232571   1.20994    0.664266                  \n",
      "   1089    0.221397   1.202641   0.668144                  \n",
      "   1090    0.217383   1.203907   0.66759                   \n",
      "   1091    0.223032   1.215136   0.664266                  \n",
      "   1092    0.226074   1.217499   0.668144                  \n",
      "   1093    0.227403   1.20655    0.668144                  \n",
      "   1094    0.229039   1.196023   0.669252                  \n",
      "   1095    0.229244   1.205338   0.665374                  \n",
      "   1096    0.241202   1.2051     0.666482                  \n",
      "   1097    0.233899   1.197874   0.663158                  \n",
      "   1098    0.236117   1.223693   0.658172                  \n",
      "   1099    0.229691   1.207336   0.660388                  \n",
      "   1100    0.235664   1.221065   0.662604                  \n",
      "   1101    0.228578   1.219323   0.667036                  \n",
      "   1102    0.223878   1.201691   0.665374                  \n",
      "   1103    0.224113   1.204897   0.668144                  \n",
      "   1104    0.225927   1.226839   0.666482                  \n",
      "   1105    0.22774    1.22054    0.672576                  \n",
      "   1106    0.225351   1.230587   0.67036                   \n",
      "   1107    0.221928   1.22579    0.668144                  \n",
      "   1108    0.235236   1.234054   0.668698                  \n",
      "   1109    0.226631   1.217729   0.66205                   \n",
      "   1110    0.230438   1.206838   0.665928                  \n",
      "   1111    0.230237   1.21109    0.665374                  \n",
      "   1112    0.236751   1.228039   0.664266                  \n",
      "   1113    0.235612   1.211653   0.664266                  \n",
      "   1114    0.228351   1.211708   0.663712                  \n",
      "   1115    0.230587   1.232775   0.666482                  \n",
      "   1116    0.226482   1.229682   0.663712                  \n",
      "   1117    0.218723   1.236572   0.660942                  \n",
      "   1118    0.218651   1.244685   0.663158                  \n",
      "   1119    0.220154   1.21841    0.663712                  \n",
      "   1120    0.222165   1.240017   0.665928                  \n",
      "   1121    0.221551   1.222788   0.66205                   \n",
      "   1122    0.219291   1.227351   0.66482                   \n",
      "   1123    0.222248   1.21041    0.667036                  \n",
      "   1124    0.228709   1.240221   0.66205                   \n",
      "   1125    0.232998   1.240558   0.661496                  \n",
      "   1126    0.23242    1.245365   0.665374                  \n",
      "   1127    0.242524   1.221652   0.660942                  \n",
      "   1128    0.234215   1.234002   0.672022                  \n",
      "   1129    0.233054   1.223652   0.663158                  \n",
      "   1130    0.222139   1.218399   0.664266                  \n",
      "   1131    0.221441   1.223337   0.665374                  \n",
      "   1132    0.220568   1.228695   0.660388                  \n",
      "   1133    0.22134    1.220485   0.660388                  \n",
      "   1134    0.220728   1.230607   0.662604                  \n",
      "   1135    0.239242   1.243831   0.66205                   \n",
      "   1136    0.231099   1.241655   0.66205                   \n",
      "   1137    0.227882   1.221252   0.663712                  \n",
      "   1138    0.226083   1.238479   0.66482                   \n",
      "   1139    0.216632   1.22393    0.66482                   \n",
      "   1140    0.221506   1.217811   0.662604                  \n",
      "   1141    0.217273   1.210358   0.66205                   \n",
      "   1142    0.229458   1.229812   0.663158                  \n",
      "   1143    0.226428   1.22654    0.660388                  \n",
      "   1144    0.230926   1.227128   0.659834                  \n",
      "   1145    0.222078   1.198667   0.660942                  \n",
      "   1146    0.231314   1.196673   0.66482                   \n",
      "   1147    0.223384   1.206384   0.665374                  \n",
      "   1148    0.222827   1.19796    0.669252                  \n",
      "   1149    0.219805   1.201132   0.669252                  \n",
      "   1150    0.218865   1.204915   0.671468                  \n",
      "   1151    0.232508   1.21815    0.667036                  \n",
      "   1152    0.229165   1.207964   0.669806                  \n",
      "   1153    0.237352   1.2231     0.660942                  \n",
      "   1154    0.237778   1.218016   0.665928                  \n",
      "   1155    0.225848   1.198298   0.666482                  \n",
      "   1156    0.236897   1.19809    0.660942                  \n",
      "   1157    0.242954   1.202265   0.668144                  \n",
      "   1158    0.237856   1.221361   0.66205                   \n",
      "   1159    0.22559    1.188506   0.669252                  \n",
      "   1160    0.227058   1.198403   0.669806                  \n",
      "   1161    0.221308   1.200945   0.668144                  \n",
      "   1162    0.218146   1.215187   0.66482                   \n",
      "   1163    0.225073   1.206066   0.668698                  \n",
      "   1164    0.224224   1.181135   0.667036                  \n",
      "   1165    0.229993   1.207878   0.663712                  \n",
      "   1166    0.218954   1.217353   0.666482                  \n",
      "   1167    0.220387   1.209328   0.668144                  \n",
      "   1168    0.242055   1.201083   0.66759                   \n",
      "   1169    0.232433   1.209222   0.660388                  \n",
      "   1170    0.226613   1.208538   0.66482                   \n",
      "   1171    0.223326   1.21239    0.661496                  \n",
      "   1172    0.230694   1.208459   0.66482                   \n",
      "   1173    0.227603   1.220266   0.660942                  \n",
      "   1174    0.222738   1.204871   0.665928                  \n",
      "   1175    0.218254   1.211453   0.665928                  \n",
      "   1176    0.228888   1.211853   0.665374                  \n",
      "   1177    0.232296   1.212014   0.66759                   \n",
      "   1178    0.229665   1.211074   0.662604                  \n",
      "   1179    0.22352    1.222024   0.660388                  \n",
      "   1180    0.230828   1.218614   0.663712                  \n",
      "   1181    0.229166   1.215838   0.661496                  \n",
      "   1182    0.226761   1.218621   0.663712                  \n",
      "   1183    0.226892   1.210554   0.665374                  \n",
      "   1184    0.235518   1.218881   0.657618                  \n",
      "   1185    0.229354   1.198269   0.65928                   \n",
      "   1186    0.236131   1.213572   0.66482                   \n",
      "   1187    0.238996   1.200212   0.663712                  \n",
      "   1188    0.230383   1.200207   0.658726                  \n",
      "   1189    0.219998   1.193653   0.65928                   \n",
      "   1190    0.229079   1.198683   0.658726                  \n",
      "   1191    0.226685   1.213205   0.660388                  \n",
      "   1192    0.22222    1.194164   0.657064                  \n",
      "   1193    0.226462   1.20088    0.66205                   \n",
      "   1194    0.221905   1.196217   0.663158                  \n",
      "   1195    0.219965   1.21537    0.66205                   \n",
      "   1196    0.230913   1.232551   0.665928                  \n",
      "   1197    0.238465   1.220792   0.657064                  \n",
      "   1198    0.223161   1.214406   0.665374                  \n",
      "   1199    0.222742   1.212866   0.660388                  \n",
      "   1200    0.227789   1.208082   0.666482                  \n",
      "   1201    0.226306   1.216344   0.664266                  \n",
      "   1202    0.225408   1.229502   0.663158                  \n",
      "   1203    0.223971   1.199582   0.664266                  \n",
      "   1204    0.228103   1.21073    0.665928                  \n",
      "   1205    0.228874   1.203622   0.662604                  \n",
      "   1206    0.226612   1.214524   0.665928                  \n",
      "   1207    0.225074   1.203116   0.665374                  \n",
      "   1208    0.224578   1.190609   0.668144                  \n",
      "   1209    0.22227    1.216691   0.665928                  \n",
      "   1210    0.230914   1.225928   0.668698                  \n",
      "   1211    0.226041   1.21666    0.665928                  \n",
      "   1212    0.217625   1.198219   0.666482                  \n",
      "   1213    0.221881   1.222137   0.659834                  \n",
      "   1214    0.220146   1.208606   0.663158                  \n",
      "   1215    0.220631   1.21203    0.664266                  \n",
      "   1216    0.21251    1.216927   0.662604                  \n",
      "   1217    0.22024    1.238726   0.661496                  \n",
      "   1218    0.240442   1.221444   0.669252                  \n",
      "   1219    0.234145   1.236204   0.665374                  \n",
      "   1220    0.220704   1.21746    0.665928                  \n",
      "   1221    0.221235   1.220103   0.668144                  \n",
      "   1222    0.222007   1.222799   0.667036                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1223    0.225795   1.234101   0.666482                  \n",
      "   1224    0.222218   1.229884   0.665374                  \n",
      "   1225    0.221205   1.228585   0.66205                   \n",
      "   1226    0.23241    1.227849   0.668698                  \n",
      "   1227    0.226713   1.222526   0.668144                  \n",
      "   1228    0.223978   1.217192   0.672022                  \n",
      "   1229    0.227128   1.2086     0.66759                   \n",
      "   1230    0.221178   1.224321   0.671468                  \n",
      "   1231    0.220072   1.215981   0.671468                  \n",
      "   1232    0.217883   1.221559   0.668698                  \n",
      "   1233    0.22064    1.219427   0.66759                   \n",
      "   1234    0.220474   1.236406   0.672022                  \n",
      "   1235    0.215614   1.221924   0.6759                    \n",
      "   1236    0.222805   1.230921   0.665374                  \n",
      "   1237    0.227069   1.209771   0.672022                  \n",
      "   1238    0.236319   1.24182    0.666482                  \n",
      "   1239    0.227724   1.203652   0.66482                   \n",
      "   1240    0.226389   1.20392    0.668698                  \n",
      "   1241    0.216455   1.208975   0.666482                  \n",
      "   1242    0.22906    1.219102   0.665928                  \n",
      "   1243    0.225606   1.21624    0.665374                  \n",
      "   1244    0.219587   1.222305   0.671468                  \n",
      "   1245    0.219514   1.225121   0.66482                   \n",
      "   1246    0.217692   1.215194   0.66482                   \n",
      "   1247    0.220785   1.212784   0.665374                  \n",
      "   1248    0.229229   1.236928   0.666482                  \n",
      "   1249    0.222968   1.209164   0.67036                   \n",
      "   1250    0.219998   1.21021    0.663158                  \n",
      "   1251    0.221518   1.21481    0.668698                  \n",
      "   1252    0.220807   1.217992   0.66759                   \n",
      "   1253    0.216745   1.209023   0.668698                  \n",
      "   1254    0.218972   1.2263     0.66482                   \n",
      "   1255    0.220391   1.219903   0.669252                  \n",
      "   1256    0.220301   1.207367   0.669806                  \n",
      "   1257    0.220499   1.221992   0.668144                  \n",
      "   1258    0.230136   1.239861   0.663712                  \n",
      "   1259    0.241234   1.225687   0.666482                  \n",
      "   1260    0.241117   1.221166   0.665928                  \n",
      "   1261    0.231909   1.224316   0.66482                   \n",
      "   1262    0.257682   1.223521   0.668144                  \n",
      "   1263    0.241964   1.225991   0.666482                  \n",
      "   1264    0.243308   1.210839   0.663712                  \n",
      "   1265    0.230072   1.217937   0.66759                   \n",
      "   1266    0.233395   1.214075   0.666482                  \n",
      "   1267    0.230754   1.217112   0.663712                  \n",
      "   1268    0.2295     1.222887   0.66205                   \n",
      "   1269    0.241313   1.230447   0.664266                  \n",
      "   1270    0.22944    1.221269   0.663712                  \n",
      "   1271    0.234817   1.241418   0.661496                  \n",
      "   1272    0.224374   1.22107    0.660388                  \n",
      "   1273    0.229466   1.220149   0.659834                  \n",
      "   1274    0.225024   1.228112   0.65651                   \n",
      "   1275    0.227229   1.230549   0.66205                   \n",
      "   1276    0.227303   1.221041   0.660942                  \n",
      "   1277    0.224578   1.240975   0.665374                  \n",
      "   1278    0.218805   1.238042   0.665928                  \n",
      "   1279    0.222613   1.2363     0.661496                  \n",
      "   1280    0.22414    1.230713   0.660942                  \n",
      "   1281    0.22924    1.226619   0.660942                  \n",
      "   1282    0.2236     1.238899   0.664266                  \n",
      "   1283    0.219276   1.231958   0.66482                   \n",
      "   1284    0.226463   1.225042   0.667036                  \n",
      "   1285    0.221662   1.22666    0.668698                  \n",
      "   1286    0.221764   1.242839   0.660942                  \n",
      "   1287    0.220262   1.207109   0.667036                  \n",
      "   1288    0.215534   1.201658   0.667036                  \n",
      "   1289    0.213153   1.21808    0.665374                  \n",
      "   1290    0.215988   1.234631   0.667036                  \n",
      "   1291    0.222402   1.215616   0.660942                  \n",
      "   1292    0.216291   1.22253    0.660942                  \n",
      "   1293    0.220082   1.235973   0.664266                  \n",
      "   1294    0.21847    1.21729    0.663712                  \n",
      "   1295    0.215796   1.223436   0.661496                  \n",
      "   1296    0.212176   1.223737   0.666482                  \n",
      "   1297    0.216984   1.228996   0.665928                  \n",
      "   1298    0.215116   1.235289   0.660388                  \n",
      "   1299    0.216991   1.242251   0.665374                  \n",
      "   1300    0.217064   1.231232   0.66482                   \n",
      "   1301    0.224227   1.223027   0.665928                  \n",
      "   1302    0.224762   1.220082   0.663158                  \n",
      "   1303    0.225308   1.208497   0.661496                  \n",
      "   1304    0.226534   1.214596   0.662604                  \n",
      "   1305    0.230433   1.224053   0.666482                  \n",
      "   1306    0.230309   1.201286   0.668144                  \n",
      "   1307    0.237373   1.210222   0.66759                   \n",
      "   1308    0.232292   1.205756   0.661496                  \n",
      "   1309    0.223906   1.209638   0.668144                  \n",
      "   1310    0.224787   1.213639   0.665928                  \n",
      "   1311    0.22171    1.213818   0.663158                  \n",
      "   1312    0.226723   1.206391   0.665928                  \n",
      "   1313    0.229023   1.216727   0.666482                  \n",
      "   1314    0.228944   1.216302   0.661496                  \n",
      "   1315    0.229841   1.220361   0.66205                   \n",
      "   1316    0.246603   1.210213   0.663712                  \n",
      "   1317    0.230746   1.218259   0.663158                  \n",
      "   1318    0.223325   1.211698   0.661496                  \n",
      "   1319    0.223477   1.209917   0.662604                  \n",
      "   1320    0.218165   1.199673   0.65928                   \n",
      "   1321    0.218801   1.205685   0.66205                   \n",
      "   1322    0.218243   1.207362   0.662604                  \n",
      "   1323    0.221973   1.205732   0.66759                   \n",
      "   1324    0.225082   1.193531   0.664266                  \n",
      "   1325    0.226883   1.210963   0.663712                  \n",
      "   1326    0.227616   1.228756   0.660942                  \n",
      "   1327    0.224018   1.209947   0.661496                  \n",
      "   1328    0.224224   1.219155   0.665374                  \n",
      "   1329    0.228556   1.191062   0.663712                  \n",
      "   1330    0.231082   1.215627   0.665374                  \n",
      "   1331    0.224684   1.203265   0.663158                  \n",
      "   1332    0.222034   1.208807   0.662604                  \n",
      "   1333    0.22732    1.216949   0.668144                  \n",
      "   1334    0.222188   1.223438   0.665374                  \n",
      "   1335    0.225928   1.235732   0.660388                  \n",
      "   1336    0.237527   1.224259   0.66482                   \n",
      "   1337    0.229554   1.215406   0.664266                  \n",
      "   1338    0.225388   1.22875    0.662604                  \n",
      "   1339    0.231066   1.237427   0.661496                  \n",
      "   1340    0.220628   1.23674    0.658726                  \n",
      "   1341    0.220156   1.228554   0.658172                  \n",
      "   1342    0.222775   1.230629   0.665374                  \n",
      "   1343    0.220178   1.214878   0.663158                  \n",
      "   1344    0.216721   1.232652   0.665374                  \n",
      "   1345    0.219252   1.230105   0.661496                  \n",
      "   1346    0.227164   1.225649   0.66759                   \n",
      "   1347    0.228358   1.233633   0.676454                  \n",
      "   1348    0.231305   1.210236   0.668698                  \n",
      "   1349    0.224787   1.231632   0.664266                  \n",
      "   1350    0.223055   1.239944   0.669806                  \n",
      "   1351    0.22842    1.218183   0.668698                  \n",
      "   1352    0.228967   1.215359   0.669252                  \n",
      "   1353    0.228341   1.230475   0.666482                  \n",
      "   1354    0.228766   1.22819    0.669806                  \n",
      "   1355    0.232156   1.251107   0.665374                  \n",
      "   1356    0.225802   1.227101   0.665928                  \n",
      "   1357    0.227276   1.237508   0.665374                  \n",
      "   1358    0.228865   1.246427   0.662604                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1359    0.221596   1.238714   0.668144                  \n",
      "   1360    0.228383   1.247261   0.665928                  \n",
      "   1361    0.23067    1.244748   0.669806                  \n",
      "   1362    0.226698   1.234948   0.66759                   \n",
      "   1363    0.222952   1.249777   0.663158                  \n",
      "   1364    0.225547   1.232368   0.662604                  \n",
      "   1365    0.224581   1.232728   0.663158                  \n",
      "   1366    0.224891   1.230632   0.664266                  \n",
      "   1367    0.242853   1.234311   0.665374                  \n",
      "   1368    0.22643    1.228883   0.661496                  \n",
      "   1369    0.229522   1.238796   0.667036                  \n",
      "   1370    0.230077   1.218884   0.665928                  \n",
      "   1371    0.228906   1.232157   0.66759                   \n",
      "   1372    0.226989   1.226087   0.668698                  \n",
      "   1373    0.235067   1.231224   0.664266                  \n",
      "   1374    0.23045    1.222039   0.66205                   \n",
      "   1375    0.239363   1.2465     0.660942                  \n",
      "   1376    0.238853   1.226323   0.663158                  \n",
      "   1377    0.234121   1.211149   0.66759                   \n",
      "   1378    0.237473   1.217371   0.663158                  \n",
      "   1379    0.240736   1.220738   0.665928                  \n",
      "   1380    0.228611   1.201754   0.663158                  \n",
      "   1381    0.228364   1.200882   0.66205                   \n",
      "   1382    0.226823   1.201402   0.66759                   \n",
      "   1383    0.231037   1.204377   0.668698                  \n",
      "   1384    0.236363   1.219473   0.66482                   \n",
      "   1385    0.238192   1.216904   0.665928                  \n",
      "   1386    0.235776   1.218166   0.662604                  \n",
      "   1387    0.24262    1.208106   0.660388                  \n",
      "   1388    0.229681   1.214922   0.663712                  \n",
      "   1389    0.220709   1.220142   0.665374                  \n",
      "   1390    0.217221   1.20796    0.661496                  \n",
      "   1391    0.226451   1.206894   0.664266                  \n",
      "   1392    0.234754   1.213121   0.66482                   \n",
      "   1393    0.227159   1.211247   0.666482                  \n",
      "   1394    0.224719   1.214596   0.663158                  \n",
      "   1395    0.219268   1.211583   0.66482                   \n",
      "   1396    0.219747   1.223001   0.665928                  \n",
      "   1397    0.220067   1.219629   0.664266                  \n",
      "   1398    0.226556   1.214139   0.663158                  \n",
      "   1399    0.221143   1.206579   0.66205                   \n",
      "   1400    0.225453   1.213627   0.663712                  \n",
      "   1401    0.217853   1.209668   0.665928                  \n",
      "   1402    0.219398   1.216296   0.665374                  \n",
      "   1403    0.226524   1.211571   0.664266                  \n",
      "   1404    0.22929    1.22313    0.664266                  \n",
      "   1405    0.236796   1.237785   0.668144                  \n",
      "   1406    0.230481   1.204094   0.666482                  \n",
      "   1407    0.227399   1.209408   0.664266                  \n",
      "   1408    0.223946   1.22159    0.667036                  \n",
      "   1409    0.228678   1.213898   0.66482                   \n",
      "   1410    0.230726   1.22341    0.665928                  \n",
      "   1411    0.222723   1.222766   0.666482                  \n",
      "   1412    0.222011   1.224598   0.66482                   \n",
      "   1413    0.219729   1.218096   0.667036                  \n",
      "   1414    0.218326   1.213272   0.663158                  \n",
      "   1415    0.231712   1.210369   0.662604                  \n",
      "   1416    0.229544   1.20452    0.663712                  \n",
      "   1417    0.220289   1.219007   0.666482                  \n",
      "   1418    0.222226   1.223234   0.663712                  \n",
      "   1419    0.226586   1.202305   0.663712                  \n",
      "   1420    0.219824   1.216177   0.664266                  \n",
      "   1421    0.216701   1.209778   0.662604                  \n",
      "   1422    0.210737   1.204503   0.662604                  \n",
      "   1423    0.210823   1.23016    0.662604                  \n",
      "   1424    0.216415   1.210643   0.663712                  \n",
      "   1425    0.224396   1.216356   0.661496                  \n",
      "   1426    0.223637   1.223166   0.661496                  \n",
      "   1427    0.224601   1.21386    0.665928                  \n",
      "   1428    0.222409   1.214901   0.65928                   \n",
      "   1429    0.227196   1.200215   0.66759                   \n",
      "   1430    0.224406   1.210812   0.666482                  \n",
      "   1431    0.226474   1.215651   0.663712                  \n",
      "   1432    0.216202   1.229863   0.665928                  \n",
      "   1433    0.218424   1.211449   0.661496                  \n",
      "   1434    0.217994   1.216616   0.666482                  \n",
      "   1435    0.215612   1.233938   0.667036                  \n",
      "   1436    0.21829    1.217802   0.665374                  \n",
      "   1437    0.217973   1.2115     0.664266                  \n",
      "   1438    0.215677   1.213487   0.66482                   \n",
      "   1439    0.21385    1.214185   0.667036                  \n",
      "   1440    0.22147    1.23158    0.666482                  \n",
      "   1441    0.215819   1.223832   0.668698                  \n",
      "   1442    0.227137   1.23501    0.667036                  \n",
      "   1443    0.219171   1.244026   0.665374                  \n",
      "   1444    0.219843   1.23467    0.66759                   \n",
      "   1445    0.225801   1.222867   0.670914                  \n",
      "   1446    0.225212   1.198854   0.664266                  \n",
      "   1447    0.234074   1.21856    0.666482                  \n",
      "   1448    0.234749   1.209932   0.66205                   \n",
      "   1449    0.230453   1.190076   0.667036                  \n",
      "   1450    0.227195   1.210914   0.663158                  \n",
      "   1451    0.223816   1.204708   0.662604                  \n",
      "   1452    0.227484   1.204789   0.662604                  \n",
      "   1453    0.229548   1.201653   0.663712                  \n",
      "   1454    0.23246    1.221344   0.661496                  \n",
      "   1455    0.223633   1.224228   0.660942                  \n",
      "   1456    0.236157   1.213291   0.65928                   \n",
      "   1457    0.229791   1.213738   0.66205                   \n",
      "   1458    0.227805   1.224972   0.664266                  \n",
      "   1459    0.243657   1.215667   0.665928                  \n",
      "   1460    0.229813   1.212283   0.663158                  \n",
      "   1461    0.231981   1.219106   0.668698                  \n",
      "   1462    0.230611   1.210264   0.663158                  \n",
      "   1463    0.22405    1.19521    0.666482                  \n",
      "   1464    0.224183   1.233641   0.663712                  \n",
      "   1465    0.221351   1.211237   0.664266                  \n",
      "   1466    0.223121   1.213966   0.66482                   \n",
      "   1467    0.222962   1.211682   0.663712                  \n",
      "   1468    0.223867   1.216012   0.659834                  \n",
      "   1469    0.226579   1.21879    0.662604                  \n",
      "   1470    0.219178   1.201898   0.663712                  \n",
      "   1471    0.230959   1.2221     0.657618                  \n",
      "   1472    0.226738   1.223187   0.660942                  \n",
      "   1473    0.215128   1.218095   0.65928                   \n",
      "   1474    0.216615   1.235378   0.659834                  \n",
      "   1475    0.217007   1.241654   0.663712                  \n",
      "   1476    0.214529   1.234725   0.66482                   \n",
      "   1477    0.227225   1.234681   0.661496                  \n",
      "   1478    0.218856   1.228423   0.662604                  \n",
      "   1479    0.219087   1.252127   0.666482                  \n",
      "   1480    0.217425   1.222438   0.664266                  \n",
      "   1481    0.224567   1.219947   0.660942                  \n",
      "   1482    0.232416   1.251126   0.663712                  \n",
      "   1483    0.227598   1.22359    0.66482                   \n",
      "   1484    0.23645    1.22969    0.664266                  \n",
      "   1485    0.224287   1.227022   0.664266                  \n",
      "   1486    0.225491   1.222565   0.66205                   \n",
      "   1487    0.228586   1.188966   0.664266                  \n",
      "   1488    0.232867   1.201365   0.663712                  \n",
      "   1489    0.235788   1.22308    0.66482                   \n",
      "   1490    0.226239   1.203811   0.661496                  \n",
      "   1491    0.219357   1.216511   0.666482                  \n",
      "   1492    0.224547   1.21051    0.660942                  \n",
      "   1493    0.229158   1.215611   0.663712                  \n",
      "   1494    0.224714   1.229536   0.661496                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1495    0.226011   1.198777   0.658726                  \n",
      "   1496    0.224254   1.214101   0.663712                  \n",
      "   1497    0.21389    1.216697   0.66482                   \n",
      "   1498    0.210199   1.236979   0.663158                  \n",
      "   1499    0.22745    1.225686   0.660942                  \n",
      "   1500    0.235822   1.234507   0.661496                  \n",
      "   1501    0.227072   1.22023    0.65928                   \n",
      "   1502    0.224454   1.235559   0.664266                  \n",
      "   1503    0.236969   1.243741   0.661496                  \n",
      "   1504    0.223586   1.223963   0.66205                   \n",
      "   1505    0.223235   1.220535   0.662604                  \n",
      "   1506    0.226862   1.23299    0.655956                  \n",
      "   1507    0.217966   1.231612   0.660942                  \n",
      "   1508    0.232948   1.207203   0.662604                  \n",
      "   1509    0.227636   1.216496   0.66205                   \n",
      "   1510    0.226349   1.201305   0.661496                  \n",
      "   1511    0.221423   1.208747   0.661496                  \n",
      "   1512    0.216921   1.230515   0.66205                   \n",
      "   1513    0.215194   1.207027   0.659834                  \n",
      "   1514    0.214895   1.206375   0.66205                   \n",
      "   1515    0.225024   1.229888   0.662604                  \n",
      "   1516    0.221685   1.220853   0.66482                   \n",
      "   1517    0.216483   1.220993   0.658172                  \n",
      "   1518    0.219918   1.237461   0.662604                  \n",
      "   1519    0.225255   1.20126    0.66759                   \n",
      "   1520    0.227064   1.21985    0.665374                  \n",
      "   1521    0.222094   1.221104   0.662604                  \n",
      "   1522    0.23503    1.224815   0.666482                  \n",
      "   1523    0.235843   1.216794   0.66205                   \n",
      "   1524    0.231759   1.228587   0.66205                   \n",
      "   1525    0.229988   1.219711   0.657618                  \n",
      "   1526    0.24412    1.235197   0.65651                   \n",
      "   1527    0.231102   1.235007   0.665928                  \n",
      "   1528    0.229065   1.244401   0.660388                  \n",
      "   1529    0.221592   1.243674   0.658726                  \n",
      "   1530    0.229201   1.242713   0.660388                  \n",
      "   1531    0.241766   1.24724    0.657064                  \n",
      "   1532    0.233736   1.242443   0.662604                  \n",
      "   1533    0.226732   1.229024   0.670914                  \n",
      "   1534    0.222606   1.236718   0.665374                  \n",
      "   1535    0.234174   1.229851   0.662604                  \n",
      "   1536    0.225432   1.233623   0.660942                  \n",
      "   1537    0.229474   1.228899   0.65928                   \n",
      "   1538    0.218988   1.214721   0.660942                  \n",
      "   1539    0.228628   1.217764   0.660388                  \n",
      "   1540    0.23319    1.214697   0.65928                   \n",
      "   1541    0.228946   1.233132   0.661496                  \n",
      "   1542    0.22364    1.227179   0.662604                  \n",
      "   1543    0.217776   1.214502   0.665928                  \n",
      "   1544    0.221265   1.222769   0.66205                   \n",
      "   1545    0.225869   1.220821   0.664266                  \n",
      "   1546    0.221597   1.213893   0.660942                  \n",
      "   1547    0.221774   1.23139    0.659834                  \n",
      "   1548    0.221135   1.221835   0.660942                  \n",
      "   1549    0.221912   1.222168   0.65928                   \n",
      "   1550    0.215203   1.220842   0.66205                   \n",
      "   1551    0.217563   1.212699   0.66759                   \n",
      "   1552    0.220511   1.211182   0.66759                   \n",
      "   1553    0.220161   1.241455   0.663158                  \n",
      "   1554    0.217404   1.224881   0.66482                   \n",
      "   1555    0.21417    1.227225   0.66205                   \n",
      "   1556    0.218426   1.226104   0.659834                  \n",
      "   1557    0.218585   1.232892   0.665374                  \n",
      "   1558    0.225371   1.227993   0.665928                  \n",
      "   1559    0.220514   1.226189   0.660942                  \n",
      "   1560    0.224042   1.2396     0.659834                  \n",
      "   1561    0.22836    1.230212   0.664266                  \n",
      "   1562    0.220832   1.231309   0.660388                  \n",
      "   1563    0.221032   1.240558   0.665928                  \n",
      "   1564    0.216892   1.244374   0.65928                   \n",
      "   1565    0.215551   1.224389   0.662604                  \n",
      "   1566    0.211303   1.228438   0.666482                  \n",
      "   1567    0.21351    1.226627   0.66482                   \n",
      "   1568    0.226447   1.23671    0.66482                   \n",
      "   1569    0.231282   1.271238   0.660942                  \n",
      "   1570    0.228312   1.242529   0.660388                  \n",
      "   1571    0.223251   1.240697   0.665928                  \n",
      "   1572    0.22422    1.238651   0.664266                  \n",
      "   1573    0.230596   1.221024   0.664266                  \n",
      "   1574    0.223335   1.232006   0.662604                  \n",
      "   1575    0.221054   1.227688   0.668698                  \n",
      "   1576    0.218926   1.227039   0.663712                  \n",
      "   1577    0.224381   1.237741   0.663158                  \n",
      "   1578    0.228843   1.238949   0.66205                   \n",
      "   1579    0.219267   1.24861    0.660942                  \n",
      "   1580    0.226452   1.246515   0.66205                   \n",
      "   1581    0.229927   1.248401   0.665374                  \n",
      "   1582    0.219789   1.223089   0.664266                  \n",
      "   1583    0.221586   1.224143   0.669806                  \n",
      "   1584    0.216842   1.241158   0.665374                  \n",
      "   1585    0.218246   1.237848   0.662604                  \n",
      "   1586    0.221406   1.240453   0.663712                  \n",
      "   1587    0.247011   1.248384   0.661496                  \n",
      "   1588    0.23224    1.228158   0.664266                  \n",
      "   1589    0.219974   1.241056   0.663712                  \n",
      "   1590    0.216907   1.219554   0.66205                   \n",
      "   1591    0.217622   1.21946    0.667036                  \n",
      "   1592    0.222016   1.241081   0.665374                  \n",
      "   1593    0.225725   1.230529   0.67036                   \n",
      "   1594    0.24329    1.239411   0.669252                  \n",
      "   1595    0.238785   1.246483   0.668144                  \n",
      "   1596    0.232028   1.233512   0.666482                  \n",
      "   1597    0.234972   1.229128   0.666482                  \n",
      "   1598    0.238495   1.219614   0.665374                  \n",
      "   1599    0.238411   1.210168   0.666482                  \n",
      "   1600    0.238476   1.201926   0.668144                  \n",
      "   1601    0.235178   1.211529   0.65928                   \n",
      "   1602    0.224072   1.206951   0.661496                  \n",
      "   1603    0.225391   1.217393   0.666482                  \n",
      "   1604    0.221558   1.215374   0.661496                  \n",
      "   1605    0.219851   1.223989   0.667036                  \n",
      "   1606    0.222731   1.233268   0.663712                  \n",
      "   1607    0.218126   1.229813   0.662604                  \n",
      "   1608    0.226791   1.224368   0.66205                   \n",
      "   1609    0.221444   1.218996   0.662604                  \n",
      "   1610    0.221877   1.22868    0.663158                  \n",
      "   1611    0.216515   1.227347   0.664266                  \n",
      "   1612    0.213289   1.232002   0.663158                  \n",
      "   1613    0.209546   1.214768   0.660942                  \n",
      "   1614    0.205843   1.239145   0.665374                  \n",
      "   1615    0.2097     1.215953   0.662604                  \n",
      "   1616    0.21776    1.23289    0.664266                  \n",
      "   1617    0.21699    1.226761   0.66205                   \n",
      "   1618    0.211954   1.201663   0.666482                  \n",
      "   1619    0.215349   1.213131   0.661496                  \n",
      "   1620    0.215616   1.224779   0.660388                  \n",
      "   1621    0.218658   1.239467   0.660942                  \n",
      "   1622    0.214985   1.219182   0.665374                  \n",
      "   1623    0.221132   1.226748   0.662604                  \n",
      "   1624    0.227302   1.2156     0.664266                  \n",
      "   1625    0.230333   1.227554   0.663158                  \n",
      "   1626    0.22419    1.214802   0.660942                  \n",
      "   1627    0.214558   1.226942   0.666482                  \n",
      "   1628    0.211099   1.224297   0.66759                   \n",
      "   1629    0.208782   1.209632   0.665928                  \n",
      "   1630    0.208787   1.2455     0.663712                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1631    0.210126   1.243201   0.66205                   \n",
      "   1632    0.218197   1.244636   0.660388                  \n",
      "   1633    0.211471   1.25696    0.662604                  \n",
      "   1634    0.212261   1.23881    0.660942                  \n",
      "   1635    0.221538   1.264029   0.66205                   \n",
      "   1636    0.217076   1.221674   0.66205                   \n",
      "   1637    0.21953    1.218598   0.660388                  \n",
      "   1638    0.215088   1.237291   0.661496                  \n",
      "   1639    0.217981   1.21641    0.665374                  \n",
      "   1640    0.218332   1.232089   0.66205                   \n",
      "   1641    0.215902   1.232282   0.66482                   \n",
      "   1642    0.215801   1.229367   0.660942                  \n",
      "   1643    0.211844   1.237578   0.661496                  \n",
      "   1644    0.215241   1.224008   0.658726                  \n",
      "   1645    0.219296   1.249096   0.660942                  \n",
      "   1646    0.222529   1.228623   0.665374                  \n",
      "   1647    0.225768   1.221579   0.664266                  \n",
      "   1648    0.22275    1.250642   0.663712                  \n",
      "   1649    0.225612   1.243503   0.662604                  \n",
      "   1650    0.21939    1.225373   0.661496                  \n",
      "   1651    0.221634   1.232202   0.661496                  \n",
      "   1652    0.218043   1.242524   0.662604                  \n",
      "   1653    0.232112   1.243819   0.665374                  \n",
      "   1654    0.244003   1.246927   0.661496                  \n",
      "   1655    0.233425   1.229962   0.663712                  \n",
      "   1656    0.220321   1.233817   0.665928                  \n",
      "   1657    0.220545   1.244856   0.665374                  \n",
      "   1658    0.229165   1.252673   0.65928                   \n",
      "   1659    0.222841   1.222449   0.65928                   \n",
      "   1660    0.22254    1.231162   0.65651                   \n",
      "   1661    0.219805   1.248247   0.65928                   \n",
      "   1662    0.21524    1.250607   0.658726                  \n",
      "   1663    0.226779   1.238129   0.658172                  \n",
      "   1664    0.226563   1.22967    0.657064                  \n",
      "   1665    0.229091   1.232285   0.658726                  \n",
      "   1666    0.227943   1.218163   0.659834                  \n",
      "   1667    0.228009   1.229068   0.658726                  \n",
      "   1668    0.238915   1.233679   0.658172                  \n",
      "   1669    0.229127   1.229915   0.659834                  \n",
      "   1670    0.22528    1.243986   0.65928                   \n",
      "   1671    0.228138   1.229338   0.663712                  \n",
      "   1672    0.230504   1.239734   0.663158                  \n",
      "   1673    0.225425   1.242284   0.663712                  \n",
      "   1674    0.219517   1.226896   0.663158                  \n",
      "   1675    0.227759   1.236136   0.659834                  \n",
      "   1676    0.228395   1.230133   0.664266                  \n",
      "   1677    0.226777   1.214779   0.660942                  \n",
      "   1678    0.222761   1.23716    0.663158                  \n",
      "   1679    0.221115   1.250249   0.666482                  \n",
      "   1680    0.226142   1.263943   0.658726                  \n",
      "   1681    0.229048   1.232243   0.661496                  \n",
      "   1682    0.219667   1.223226   0.665374                  \n",
      "   1683    0.219275   1.232416   0.663158                  \n",
      "   1684    0.224104   1.223683   0.668144                  \n",
      "   1685    0.224219   1.220652   0.663158                  \n",
      "   1686    0.225573   1.241515   0.662604                  \n",
      "   1687    0.213301   1.238875   0.668698                  \n",
      "   1688    0.214464   1.237434   0.663158                  \n",
      "   1689    0.216372   1.244136   0.663158                  \n",
      "   1690    0.216181   1.257133   0.665928                  \n",
      "   1691    0.224818   1.255429   0.66759                   \n",
      "   1692    0.229384   1.244828   0.664266                  \n",
      "   1693    0.232812   1.24819    0.66205                   \n",
      "   1694    0.221285   1.238243   0.661496                  \n",
      "   1695    0.222187   1.217501   0.664266                  \n",
      "   1696    0.21711    1.235142   0.669806                  \n",
      "   1697    0.209066   1.229485   0.666482                  \n",
      "   1698    0.212948   1.237169   0.666482                  \n",
      "   1699    0.216105   1.241493   0.663158                  \n",
      "   1700    0.21832    1.242482   0.661496                  \n",
      "   1701    0.214159   1.23007    0.661496                  \n",
      "   1702    0.208709   1.245535   0.665928                  \n",
      "   1703    0.208608   1.257726   0.664266                  \n",
      "   1704    0.227563   1.244125   0.668698                  \n",
      "   1705    0.233291   1.232366   0.665928                  \n",
      "   1706    0.231357   1.255464   0.658726                  \n",
      "   1707    0.236328   1.220394   0.66205                   \n",
      "   1708    0.232757   1.245507   0.669252                  \n",
      "   1709    0.225565   1.243069   0.665374                  \n",
      "   1710    0.242382   1.242653   0.660942                  \n",
      "   1711    0.239639   1.235353   0.66482                   \n",
      "   1712    0.236455   1.218709   0.660942                  \n",
      "   1713    0.232994   1.222787   0.663158                  \n",
      "   1714    0.223228   1.238117   0.66482                   \n",
      "   1715    0.232608   1.246539   0.663712                  \n",
      "   1716    0.218982   1.242871   0.663158                  \n",
      "   1717    0.223153   1.248608   0.662604                  \n",
      "   1718    0.213506   1.228447   0.659834                  \n",
      "   1719    0.220772   1.247855   0.663158                  \n",
      "   1720    0.226471   1.241512   0.660942                  \n",
      "   1721    0.226669   1.236038   0.66482                   \n",
      "   1722    0.221114   1.258926   0.66482                   \n",
      "   1723    0.218265   1.246343   0.664266                  \n",
      "   1724    0.22503    1.243442   0.66482                   \n",
      "   1725    0.219834   1.260058   0.663712                  \n",
      "   1726    0.217378   1.247267   0.663712                  \n",
      "   1727    0.221116   1.240454   0.663158                  \n",
      "   1728    0.226607   1.263965   0.661496                  \n",
      "   1729    0.218042   1.232253   0.666482                  \n",
      "   1730    0.219989   1.259235   0.66482                   \n",
      "   1731    0.231898   1.24799    0.666482                  \n",
      "   1732    0.233567   1.255259   0.66482                   \n",
      "   1733    0.247862   1.23893    0.66482                   \n",
      "   1734    0.237487   1.256631   0.663158                  \n",
      "   1735    0.225107   1.272303   0.663158                  \n",
      "   1736    0.222255   1.254161   0.662604                  \n",
      "   1737    0.220648   1.24961    0.66205                   \n",
      "   1738    0.218967   1.247041   0.66759                   \n",
      "   1739    0.219949   1.244546   0.66205                   \n",
      "   1740    0.224663   1.244568   0.668144                  \n",
      "   1741    0.220382   1.232447   0.664266                  \n",
      "   1742    0.214829   1.245493   0.669252                  \n",
      "   1743    0.212913   1.263367   0.66205                   \n",
      "   1744    0.211351   1.261755   0.664266                  \n",
      "   1745    0.207858   1.269638   0.658726                  \n",
      "   1746    0.211632   1.257274   0.663712                  \n",
      "   1747    0.216625   1.260996   0.663712                  \n",
      "   1748    0.220737   1.251376   0.65928                   \n",
      "   1749    0.23405    1.266938   0.663158                  \n",
      "   1750    0.228002   1.257942   0.65928                   \n",
      "   1751    0.231213   1.256749   0.66205                   \n",
      "   1752    0.220727   1.228736   0.660388                  \n",
      "   1753    0.222798   1.240819   0.662604                  \n",
      "   1754    0.225871   1.227922   0.665928                  \n",
      "   1755    0.22419    1.234263   0.664266                  \n",
      "   1756    0.219009   1.230631   0.663158                  \n",
      "   1757    0.230088   1.24909    0.660942                  \n",
      "   1758    0.234599   1.239435   0.660942                  \n",
      "   1759    0.2264     1.238658   0.658726                  \n",
      "   1760    0.242474   1.24794    0.66205                   \n",
      "   1761    0.236537   1.219471   0.663158                  \n",
      "   1762    0.237383   1.237197   0.662604                  \n",
      "   1763    0.226383   1.254116   0.657064                  \n",
      "   1764    0.226349   1.24484    0.659834                  \n",
      "   1765    0.233786   1.23494    0.66205                   \n",
      "   1766    0.224367   1.233663   0.65928                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1767    0.228842   1.229618   0.660942                  \n",
      "   1768    0.226111   1.238544   0.652632                  \n",
      "   1769    0.224172   1.224287   0.65651                   \n",
      "   1770    0.227132   1.242088   0.654294                  \n",
      "   1771    0.23064    1.248279   0.654848                  \n",
      "   1772    0.218404   1.23965    0.660942                  \n",
      "   1773    0.224761   1.249521   0.658726                  \n",
      "   1774    0.228497   1.247503   0.65928                   \n",
      "   1775    0.219317   1.234347   0.660388                  \n",
      "   1776    0.236978   1.227065   0.660388                  \n",
      "   1777    0.22975    1.248346   0.66205                   \n",
      "   1778    0.234137   1.244616   0.654294                  \n",
      "   1779    0.228212   1.249794   0.658726                  \n",
      "   1780    0.222226   1.240656   0.65928                   \n",
      "   1781    0.231475   1.256066   0.65928                   \n",
      "   1782    0.227263   1.237212   0.659834                  \n",
      "   1783    0.232167   1.245671   0.65928                   \n",
      "   1784    0.234249   1.240187   0.65651                   \n",
      "   1785    0.22292    1.267412   0.657064                  \n",
      "   1786    0.21982    1.249702   0.657064                  \n",
      "   1787    0.217372   1.251181   0.659834                  \n",
      "   1788    0.221853   1.261902   0.658726                  \n",
      "   1789    0.217369   1.238025   0.658172                  \n",
      "   1790    0.217544   1.263413   0.65651                   \n",
      "   1791    0.241733   1.27774    0.657064                  \n",
      "   1792    0.233876   1.259323   0.658726                  \n",
      "   1793    0.227666   1.270267   0.655402                  \n",
      "   1794    0.220609   1.229735   0.657618                  \n",
      "   1795    0.222352   1.26306    0.654848                  \n",
      "   1796    0.227959   1.247091   0.660388                  \n",
      "   1797    0.221476   1.257479   0.653186                  \n",
      "   1798    0.220013   1.242583   0.663158                  \n",
      "   1799    0.215074   1.247572   0.663158                  \n",
      "   1800    0.212589   1.266306   0.663158                  \n",
      "   1801    0.218625   1.24909    0.660388                  \n",
      "   1802    0.217672   1.278335   0.659834                  \n",
      "   1803    0.219541   1.241444   0.665374                  \n",
      "   1804    0.212933   1.239095   0.660942                  \n",
      "   1805    0.215328   1.244604   0.660388                  \n",
      "   1806    0.224767   1.239959   0.663158                  \n",
      "   1807    0.22351    1.248893   0.66759                   \n",
      "   1808    0.214174   1.2737     0.666482                  \n",
      "   1809    0.214683   1.248877   0.659834                  \n",
      "   1810    0.213581   1.262391   0.662604                  \n",
      "   1811    0.210646   1.252551   0.662604                  \n",
      "   1812    0.218055   1.243327   0.665374                  \n",
      "   1813    0.224287   1.25174    0.663158                  \n",
      "   1814    0.226998   1.282772   0.660388                  \n",
      "   1815    0.225239   1.278525   0.660388                  \n",
      "   1816    0.236161   1.269633   0.666482                  \n",
      "   1817    0.246669   1.282132   0.65928                   \n",
      "   1818    0.236844   1.255179   0.663158                  \n",
      "   1819    0.234817   1.256411   0.660942                  \n",
      "   1820    0.234369   1.268338   0.65651                   \n",
      "   1821    0.237165   1.257693   0.657618                  \n",
      "   1822    0.239596   1.269052   0.659834                  \n",
      "   1823    0.236167   1.257154   0.660388                  \n",
      "   1824    0.223875   1.24909    0.65928                   \n",
      "   1825    0.233844   1.267165   0.661496                  \n",
      "   1826    0.237379   1.279048   0.661496                  \n",
      "   1827    0.223279   1.247172   0.663158                  \n",
      "   1828    0.227338   1.254891   0.65928                   \n",
      "   1829    0.224426   1.269154   0.659834                  \n",
      "   1830    0.222813   1.255558   0.660388                  \n",
      "   1831    0.218594   1.242439   0.65651                   \n",
      "   1832    0.219883   1.23893    0.65651                   \n",
      "   1833    0.222544   1.245416   0.660942                  \n",
      "   1834    0.231984   1.252949   0.655956                  \n",
      "   1835    0.223136   1.255762   0.659834                  \n",
      "   1836    0.218237   1.241149   0.657618                  \n",
      "   1837    0.226349   1.235792   0.660942                  \n",
      "   1838    0.231661   1.243205   0.660942                  \n",
      "   1839    0.224052   1.23318    0.659834                  \n",
      "   1840    0.222289   1.244608   0.657064                  \n",
      "   1841    0.228297   1.233111   0.65928                   \n",
      "   1842    0.222612   1.226044   0.660388                  \n",
      "   1843    0.227642   1.249681   0.661496                  \n",
      "   1844    0.222503   1.243896   0.66205                   \n",
      "   1845    0.222684   1.240578   0.658172                  \n",
      "   1846    0.219147   1.239892   0.655402                  \n",
      "   1847    0.211571   1.247649   0.655956                  \n",
      "   1848    0.219301   1.24641    0.660388                  \n",
      "   1849    0.213371   1.242122   0.65928                   \n",
      "   1850    0.214687   1.257898   0.658172                  \n",
      "   1851    0.215596   1.251526   0.658172                  \n",
      "   1852    0.210847   1.252597   0.657064                  \n",
      "   1853    0.216494   1.257454   0.658726                  \n",
      "   1854    0.210869   1.244429   0.66205                   \n",
      "   1855    0.224977   1.250159   0.661496                  \n",
      "   1856    0.215068   1.246718   0.659834                  \n",
      "   1857    0.222078   1.268243   0.660942                  \n",
      "   1858    0.222216   1.281489   0.660388                  \n",
      "   1859    0.221826   1.284227   0.660388                  \n",
      "   1860    0.217241   1.286139   0.663158                  \n",
      "   1861    0.213866   1.265699   0.663712                  \n",
      "   1862    0.221966   1.254484   0.655956                  \n",
      "   1863    0.222304   1.270059   0.662604                  \n",
      "   1864    0.218262   1.26832    0.660388                  \n",
      "   1865    0.213294   1.275055   0.65928                   \n",
      "   1866    0.215096   1.258055   0.657618                  \n",
      "   1867    0.221175   1.261757   0.658726                  \n",
      "   1868    0.217107   1.255734   0.659834                  \n",
      "   1869    0.216723   1.265845   0.658726                  \n",
      "   1870    0.218263   1.249046   0.66205                   \n",
      "   1871    0.223971   1.267917   0.660942                  \n",
      "   1872    0.212754   1.260016   0.663712                  \n",
      "   1873    0.218907   1.256374   0.663712                  \n",
      "   1874    0.210367   1.2377     0.665374                  \n",
      "   1875    0.212771   1.262363   0.661496                  \n",
      "   1876    0.219486   1.247404   0.66759                   \n",
      "   1877    0.221789   1.265585   0.658172                  \n",
      "   1878    0.216965   1.269836   0.658726                  \n",
      "   1879    0.218649   1.266571   0.663712                  \n",
      "   1880    0.21983    1.253249   0.655402                  \n",
      "   1881    0.229027   1.260724   0.65651                   \n",
      "   1882    0.220199   1.261372   0.663158                  \n",
      "   1883    0.217083   1.248177   0.658726                  \n",
      "   1884    0.219771   1.259764   0.660942                  \n",
      "   1885    0.220023   1.250064   0.665928                  \n",
      "   1886    0.22918    1.273844   0.65651                   \n",
      "   1887    0.227474   1.279125   0.657618                  \n",
      "   1888    0.225211   1.266741   0.659834                  \n",
      "   1889    0.219206   1.264466   0.66205                   \n",
      "   1890    0.215159   1.268681   0.660942                  \n",
      "   1891    0.2158     1.241442   0.664266                  \n",
      "   1892    0.214365   1.264009   0.66205                   \n",
      "   1893    0.218847   1.254217   0.668698                  \n",
      "   1894    0.217244   1.243371   0.66205                   \n",
      "   1895    0.21776    1.254443   0.663158                  \n",
      "   1896    0.216272   1.269194   0.666482                  \n",
      "   1897    0.210621   1.234624   0.666482                  \n",
      "   1898    0.21161    1.25674    0.661496                  \n",
      "   1899    0.223111   1.255166   0.665374                  \n",
      "   1900    0.22019    1.258279   0.66205                   \n",
      "   1901    0.22325    1.262748   0.65928                   \n",
      "   1902    0.21589    1.238746   0.664266                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1903    0.210084   1.271956   0.666482                  \n",
      "   1904    0.205231   1.247098   0.660388                  \n",
      "   1905    0.21975    1.28063    0.66205                   \n",
      "   1906    0.215612   1.254798   0.659834                  \n",
      "   1907    0.214528   1.256087   0.66482                   \n",
      "   1908    0.216745   1.252875   0.664266                  \n",
      "   1909    0.21301    1.264331   0.66482                   \n",
      "   1910    0.217989   1.282073   0.661496                  \n",
      "   1911    0.214654   1.265579   0.65928                   \n",
      "   1912    0.218895   1.276716   0.665928                  \n",
      "   1913    0.211241   1.276125   0.66482                   \n",
      "   1914    0.221919   1.274231   0.660942                  \n",
      "   1915    0.217209   1.286281   0.665928                  \n",
      "   1916    0.218515   1.271908   0.665374                  \n",
      "   1917    0.215731   1.269308   0.660942                  \n",
      "   1918    0.215676   1.248785   0.661496                  \n",
      "   1919    0.214981   1.276236   0.660942                  \n",
      "   1920    0.223183   1.263375   0.662604                  \n",
      "   1921    0.256088   1.262612   0.662604                  \n",
      "   1922    0.235895   1.265812   0.658172                  \n",
      "   1923    0.225032   1.249296   0.66205                   \n",
      "   1924    0.225234   1.28814    0.662604                  \n",
      "   1925    0.230377   1.255221   0.665928                  \n",
      "   1926    0.223448   1.241275   0.66205                   \n",
      "   1927    0.221928   1.255094   0.66482                   \n",
      "   1928    0.221495   1.254903   0.662604                  \n",
      "   1929    0.236053   1.239092   0.669252                  \n",
      "   1930    0.218012   1.23456    0.672022                  \n",
      "   1931    0.219224   1.248626   0.666482                  \n",
      "   1932    0.225425   1.265653   0.663712                  \n",
      "   1933    0.219858   1.244967   0.668144                  \n",
      "   1934    0.212358   1.238547   0.662604                  \n",
      "   1935    0.214937   1.257703   0.664266                  \n",
      "   1936    0.217046   1.273772   0.663712                  \n",
      "   1937    0.225355   1.249793   0.663158                  \n",
      "   1938    0.21898    1.241647   0.663158                  \n",
      "   1939    0.212964   1.232579   0.660388                  \n",
      "   1940    0.209266   1.25289    0.664266                  \n",
      "   1941    0.223204   1.252883   0.665928                  \n",
      "   1942    0.223106   1.272958   0.66205                   \n",
      "   1943    0.22159    1.248972   0.663712                  \n",
      "   1944    0.219841   1.245877   0.66205                   \n",
      "   1945    0.216058   1.252128   0.663158                  \n",
      "   1946    0.209062   1.261159   0.660388                  \n",
      "   1947    0.208295   1.234363   0.65928                   \n",
      "   1948    0.213714   1.234925   0.662604                  \n",
      "   1949    0.217652   1.245795   0.663158                  \n",
      "   1950    0.215963   1.241187   0.663712                  \n",
      "   1951    0.217778   1.231617   0.663712                  \n",
      "   1952    0.218779   1.241899   0.65928                   \n",
      "   1953    0.214826   1.249046   0.66205                   \n",
      "   1954    0.225127   1.253828   0.66482                   \n",
      "   1955    0.222424   1.255397   0.663712                  \n",
      "   1956    0.21669    1.261025   0.657064                  \n",
      "   1957    0.224118   1.265771   0.663158                  \n",
      "   1958    0.214704   1.261155   0.658726                  \n",
      "   1959    0.204309   1.275461   0.658172                  \n",
      "   1960    0.213464   1.268956   0.665928                  \n",
      "   1961    0.210478   1.263407   0.665928                  \n",
      "   1962    0.22407    1.260714   0.663158                  \n",
      "   1963    0.220379   1.257636   0.659834                  \n",
      "   1964    0.220905   1.277343   0.657064                  \n",
      "   1965    0.229511   1.277849   0.658726                  \n",
      "   1966    0.229006   1.239437   0.658726                  \n",
      "   1967    0.221435   1.250659   0.660942                  \n",
      "   1968    0.216706   1.259009   0.658726                  \n",
      "   1969    0.213896   1.26195    0.660942                  \n",
      "   1970    0.221064   1.255289   0.659834                  \n",
      "   1971    0.216323   1.244125   0.660942                  \n",
      "   1972    0.211188   1.262997   0.660942                  \n",
      "   1973    0.211919   1.275802   0.658726                  \n",
      "   1974    0.210017   1.282002   0.660942                  \n",
      "   1975    0.207723   1.260761   0.660942                  \n",
      "   1976    0.211227   1.271579   0.66482                   \n",
      "   1977    0.209235   1.260034   0.665928                  \n",
      "   1978    0.214122   1.263502   0.664266                  \n",
      "   1979    0.218164   1.262453   0.66482                   \n",
      "   1980    0.220497   1.258675   0.65928                   \n",
      "   1981    0.215535   1.277284   0.659834                  \n",
      "   1982    0.211909   1.257631   0.662604                  \n",
      "   1983    0.219272   1.257506   0.667036                  \n",
      "   1984    0.214525   1.25353    0.663712                  \n",
      "   1985    0.216798   1.26776    0.662604                  \n",
      "   1986    0.218481   1.269023   0.664266                  \n",
      "   1987    0.211177   1.264483   0.661496                  \n",
      "   1988    0.214571   1.284933   0.663158                  \n",
      "   1989    0.218378   1.271509   0.664266                  \n",
      "   1990    0.219252   1.266102   0.661496                  \n",
      "   1991    0.222734   1.286534   0.66482                   \n",
      "   1992    0.226864   1.265121   0.663158                  \n",
      "   1993    0.225399   1.283824   0.664266                  \n",
      "   1994    0.215614   1.280699   0.664266                  \n",
      "   1995    0.222189   1.264408   0.667036                  \n",
      "   1996    0.223284   1.264449   0.664266                  \n",
      "   1997    0.225907   1.265335   0.662604                  \n",
      "   1998    0.224899   1.266169   0.664266                  \n",
      "   1999    0.224425   1.269298   0.66759                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.2693]), 0.6675900252241838]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(1e-2, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.precompute = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c844ad59c44cc6ac57e598f95498d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                  \n",
      "    0      0.961866   1.313622   0.665374  \n",
      "    1      0.897099   1.277027   0.664266                  \n",
      "    2      0.899265   1.231513   0.659834                  \n",
      "    3      0.893445   1.214194   0.660942                  \n",
      "    4      0.86523    1.168033   0.66205                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.16803]), 0.66204985687277]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(1e-2, 5, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('224_preF_b58')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('224_preF_b58')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96784f7dee6249c891fdde4ff2d4f690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Starting training on small images for a few epochs, then switching to bigger images, and continuing training is an amazingly effective way to avoid overfitting.\n",
    "\n",
    "# http://forums.fast.ai/t/planet-classification-challenge/7824/96\n",
    "# set_data doesn’t change the model at all. It just gives it new data to train with.\n",
    "learn.set_data(get_data(299, 80)) \n",
    "learn.freeze()\n",
    "\n",
    "#Source:   \n",
    "#    def set_data(self, data, precompute=False):\n",
    "#        super().set_data(data)\n",
    "#        if precompute:\n",
    "#            self.unfreeze()\n",
    "#            self.save_fc1()\n",
    "#            self.freeze()\n",
    "#            self.precompute = True\n",
    "#        else:\n",
    "#            self.freeze()\n",
    "#File:      ~/fastai/courses/dl1/fastai/conv_learner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Conv2d-1',\n",
       "              OrderedDict([('input_shape', [-1, 3, 299, 299]),\n",
       "                           ('output_shape', [-1, 64, 150, 150]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 9408)])),\n",
       "             ('BatchNorm2d-2',\n",
       "              OrderedDict([('input_shape', [-1, 64, 150, 150]),\n",
       "                           ('output_shape', [-1, 64, 150, 150]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 128)])),\n",
       "             ('ReLU-3',\n",
       "              OrderedDict([('input_shape', [-1, 64, 150, 150]),\n",
       "                           ('output_shape', [-1, 64, 150, 150]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('MaxPool2d-4',\n",
       "              OrderedDict([('input_shape', [-1, 64, 150, 150]),\n",
       "                           ('output_shape', [-1, 64, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-5',\n",
       "              OrderedDict([('input_shape', [-1, 64, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 16384)])),\n",
       "             ('BatchNorm2d-6',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-7',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-8',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 9216)])),\n",
       "             ('BatchNorm2d-9',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-10',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-11',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-12',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('Conv2d-13',\n",
       "              OrderedDict([('input_shape', [-1, 64, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 16384)])),\n",
       "             ('BatchNorm2d-14',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-15',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-16',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-17',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-18',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-19',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 9216)])),\n",
       "             ('BatchNorm2d-20',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-21',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-22',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-23',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-24',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-25',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-26',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-27',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-28',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 9216)])),\n",
       "             ('BatchNorm2d-29',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-30',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-31',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 65536)])),\n",
       "             ('BatchNorm2d-32',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('ReLU-33',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 256, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-34',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 512, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 131072)])),\n",
       "             ('BatchNorm2d-35',\n",
       "              OrderedDict([('input_shape', [-1, 512, 75, 75]),\n",
       "                           ('output_shape', [-1, 512, 75, 75]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-36',\n",
       "              OrderedDict([('input_shape', [-1, 512, 75, 75]),\n",
       "                           ('output_shape', [-1, 512, 75, 75]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-37',\n",
       "              OrderedDict([('input_shape', [-1, 512, 75, 75]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 36864)])),\n",
       "             ('BatchNorm2d-38',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-39',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-40',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-41',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('Conv2d-42',\n",
       "              OrderedDict([('input_shape', [-1, 256, 75, 75]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 131072)])),\n",
       "             ('BatchNorm2d-43',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-44',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-45',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-46',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-47',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-48',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 36864)])),\n",
       "             ('BatchNorm2d-49',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-50',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-51',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-52',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-53',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-54',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-55',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-56',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-57',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 36864)])),\n",
       "             ('BatchNorm2d-58',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-59',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-60',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-61',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-62',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-63',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-64',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-65',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-66',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 36864)])),\n",
       "             ('BatchNorm2d-67',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-68',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-69',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 262144)])),\n",
       "             ('BatchNorm2d-70',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('ReLU-71',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 512, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-72',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 1024, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 524288)])),\n",
       "             ('BatchNorm2d-73',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 38, 38]),\n",
       "                           ('output_shape', [-1, 1024, 38, 38]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-74',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 38, 38]),\n",
       "                           ('output_shape', [-1, 1024, 38, 38]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-75',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 38, 38]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-76',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-77',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-78',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-79',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('Conv2d-80',\n",
       "              OrderedDict([('input_shape', [-1, 512, 38, 38]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 524288)])),\n",
       "             ('BatchNorm2d-81',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-82',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-83',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-84',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-85',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-86',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-87',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-88',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-89',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-90',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-91',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-92',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-93',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-94',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-95',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-96',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-97',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-98',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-99',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-100',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-101',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-102',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-103',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-104',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-105',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-106',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-107',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-108',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-109',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-110',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-111',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-112',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-113',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-114',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-115',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-116',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-117',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-118',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-119',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-120',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-121',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-122',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-123',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-124',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-125',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-126',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-127',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-128',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-129',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-130',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-131',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-132',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-133',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-134',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-135',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-136',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-137',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-138',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-139',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-140',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-141',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-142',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-143',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-144',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-145',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-146',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-147',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-148',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-149',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-150',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-151',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-152',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-153',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-154',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-155',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-156',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-157',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-158',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-159',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-160',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-161',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-162',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-163',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-164',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-165',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-166',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-167',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-168',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-169',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-170',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-171',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-172',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-173',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-174',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-175',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-176',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-177',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-178',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-179',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-180',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-181',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-182',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-183',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-184',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-185',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-186',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-187',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-188',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-189',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-190',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-191',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-192',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-193',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-194',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-195',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-196',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-197',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-198',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-199',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-200',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-201',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-202',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-203',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-204',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-205',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-206',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-207',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-208',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-209',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-210',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-211',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-212',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-213',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-214',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-215',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-216',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-217',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-218',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-219',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-220',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-221',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-222',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-223',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-224',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-225',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-226',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-227',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-228',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-229',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-230',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-231',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-232',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-233',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-234',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-235',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-236',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-237',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-238',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-239',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-240',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-241',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-242',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-243',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-244',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-245',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-246',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-247',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-248',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-249',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-250',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-251',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-252',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-253',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-254',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-255',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-256',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-257',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-258',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-259',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-260',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-261',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-262',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-263',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-264',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-265',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-266',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-267',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-268',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-269',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-270',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-271',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-272',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-273',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-274',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-275',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 147456)])),\n",
       "             ('BatchNorm2d-276',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-277',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-278',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 1048576)])),\n",
       "             ('BatchNorm2d-279',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2048)])),\n",
       "             ('ReLU-280',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 1024, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-281',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 2048, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2097152)])),\n",
       "             ('BatchNorm2d-282',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 19, 19]),\n",
       "                           ('output_shape', [-1, 2048, 19, 19]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-283',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 19, 19]),\n",
       "                           ('output_shape', [-1, 2048, 19, 19]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-284',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 19, 19]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 589824)])),\n",
       "             ('BatchNorm2d-285',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-286',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-287',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-288',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('Conv2d-289',\n",
       "              OrderedDict([('input_shape', [-1, 1024, 19, 19]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 2097152)])),\n",
       "             ('BatchNorm2d-290',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-291',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-292',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-293',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-294',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-295',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 589824)])),\n",
       "             ('BatchNorm2d-296',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-297',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-298',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-299',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-300',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-301',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-302',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-303',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-304',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 589824)])),\n",
       "             ('BatchNorm2d-305',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-306',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Conv2d-307',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4194304)])),\n",
       "             ('BatchNorm2d-308',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('trainable', False),\n",
       "                           ('nb_params', 4096)])),\n",
       "             ('ReLU-309',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 10, 10]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('AdaptiveMaxPool2d-310',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 1, 1]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('AdaptiveAvgPool2d-311',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 2048, 1, 1]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('AdaptiveConcatPool2d-312',\n",
       "              OrderedDict([('input_shape', [-1, 2048, 10, 10]),\n",
       "                           ('output_shape', [-1, 4096, 1, 1]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Flatten-313',\n",
       "              OrderedDict([('input_shape', [-1, 4096, 1, 1]),\n",
       "                           ('output_shape', [-1, 4096]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('BatchNorm1d-314',\n",
       "              OrderedDict([('input_shape', [-1, 4096]),\n",
       "                           ('output_shape', [-1, 4096]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 8192)])),\n",
       "             ('Dropout-315',\n",
       "              OrderedDict([('input_shape', [-1, 4096]),\n",
       "                           ('output_shape', [-1, 4096]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Linear-316',\n",
       "              OrderedDict([('input_shape', [-1, 4096]),\n",
       "                           ('output_shape', [-1, 512]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 2097664)])),\n",
       "             ('ReLU-317',\n",
       "              OrderedDict([('input_shape', [-1, 512]),\n",
       "                           ('output_shape', [-1, 512]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('BatchNorm1d-318',\n",
       "              OrderedDict([('input_shape', [-1, 512]),\n",
       "                           ('output_shape', [-1, 512]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 1024)])),\n",
       "             ('Dropout-319',\n",
       "              OrderedDict([('input_shape', [-1, 512]),\n",
       "                           ('output_shape', [-1, 512]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Linear-320',\n",
       "              OrderedDict([('input_shape', [-1, 512]),\n",
       "                           ('output_shape', [-1, 12]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 6156)])),\n",
       "             ('LogSoftmax-321',\n",
       "              OrderedDict([('input_shape', [-1, 12]),\n",
       "                           ('output_shape', [-1, 12]),\n",
       "                           ('nb_params', 0)]))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c463da88530e46be93028e6fa29c03ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.626417   1.426835   0.503524  \n",
      "    1      1.595296   1.415666   0.511013                   \n",
      "    2      1.552414   1.397453   0.514097                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.39745]), 0.514096918203232]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(5e-3, 3, cycle_len=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss is much lower than training loss. This is a sign of underfitting. Cycle_len=1 may be too short. Let's set cycle_mult=2 to find better parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4bd11556e14dc3be65e09eda4dfb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.515581   1.383625   0.522026  \n",
      "    1      1.507568   1.370226   0.519383                   \n",
      "    2      1.471789   1.36212    0.52467                    \n",
      "    3      1.465919   1.36207    0.522026                   \n",
      "    4      1.451652   1.337367   0.533921                   \n",
      "    5      1.427047   1.333412   0.53304                    \n",
      "    6      1.415496   1.328908   0.531278                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.32891]), 0.531277537477174]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When you are under fitting, it means cycle_len=1 is too short (learning rate is getting reset before it had the chance to zoom in properly).\n",
    "learn.fit(1e-2, 3, cycle_len=1, cycle_mult=2) # 1+2+4 = 7 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loss and validation loss are getting closer and smaller. We are on right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5330396475770925, 1.3259945803139326)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_preds, y = learn.TTA() # (5, 2044, 120), (2044,)\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "accuracy_np(probs, y), metrics.log_loss(y, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2270, array([9, 9, 9, 9, 9]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.val_ds.y), data.val_ds.y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('299_pre_bs80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('299_pre_bs80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91aa48489fce4863bbefd3ad30476171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.437424   1.327512   0.537445  \n",
      "    1      1.412686   1.320846   0.529956                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.32085]), 0.5299559487645321]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(1e-2, 1, cycle_len=2) # 1+1 = 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('299_pre_bs80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('299_pre_bs80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5259911894273128, 1.3275131736628687)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_preds, y = learn.TTA()\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "accuracy_np(probs, y), metrics.log_loss(y, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=np.array([5e-5,5e-4,5e-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af18979f8d24f6d83dcac04a6ed7151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.set_data(get_data(299, 48)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668e2c7c4ca64a0d8ee3ea988160294a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 193/228 [01:58<00:20,  1.67it/s, loss=5.47]"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrf=learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0d779acb94408da8bc432d45dc97ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.448863   1.311074   0.534361  \n",
      "    1      1.405124   1.303028   0.538767                   \n",
      "    2      1.396005   1.306464   0.535683                   \n",
      "    3      1.402845   1.303234   0.547137                   \n",
      "    4      1.372364   1.297798   0.544934                   \n",
      "    5      1.342952   1.290582   0.542731                   \n",
      "    6      1.379837   1.28929    0.546696                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.28929]), 0.5466960388395755]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr, 3, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('299_unfroze_bs48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('299_unfroze_bs48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ecb8b5806a417dbfe25ade95d37e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1023), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 51/190 [00:36<01:35,  1.45it/s, loss=1.38]"
     ]
    }
   ],
   "source": [
    "learn.fit(lr, 10, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is so similar to ImageNet dataset. Training convolution layers doesn't help much. We are not going to unfreeze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create submission\n",
    "\n",
    "https://youtu.be/9C06ZPF8Uuc?t=1905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test_ds.fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds, y = learn.TTA(is_test=True) # use test dataset rather than validation dataset\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "#accuracy_np(probs, y), metrcs.log_loss(y, probs) # This does not make sense since test dataset has no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape # (n_images, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(probs)\n",
    "df.columns = data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(0, 'id', [o[5:-4] for o in data.test_ds.fnames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBM = f'{PATH}/subm/'\n",
    "os.makedirs(SUBM, exist_ok=True)\n",
    "df.to_csv(f'{SUBM}subm.gz', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileLink(f'{SUBM}subm.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = data.val_ds.fnames[0]\n",
    "fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(PATH + fn).resize((150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1.\n",
    "trn_tfms, val_tfms = tfms_from_model(arch, sz)\n",
    "ds = FilesIndexArrayDataset([fn], np.array([0]), val_tfms, PATH)\n",
    "dl = DataLoader(ds)\n",
    "preds = learn.predict_dl(dl)\n",
    "np.argmax(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.classes[np.argmax(preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2.\n",
    "trn_tfms, val_tfms = tfms_from_model(arch, sz)\n",
    "im = val_tfms(open_image(PATH + fn)) # open_image() returns numpy.ndarray\n",
    "preds = learn.predict_array(im[None])\n",
    "np.argmax(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastaiold",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "180.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0147ea39a83144aa99e7877a908ca502": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "02af08793c9a4c5ba0cbd7462a715f89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "03c7476e32ce48c2b38a7023c2dcc0a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "075434e4b0234c5fa61c8d63fec51679": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "08ba1e10c1a641f7876680a632f6821d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_b0edcf9ef0754ca4bb7fe6e8790c464b",
       "max": 6,
       "style": "IPY_MODEL_c13bdace91114d639b408aa120c97f79",
       "value": 6
      }
     },
     "0b49f5119c6d4a25bb575a20e33fe219": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "12cc98b1fc954667bd49c08972278f1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "13cbbf3a554a40f09d40df39c2cacf47": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "148c1abd5f1747a2a787c09cef3071bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_902c060a37b4491788462dce6be1798b",
       "style": "IPY_MODEL_88db019106ca4893816ea780e567a175",
       "value": "100% 3/3 [16:30&lt;00:00, 330.17s/it]"
      }
     },
     "16993380d0fa42b888a52dbf1bb42f20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_c74dedf09f57490a9f1ac9af54e14bf3",
       "max": 6,
       "style": "IPY_MODEL_557d35c901f54575b5963e3b7ce932a3",
       "value": 6
      }
     },
     "19c5557812814cb28bc7e0cc80c65c5f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c241c0e5b5844508c1c50e2a7c09b1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c509f99160e41d5af6e565b1541e571": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9352e3fd2af54e09ad67afb4ac446b27",
       "style": "IPY_MODEL_62e7ecfbbea7427385deb25ed2f4232f",
       "value": "100% 6/6 [00:00&lt;00:00, 380.26it/s]"
      }
     },
     "1e2f1c40607144eb9cb7b9e257d06f35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1fc90792a2b74c38aaf73659c3756831": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "201ae86e437e4b5390782e9df272326d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c8871c57d7bd4b61a99633668fb44621",
        "IPY_MODEL_38eb37d7293d4273a6ae06b5a451617d"
       ],
       "layout": "IPY_MODEL_1c241c0e5b5844508c1c50e2a7c09b1b"
      }
     },
     "20bfce64bf354e2398353b6a2a31d0e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "21e75994f5e749748dd5693819e51d93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "22952b3bfa674aff898e452b30bd319e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "26e2b0e7abb24e3fad1b5c0c166aa44d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_363595af2f5f4ad3a3f5a9e46069d58e",
       "max": 5,
       "style": "IPY_MODEL_02af08793c9a4c5ba0cbd7462a715f89",
       "value": 5
      }
     },
     "288366a4c5ba435286c34d6d5ad69201": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_33f7abed940a4b24b01110da8ec67ddc",
        "IPY_MODEL_93f4380114fe48b88db18d0c97736fd4"
       ],
       "layout": "IPY_MODEL_5b17418f64954c14843a914942f8ce3d"
      }
     },
     "292be092bc964dd7a48bc62c906f5814": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2d1dfcb285154a7bac2db184a87f3c4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2ff92f760a2b46a99576ab77daccac1d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "33ce7c9b732e4715ae12b71669addfdf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_20bfce64bf354e2398353b6a2a31d0e5",
       "max": 6,
       "style": "IPY_MODEL_830aa543858f4a54a14aad440bcbde34",
       "value": 6
      }
     },
     "33f7abed940a4b24b01110da8ec67ddc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_7ba0d94e2ef544fe9126eacee6588468",
       "max": 6,
       "style": "IPY_MODEL_8c8cb52ff5e54c55a297af7571a105ea",
       "value": 6
      }
     },
     "363595af2f5f4ad3a3f5a9e46069d58e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "37f00a662a3c4b5f9f29db6903cfc5ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "37faed3c96f343e895d8f29ab07137c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "38eb37d7293d4273a6ae06b5a451617d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_42c23fec7e814a9aaf39ffa59c0ffe20",
       "style": "IPY_MODEL_7163788d004d4c15b0b0137a654079a0",
       "value": "100% 7/7 [35:35&lt;00:00, 305.08s/it]"
      }
     },
     "39cf7d6b5e98460e9a621f1d74e32fad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cb554ddf38bb4597b282655ee74ca321",
       "style": "IPY_MODEL_c45beabad9d34af5b39ce603b5308ef9",
       "value": "100% 5/5 [27:26&lt;00:00, 329.38s/it]"
      }
     },
     "39d7df82d70b40df9f1171dad9ce2b4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3a1160614abc4724b73cc9bc2e0d0d46": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_19c5557812814cb28bc7e0cc80c65c5f",
       "style": "IPY_MODEL_87707de469bc4c4bb2ba2a2ed7322de1",
       "value": "100% 5/5 [00:28&lt;00:00,  5.70s/it]"
      }
     },
     "3b2e29458fdc49d0864a7fc9a1faafb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3ed035ff92d947ad9059d4bc03f2576c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_16993380d0fa42b888a52dbf1bb42f20",
        "IPY_MODEL_a52bd3eee7aa4e9093ee525fc42ff232"
       ],
       "layout": "IPY_MODEL_66e271ee790148eb89ae51b9b22216aa"
      }
     },
     "40060908ceee46fb859195274eca3da8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4b3362ffeac948e490ce91f1e0786f2e",
        "IPY_MODEL_c09f764cc1c74e3aa75fea864a8d21f3"
       ],
       "layout": "IPY_MODEL_e6926f2efb5340528a00c5045376c380"
      }
     },
     "405077ae66ac44afbd35d90a6afd6cab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "41a9d48c29ef4752bcd2ddc7076dac3c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "42c23fec7e814a9aaf39ffa59c0ffe20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "43f1cb9394c5422798a91c1b6b0a5907": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "45a9f25bf3964af68dfce8bbb2091917": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "4654ebf91a484cb8b153920fb89bb522": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4b3362ffeac948e490ce91f1e0786f2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_67e1b7b183484f03b2f63c22c44e8713",
       "max": 6,
       "style": "IPY_MODEL_12cc98b1fc954667bd49c08972278f1d",
       "value": 6
      }
     },
     "4c5e108b0b9145cb9987a673dda89bb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4d7cbd1c9b0b45359d4d31ad4e50e441": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "54f04d9c381446649b742e1242de6953": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_f6539e378c294a6c955da278da4ade4e",
       "max": 2,
       "style": "IPY_MODEL_b1cddc992f864976b0f4132e0c4819f4",
       "value": 2
      }
     },
     "557d35c901f54575b5963e3b7ce932a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5791e4167111408ca3f1be2e16334803": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5b0ec0306b6440c1a068951bf0d4a50c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5b17418f64954c14843a914942f8ce3d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5b494db622384a7fabd7ed86d7f0575c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_26e2b0e7abb24e3fad1b5c0c166aa44d",
        "IPY_MODEL_c05cfeeac9a04c68a7387dc65bf48227"
       ],
       "layout": "IPY_MODEL_4654ebf91a484cb8b153920fb89bb522"
      }
     },
     "5c13867f02614d49bc038af563f5a5a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_7234b227a46b404598576e19a230a94e",
       "max": 6,
       "style": "IPY_MODEL_7ae2ea4a6e144242ad504c2032fbb9b9",
       "value": 6
      }
     },
     "5d3aa8de854a491485458bcaf88b9de4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c6a3a9d502b94ffba6cb6708c9762bd0",
        "IPY_MODEL_8d9389e09e1d44d88ffffdb83ac6f407"
       ],
       "layout": "IPY_MODEL_03c7476e32ce48c2b38a7023c2dcc0a1"
      }
     },
     "5f4033806f5144f28e0999110bf34612": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_e59a2233ae454d81ada1b8494763e7bc",
       "max": 2,
       "style": "IPY_MODEL_292be092bc964dd7a48bc62c906f5814",
       "value": 2
      }
     },
     "61defccfe9dc4330931ce420b9976dcd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4d7cbd1c9b0b45359d4d31ad4e50e441",
       "style": "IPY_MODEL_45a9f25bf3964af68dfce8bbb2091917",
       "value": "100% 2/2 [10:08&lt;00:00, 304.26s/it]"
      }
     },
     "62e7ecfbbea7427385deb25ed2f4232f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "62e8fbf1622b4a36a186e7f319552990": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_41a9d48c29ef4752bcd2ddc7076dac3c",
       "max": 2,
       "style": "IPY_MODEL_5b0ec0306b6440c1a068951bf0d4a50c",
       "value": 2
      }
     },
     "65d9210f3d3345019a1fcf4ad82f1ae9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "66179fdeb4534eb281f2690ded33950e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "66e271ee790148eb89ae51b9b22216aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "67e1b7b183484f03b2f63c22c44e8713": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7163788d004d4c15b0b0137a654079a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7234b227a46b404598576e19a230a94e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "77132b06e4ad40fdabdc0dd93922af21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "784cd320488247aa87d575558f81d06b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7998921f656343ffadf133220692f859": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_0b49f5119c6d4a25bb575a20e33fe219",
       "max": 5,
       "style": "IPY_MODEL_cfeaf84bdbe34c7898e1ea8fc61b539c",
       "value": 5
      }
     },
     "7ad08fc0082a41bc882cedf6004206dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_33ce7c9b732e4715ae12b71669addfdf",
        "IPY_MODEL_bc6e6b33b84344f094704454a0cf6f6b"
       ],
       "layout": "IPY_MODEL_2ff92f760a2b46a99576ab77daccac1d"
      }
     },
     "7adf432cd98f46df927bf9da9d94b182": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7ae2ea4a6e144242ad504c2032fbb9b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7b215de0d6b64c3897c22bd817bbcb8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_1e2f1c40607144eb9cb7b9e257d06f35",
       "max": 3,
       "style": "IPY_MODEL_f38f8ae81eca43f9b1828369ebffd849",
       "value": 3
      }
     },
     "7ba0d94e2ef544fe9126eacee6588468": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8228413e70844312b5aa3128e3bea3ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_890f789420e4416697cafbf732f38b95",
       "style": "IPY_MODEL_b52cba9c0daf49aab716b75c91137766",
       "value": "100% 2/2 [00:06&lt;00:00,  3.12s/it]"
      }
     },
     "830aa543858f4a54a14aad440bcbde34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "868f105f38aa47b2b0223680dc202859": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "87707de469bc4c4bb2ba2a2ed7322de1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "879f0f26c06041698336e79b9b92b0c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c19f1e5cf61444b5926c89b67c374dc8",
        "IPY_MODEL_148c1abd5f1747a2a787c09cef3071bc"
       ],
       "layout": "IPY_MODEL_b4a9636246544cd7a3ebcff56595bea2"
      }
     },
     "88db019106ca4893816ea780e567a175": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "890f789420e4416697cafbf732f38b95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8c8cb52ff5e54c55a297af7571a105ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8d9389e09e1d44d88ffffdb83ac6f407": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1fc90792a2b74c38aaf73659c3756831",
       "style": "IPY_MODEL_66179fdeb4534eb281f2690ded33950e",
       "value": "100% 2/2 [00:06&lt;00:00,  3.17s/it]"
      }
     },
     "902c060a37b4491788462dce6be1798b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "91b150ff417c44d1bc68e66d1b3c5925": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9352e3fd2af54e09ad67afb4ac446b27": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "93f4380114fe48b88db18d0c97736fd4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b7e6303bb38542c9a8282079c33beca7",
       "style": "IPY_MODEL_21e75994f5e749748dd5693819e51d93",
       "value": "100% 6/6 [00:00&lt;00:00, 343.35it/s]"
      }
     },
     "956dbbddf33a476db031c3d1b8ad5b4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7998921f656343ffadf133220692f859",
        "IPY_MODEL_39cf7d6b5e98460e9a621f1d74e32fad"
       ],
       "layout": "IPY_MODEL_65d9210f3d3345019a1fcf4ad82f1ae9"
      }
     },
     "a19c0f9c5cb34e9e8cdfe5c356084b80": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a52bd3eee7aa4e9093ee525fc42ff232": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_405077ae66ac44afbd35d90a6afd6cab",
       "style": "IPY_MODEL_fa7fbd7aa5ab41a5aaabfca1db46058e",
       "value": "100% 6/6 [00:00&lt;00:00, 377.47it/s]"
      }
     },
     "a6ba13af33984f1b96ed120d3798b035": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a6cf535c8f30471e947503cae6bdb611": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_e77a75b0e9b446318ef399e225003e65",
       "max": 7,
       "style": "IPY_MODEL_c8469a077e5449b69821e6b27b8e74e5",
       "value": 7
      }
     },
     "a7afeac8de60479e8c92248312fde8fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aad1581c410d48feb48e6b943c14bdb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b0edcf9ef0754ca4bb7fe6e8790c464b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b1258a718296423d9f4d9c0535aae63b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3b2e29458fdc49d0864a7fc9a1faafb6",
       "style": "IPY_MODEL_37f00a662a3c4b5f9f29db6903cfc5ba",
       "value": "100% 3/3 [15:16&lt;00:00, 305.39s/it]"
      }
     },
     "b1cddc992f864976b0f4132e0c4819f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b3ae58ce0bce4cd1b678a043b8ce4415": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_54f04d9c381446649b742e1242de6953",
        "IPY_MODEL_e10c6be126a045d9809b90c0880e7362"
       ],
       "layout": "IPY_MODEL_77132b06e4ad40fdabdc0dd93922af21"
      }
     },
     "b4a9636246544cd7a3ebcff56595bea2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b4c9fe6ac6f649dc9ed8a077a5eaeb20": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b52cba9c0daf49aab716b75c91137766": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b7e6303bb38542c9a8282079c33beca7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ba26964d72134b3aa6095e3102f1531b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bc6e6b33b84344f094704454a0cf6f6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e6337586212341de8a4de0ce6bd25649",
       "style": "IPY_MODEL_7adf432cd98f46df927bf9da9d94b182",
       "value": "100% 6/6 [00:00&lt;00:00, 270.73it/s]"
      }
     },
     "bd8469f87f2d4e789610110acd07571d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ed2e720acb7a467c85bd7db465281523",
        "IPY_MODEL_3a1160614abc4724b73cc9bc2e0d0d46"
       ],
       "layout": "IPY_MODEL_37faed3c96f343e895d8f29ab07137c0"
      }
     },
     "c01a6ed0806a4aaaaf580bf56e78398f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c05cfeeac9a04c68a7387dc65bf48227": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a7afeac8de60479e8c92248312fde8fc",
       "style": "IPY_MODEL_22952b3bfa674aff898e452b30bd319e",
       "value": "100% 5/5 [00:16&lt;00:00,  3.28s/it]"
      }
     },
     "c09f764cc1c74e3aa75fea864a8d21f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c80d11f161704ab3bda86e7d1dfaff03",
       "style": "IPY_MODEL_e1df5ccb47594688afaee7c51d9cfeed",
       "value": "100% 6/6 [00:00&lt;00:00, 377.26it/s]"
      }
     },
     "c13bdace91114d639b408aa120c97f79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c19f1e5cf61444b5926c89b67c374dc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_43f1cb9394c5422798a91c1b6b0a5907",
       "max": 3,
       "style": "IPY_MODEL_aad1581c410d48feb48e6b943c14bdb8",
       "value": 3
      }
     },
     "c28408357da544cd8c8bbd5c7469e541": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5c13867f02614d49bc038af563f5a5a2",
        "IPY_MODEL_1c509f99160e41d5af6e565b1541e571"
       ],
       "layout": "IPY_MODEL_e41e9dc77ce44f3fad0aee4db429cd7d"
      }
     },
     "c30905e882964682bbe231e445b57833": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_08ba1e10c1a641f7876680a632f6821d",
        "IPY_MODEL_e6f6231f671e4871b9e496cce50bf2b9"
       ],
       "layout": "IPY_MODEL_db256ef0046f46d085353ffba880c8fa"
      }
     },
     "c438be876cfc49e8920f2704c6cddbdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_de9c288444e74ea38dbac2ef1b1078e4",
        "IPY_MODEL_fe3c562acb4e432280f6432df0710577"
       ],
       "layout": "IPY_MODEL_ba26964d72134b3aa6095e3102f1531b"
      }
     },
     "c45beabad9d34af5b39ce603b5308ef9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c522ca216e5a49e4a7624f94b7215c52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_62e8fbf1622b4a36a186e7f319552990",
        "IPY_MODEL_8228413e70844312b5aa3128e3bea3ad"
       ],
       "layout": "IPY_MODEL_cb6af7a441964132bb58e75ef7b4bd19"
      }
     },
     "c6a3a9d502b94ffba6cb6708c9762bd0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_868f105f38aa47b2b0223680dc202859",
       "max": 2,
       "style": "IPY_MODEL_f4c78df23c774bb290250936916d962e",
       "value": 2
      }
     },
     "c74dedf09f57490a9f1ac9af54e14bf3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c80d11f161704ab3bda86e7d1dfaff03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c8469a077e5449b69821e6b27b8e74e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c8871c57d7bd4b61a99633668fb44621": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_2d1dfcb285154a7bac2db184a87f3c4e",
       "max": 7,
       "style": "IPY_MODEL_ce4922cb72aa4e5c8b5713e47e49029e",
       "value": 7
      }
     },
     "c8ef187b9a1249018de7378cee824894": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7b215de0d6b64c3897c22bd817bbcb8d",
        "IPY_MODEL_b1258a718296423d9f4d9c0535aae63b"
       ],
       "layout": "IPY_MODEL_784cd320488247aa87d575558f81d06b"
      }
     },
     "cac9427c464f4db9b71c36da9245c066": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cb554ddf38bb4597b282655ee74ca321": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cb6af7a441964132bb58e75ef7b4bd19": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ce4922cb72aa4e5c8b5713e47e49029e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cfeaf84bdbe34c7898e1ea8fc61b539c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d2b4db8ba2cb4c859ba3e0041c243193": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_13cbbf3a554a40f09d40df39c2cacf47",
       "style": "IPY_MODEL_91b150ff417c44d1bc68e66d1b3c5925",
       "value": "100% 7/7 [38:27&lt;00:00, 329.65s/it]"
      }
     },
     "db256ef0046f46d085353ffba880c8fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "de9c288444e74ea38dbac2ef1b1078e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_0147ea39a83144aa99e7877a908ca502",
       "max": 5,
       "style": "IPY_MODEL_075434e4b0234c5fa61c8d63fec51679",
       "value": 5
      }
     },
     "def3b0d0275c454e95c5c2952fc9d017": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5f4033806f5144f28e0999110bf34612",
        "IPY_MODEL_61defccfe9dc4330931ce420b9976dcd"
       ],
       "layout": "IPY_MODEL_e22e48eeda32478cbb6bf53babd9e700"
      }
     },
     "e10c6be126a045d9809b90c0880e7362": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5791e4167111408ca3f1be2e16334803",
       "style": "IPY_MODEL_c01a6ed0806a4aaaaf580bf56e78398f",
       "value": "100% 2/2 [10:59&lt;00:00, 329.53s/it]"
      }
     },
     "e1df5ccb47594688afaee7c51d9cfeed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e22e48eeda32478cbb6bf53babd9e700": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e41e9dc77ce44f3fad0aee4db429cd7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e59a2233ae454d81ada1b8494763e7bc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6337586212341de8a4de0ce6bd25649": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6926f2efb5340528a00c5045376c380": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6f6231f671e4871b9e496cce50bf2b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4c5e108b0b9145cb9987a673dda89bb6",
       "style": "IPY_MODEL_a19c0f9c5cb34e9e8cdfe5c356084b80",
       "value": "100% 6/6 [00:00&lt;00:00, 42.32it/s]"
      }
     },
     "e77a75b0e9b446318ef399e225003e65": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ed2e720acb7a467c85bd7db465281523": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Epoch",
       "layout": "IPY_MODEL_f9852feeb51543e4a0d21c5cd6326380",
       "max": 5,
       "style": "IPY_MODEL_cac9427c464f4db9b71c36da9245c066",
       "value": 5
      }
     },
     "f2cf6963cc204b18921e63e1933ad18b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a6cf535c8f30471e947503cae6bdb611",
        "IPY_MODEL_d2b4db8ba2cb4c859ba3e0041c243193"
       ],
       "layout": "IPY_MODEL_b4c9fe6ac6f649dc9ed8a077a5eaeb20"
      }
     },
     "f38f8ae81eca43f9b1828369ebffd849": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f4c78df23c774bb290250936916d962e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f6539e378c294a6c955da278da4ade4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f9852feeb51543e4a0d21c5cd6326380": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fa7fbd7aa5ab41a5aaabfca1db46058e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "fe3c562acb4e432280f6432df0710577": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.1.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a6ba13af33984f1b96ed120d3798b035",
       "style": "IPY_MODEL_39d7df82d70b40df9f1171dad9ce2b4c",
       "value": "100% 5/5 [25:26&lt;00:00, 305.38s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
